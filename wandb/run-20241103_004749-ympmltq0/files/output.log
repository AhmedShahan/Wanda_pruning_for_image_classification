Set warmup steps = 9750
Set warmup steps = 0
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.dwconv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.dwconv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.dwconv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.dwconv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.dwconv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.dwconv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.dwconv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.dwconv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.dwconv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.dwconv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.dwconv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.dwconv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.dwconv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.dwconv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.dwconv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.dwconv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.dwconv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.dwconv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.dwconv.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.dwconv.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.dwconv.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.dwconv.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.dwconv.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.dwconv.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.dwconv.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.dwconv.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.dwconv.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.dwconv.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.dwconv.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.dwconv.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.dwconv.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.dwconv.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.dwconv.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.dwconv.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.dwconv.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.dwconv.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
/home/shahanahmed/.local/lib/python3.10/site-packages/timm/utils/cuda.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
**************Prune Round 1**********************

Current sparsity level: 0.0017724827175138512
Non-zero weights before pruning: 27847581
Processing block 0
Layer 0.pwconv1 sparsity: 0.5000
Layer 0.pwconv1 non-zero weights remaining: 18431
Layer 0.pwconv2 sparsity: 0.5000
Layer 0.pwconv2 non-zero weights remaining: 18431
Layer 1.pwconv1 sparsity: 0.5000
Layer 1.pwconv1 non-zero weights remaining: 18431
Layer 1.pwconv2 sparsity: 0.5000
Layer 1.pwconv2 non-zero weights remaining: 18431
Layer 2.pwconv1 sparsity: 0.5000
Layer 2.pwconv1 non-zero weights remaining: 18431
Layer 2.pwconv2 sparsity: 0.5000
Layer 2.pwconv2 non-zero weights remaining: 18431
Processing block 1
Layer 0.pwconv1 sparsity: 0.5000
Layer 0.pwconv1 non-zero weights remaining: 73727
Layer 0.pwconv2 sparsity: 0.5000
Layer 0.pwconv2 non-zero weights remaining: 73727
Layer 1.pwconv1 sparsity: 0.5000
Layer 1.pwconv1 non-zero weights remaining: 73727
Layer 1.pwconv2 sparsity: 0.5000
Layer 1.pwconv2 non-zero weights remaining: 73727
Layer 2.pwconv1 sparsity: 0.5000
Layer 2.pwconv1 non-zero weights remaining: 73727
Layer 2.pwconv2 sparsity: 0.5000
Layer 2.pwconv2 non-zero weights remaining: 73727
Processing block 2
Layer 0.pwconv1 sparsity: 0.5000
Layer 0.pwconv1 non-zero weights remaining: 294911
Layer 0.pwconv2 sparsity: 0.5000
Layer 0.pwconv2 non-zero weights remaining: 294910
Layer 1.pwconv1 sparsity: 0.5000
Layer 1.pwconv1 non-zero weights remaining: 294911
Layer 1.pwconv2 sparsity: 0.5000
Layer 1.pwconv2 non-zero weights remaining: 294911
Layer 2.pwconv1 sparsity: 0.5000
Layer 2.pwconv1 non-zero weights remaining: 294911
Layer 2.pwconv2 sparsity: 0.5000
Layer 2.pwconv2 non-zero weights remaining: 294911
Layer 3.pwconv1 sparsity: 0.5000
Layer 3.pwconv1 non-zero weights remaining: 294911
Layer 3.pwconv2 sparsity: 0.5000
Layer 3.pwconv2 non-zero weights remaining: 294911
Layer 4.pwconv1 sparsity: 0.5000
Layer 4.pwconv1 non-zero weights remaining: 294911
Layer 4.pwconv2 sparsity: 0.5000
Layer 4.pwconv2 non-zero weights remaining: 294911
Layer 5.pwconv1 sparsity: 0.5000
Layer 5.pwconv1 non-zero weights remaining: 294911
Layer 5.pwconv2 sparsity: 0.5000
Layer 5.pwconv2 non-zero weights remaining: 294911
Layer 6.pwconv1 sparsity: 0.5000
Layer 6.pwconv1 non-zero weights remaining: 294911
Layer 6.pwconv2 sparsity: 0.5000
Layer 6.pwconv2 non-zero weights remaining: 294911
Layer 7.pwconv1 sparsity: 0.5000
Layer 7.pwconv1 non-zero weights remaining: 294911
Layer 7.pwconv2 sparsity: 0.5000
Layer 7.pwconv2 non-zero weights remaining: 294911
Layer 8.pwconv1 sparsity: 0.5000
Layer 8.pwconv1 non-zero weights remaining: 294911
Layer 8.pwconv2 sparsity: 0.5000
Layer 8.pwconv2 non-zero weights remaining: 294911
Processing block 3
Layer 0.pwconv1 sparsity: 0.5000
Layer 0.pwconv1 non-zero weights remaining: 1179647
Layer 0.pwconv2 sparsity: 0.5000
Layer 0.pwconv2 non-zero weights remaining: 1179647
Layer 1.pwconv1 sparsity: 0.5000
Layer 1.pwconv1 non-zero weights remaining: 1179647
Layer 1.pwconv2 sparsity: 0.5000
Layer 1.pwconv2 non-zero weights remaining: 1179647
Layer 2.pwconv1 sparsity: 0.5000
Layer 2.pwconv1 non-zero weights remaining: 1179646
Layer 2.pwconv2 sparsity: 0.5000
Layer 2.pwconv2 non-zero weights remaining: 1179647
Overall model sparsity: 0.5000
Sparsity after pruning: 0.4656
Non-zero weights after pruning: 14908282

Epoch 1/1
/home/shahanahmed/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch: [0]  [  0/196]  eta: 0:42:13  lr: 0.000000  min_lr: 0.000000  loss: 4.8022 (4.8022)  class_acc: 0.0117 (0.0117)  weight_decay: 0.0500 (0.0500)  time: 12.9284  data: 1.4052  max mem: 1752
Epoch: [0]  [ 10/196]  eta: 0:07:48  lr: 0.000004  min_lr: 0.000004  loss: 4.7426 (4.7352)  class_acc: 0.0078 (0.0110)  weight_decay: 0.0499 (0.0499)  time: 2.5173  data: 0.1305  max mem: 1752
Epoch: [0]  [ 20/196]  eta: 0:06:04  lr: 0.000008  min_lr: 0.000008  loss: 4.7407 (4.7398)  class_acc: 0.0078 (0.0093)  weight_decay: 0.0496 (0.0496)  time: 1.5285  data: 0.0021  max mem: 1752
Epoch: [0]  [ 30/196]  eta: 0:05:20  lr: 0.000012  min_lr: 0.000012  loss: 4.7317 (4.7296)  class_acc: 0.0078 (0.0106)  weight_decay: 0.0486 (0.0490)  time: 1.6122  data: 0.0017  max mem: 1752
Epoch: [0]  [ 40/196]  eta: 0:04:54  lr: 0.000016  min_lr: 0.000016  loss: 4.6985 (4.7182)  class_acc: 0.0117 (0.0116)  weight_decay: 0.0469 (0.0483)  time: 1.6951  data: 0.0023  max mem: 1752
Epoch: [0]  [ 50/196]  eta: 0:04:34  lr: 0.000021  min_lr: 0.000021  loss: 4.6602 (4.7051)  class_acc: 0.0117 (0.0126)  weight_decay: 0.0447 (0.0474)  time: 1.7968  data: 0.0031  max mem: 1752
Epoch: [0]  [ 60/196]  eta: 0:04:15  lr: 0.000025  min_lr: 0.000025  loss: 4.6275 (4.6911)  class_acc: 0.0156 (0.0136)  weight_decay: 0.0420 (0.0463)  time: 1.8590  data: 0.0032  max mem: 1752
Epoch: [0]  [ 70/196]  eta: 0:03:57  lr: 0.000029  min_lr: 0.000029  loss: 4.6092 (4.6795)  class_acc: 0.0195 (0.0147)  weight_decay: 0.0389 (0.0450)  time: 1.9039  data: 0.0026  max mem: 1752
Epoch: [0]  [ 80/196]  eta: 0:03:41  lr: 0.000033  min_lr: 0.000033  loss: 4.6154 (4.6713)  class_acc: 0.0234 (0.0158)  weight_decay: 0.0354 (0.0436)  time: 2.0173  data: 0.0024  max mem: 1752
Epoch: [0]  [ 90/196]  eta: 0:03:25  lr: 0.000037  min_lr: 0.000037  loss: 4.6024 (4.6608)  class_acc: 0.0273 (0.0175)  weight_decay: 0.0316 (0.0421)  time: 2.1367  data: 0.0029  max mem: 1752
Epoch: [0]  [100/196]  eta: 0:03:09  lr: 0.000041  min_lr: 0.000041  loss: 4.5594 (4.6493)  class_acc: 0.0273 (0.0192)  weight_decay: 0.0276 (0.0405)  time: 2.1972  data: 0.0033  max mem: 1752
Epoch: [0]  [110/196]  eta: 0:02:52  lr: 0.000045  min_lr: 0.000045  loss: 4.5461 (4.6418)  class_acc: 0.0273 (0.0200)  weight_decay: 0.0236 (0.0388)  time: 2.2718  data: 0.0023  max mem: 1752
Epoch: [0]  [120/196]  eta: 0:02:34  lr: 0.000049  min_lr: 0.000049  loss: 4.5420 (4.6317)  class_acc: 0.0312 (0.0210)  weight_decay: 0.0196 (0.0371)  time: 2.3260  data: 0.0021  max mem: 1752
Epoch: [0]  [130/196]  eta: 0:02:15  lr: 0.000053  min_lr: 0.000053  loss: 4.5222 (4.6230)  class_acc: 0.0352 (0.0223)  weight_decay: 0.0158 (0.0353)  time: 2.3264  data: 0.0024  max mem: 1752
Epoch: [0]  [140/196]  eta: 0:01:55  lr: 0.000057  min_lr: 0.000057  loss: 4.5111 (4.6158)  class_acc: 0.0352 (0.0237)  weight_decay: 0.0122 (0.0336)  time: 2.3263  data: 0.0025  max mem: 1752
Epoch: [0]  [150/196]  eta: 0:01:36  lr: 0.000062  min_lr: 0.000062  loss: 4.5051 (4.6083)  class_acc: 0.0352 (0.0243)  weight_decay: 0.0089 (0.0318)  time: 2.3271  data: 0.0026  max mem: 1752
Epoch: [0]  [160/196]  eta: 0:01:15  lr: 0.000066  min_lr: 0.000066  loss: 4.4725 (4.5990)  class_acc: 0.0352 (0.0254)  weight_decay: 0.0060 (0.0302)  time: 2.3267  data: 0.0023  max mem: 1752
Epoch: [0]  [170/196]  eta: 0:00:54  lr: 0.000070  min_lr: 0.000070  loss: 4.4694 (4.5930)  class_acc: 0.0430 (0.0262)  weight_decay: 0.0037 (0.0286)  time: 2.3263  data: 0.0025  max mem: 1752
Epoch: [0]  [180/196]  eta: 0:00:34  lr: 0.000074  min_lr: 0.000074  loss: 4.4962 (4.5868)  class_acc: 0.0391 (0.0273)  weight_decay: 0.0018 (0.0271)  time: 2.3283  data: 0.0034  max mem: 1752
Epoch: [0]  [190/196]  eta: 0:00:12  lr: 0.000078  min_lr: 0.000078  loss: 4.4918 (4.5823)  class_acc: 0.0391 (0.0280)  weight_decay: 0.0006 (0.0257)  time: 2.3289  data: 0.0032  max mem: 1752
Epoch: [0]  [195/196]  eta: 0:00:02  lr: 0.000080  min_lr: 0.000080  loss: 4.4804 (4.5807)  class_acc: 0.0391 (0.0281)  weight_decay: 0.0003 (0.0251)  time: 2.2112  data: 0.0028  max mem: 1752
Epoch: [0] Total time: 0:06:57 (2.1319 s / it)
Averaged stats: lr: 0.000080  min_lr: 0.000080  loss: 4.4804 (4.5807)  class_acc: 0.0391 (0.0281)  weight_decay: 0.0003 (0.0251)
/home/shahanahmed/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test:  [  0/196]  eta: 0:09:45  loss: 4.4335 (4.4335)  acc1: 5.4688 (5.4688)  acc5: 20.3125 (20.3125)  time: 2.9886  data: 2.5914  max mem: 1752
Test:  [ 10/196]  eta: 0:02:21  loss: 4.4902 (4.4856)  acc1: 3.5156 (3.7642)  acc5: 13.6719 (14.1335)  time: 0.7628  data: 0.2382  max mem: 1752
Test:  [ 20/196]  eta: 0:01:58  loss: 4.4680 (4.4816)  acc1: 3.5156 (3.7574)  acc5: 13.6719 (14.0253)  time: 0.5601  data: 0.0025  max mem: 1752
Test:  [ 30/196]  eta: 0:01:32  loss: 4.4677 (4.4785)  acc1: 3.9062 (3.9819)  acc5: 14.4531 (14.5917)  time: 0.4423  data: 0.0027  max mem: 1752
Test:  [ 40/196]  eta: 0:01:26  loss: 4.4654 (4.4786)  acc1: 4.2969 (4.0682)  acc5: 14.8438 (14.7961)  time: 0.4269  data: 0.0031  max mem: 1752
Test:  [ 50/196]  eta: 0:01:22  loss: 4.4693 (4.4798)  acc1: 4.2969 (4.1360)  acc5: 14.8438 (14.7059)  time: 0.5761  data: 0.0029  max mem: 1752
Test:  [ 60/196]  eta: 0:01:12  loss: 4.4697 (4.4762)  acc1: 4.6875 (4.2008)  acc5: 14.8438 (14.8502)  time: 0.4928  data: 0.0029  max mem: 1752
Test:  [ 70/196]  eta: 0:01:06  loss: 4.4697 (4.4772)  acc1: 4.2969 (4.1648)  acc5: 14.8438 (14.8162)  time: 0.4348  data: 0.0033  max mem: 1752
Test:  [ 80/196]  eta: 0:01:02  loss: 4.4659 (4.4747)  acc1: 4.2969 (4.2728)  acc5: 15.2344 (14.9691)  time: 0.5473  data: 0.0045  max mem: 1752
Test:  [ 90/196]  eta: 0:00:56  loss: 4.4774 (4.4754)  acc1: 4.6875 (4.3183)  acc5: 15.2344 (14.9081)  time: 0.5674  data: 0.0060  max mem: 1752
Test:  [100/196]  eta: 0:00:49  loss: 4.4805 (4.4753)  acc1: 4.2969 (4.2930)  acc5: 13.6719 (14.8786)  time: 0.4194  data: 0.0069  max mem: 1752
Test:  [110/196]  eta: 0:00:44  loss: 4.4738 (4.4754)  acc1: 3.5156 (4.2335)  acc5: 14.4531 (14.8121)  time: 0.4591  data: 0.0063  max mem: 1752
Test:  [120/196]  eta: 0:00:39  loss: 4.4819 (4.4751)  acc1: 3.9062 (4.2581)  acc5: 14.4531 (14.8179)  time: 0.5362  data: 0.0065  max mem: 1752
Test:  [130/196]  eta: 0:00:33  loss: 4.4669 (4.4749)  acc1: 4.2969 (4.2730)  acc5: 14.4531 (14.8646)  time: 0.3912  data: 0.0077  max mem: 1752
Test:  [140/196]  eta: 0:00:28  loss: 4.4669 (4.4751)  acc1: 4.2969 (4.3052)  acc5: 14.8438 (14.9019)  time: 0.4650  data: 0.0077  max mem: 1752
Test:  [150/196]  eta: 0:00:23  loss: 4.4898 (4.4766)  acc1: 4.2969 (4.2995)  acc5: 14.8438 (14.8567)  time: 0.6117  data: 0.0068  max mem: 1752
Test:  [160/196]  eta: 0:00:18  loss: 4.4983 (4.4759)  acc1: 4.6875 (4.3527)  acc5: 14.8438 (14.8971)  time: 0.4812  data: 0.0056  max mem: 1752
Test:  [170/196]  eta: 0:00:13  loss: 4.4519 (4.4750)  acc1: 4.2969 (4.3289)  acc5: 15.2344 (14.8917)  time: 0.4812  data: 0.0053  max mem: 1752
Test:  [180/196]  eta: 0:00:08  loss: 4.4886 (4.4757)  acc1: 3.5156 (4.3228)  acc5: 14.4531 (14.8912)  time: 0.5831  data: 0.0058  max mem: 1752
Test:  [190/196]  eta: 0:00:03  loss: 4.4811 (4.4751)  acc1: 3.5156 (4.3235)  acc5: 14.8438 (14.9051)  time: 0.4326  data: 0.0059  max mem: 1752
Test:  [195/196]  eta: 0:00:00  loss: 4.4811 (4.4759)  acc1: 3.5156 (4.3220)  acc5: 14.8438 (14.8880)  time: 0.4553  data: 0.0052  max mem: 1752
Test: Total time: 0:01:40 (0.5109 s / it)
* Acc@1 4.322 Acc@5 14.888 loss 4.476
Test:  [ 0/40]  eta: 0:01:20  loss: 4.1680 (4.1680)  acc1: 7.4219 (7.4219)  acc5: 26.9531 (26.9531)  time: 2.0246  data: 1.8124  max mem: 1752
Test:  [10/40]  eta: 0:00:20  loss: 4.2010 (4.1798)  acc1: 7.4219 (7.8835)  acc5: 25.3906 (24.7159)  time: 0.6909  data: 0.1707  max mem: 1752
Test:  [20/40]  eta: 0:00:10  loss: 4.2010 (4.1843)  acc1: 7.8125 (7.8497)  acc5: 24.2188 (24.3490)  time: 0.4677  data: 0.0051  max mem: 1752
Test:  [30/40]  eta: 0:00:05  loss: 4.2006 (4.1879)  acc1: 7.8125 (7.8251)  acc5: 23.8281 (24.0297)  time: 0.4027  data: 0.0041  max mem: 1752
Test:  [39/40]  eta: 0:00:00  loss: 4.1709 (4.1891)  acc1: 7.4219 (7.7500)  acc5: 24.2188 (24.2600)  time: 0.4961  data: 0.0054  max mem: 1752
Test: Total time: 0:00:21 (0.5316 s / it)
* Acc@1 7.750 Acc@5 24.260 loss 4.189
Training Accuracy: 0.00%
Testing Accuracy: 0.00%

Final sparsity after training: 0.4638
Final non-zero weights: 14957726
**************Prune Round 2**********************

Current sparsity level: 0.463823673260105
Non-zero weights before pruning: 14957726
Processing block 0
Layer 0.pwconv1 sparsity: 0.7500
Layer 0.pwconv1 non-zero weights remaining: 9215
Layer 0.pwconv2 sparsity: 0.7500
Layer 0.pwconv2 non-zero weights remaining: 9215
Layer 1.pwconv1 sparsity: 0.7500
Layer 1.pwconv1 non-zero weights remaining: 9215
Layer 1.pwconv2 sparsity: 0.7500
Layer 1.pwconv2 non-zero weights remaining: 9215
Layer 2.pwconv1 sparsity: 0.7500
Layer 2.pwconv1 non-zero weights remaining: 9215
Layer 2.pwconv2 sparsity: 0.7500
Layer 2.pwconv2 non-zero weights remaining: 9215
Processing block 1
Layer 0.pwconv1 sparsity: 0.7500
Layer 0.pwconv1 non-zero weights remaining: 36863
Layer 0.pwconv2 sparsity: 0.7500
Layer 0.pwconv2 non-zero weights remaining: 36863
Layer 1.pwconv1 sparsity: 0.7500
Layer 1.pwconv1 non-zero weights remaining: 36863
Layer 1.pwconv2 sparsity: 0.7500
Layer 1.pwconv2 non-zero weights remaining: 36863
Layer 2.pwconv1 sparsity: 0.7500
Layer 2.pwconv1 non-zero weights remaining: 36863
Layer 2.pwconv2 sparsity: 0.7500
Layer 2.pwconv2 non-zero weights remaining: 36863
Processing block 2
Layer 0.pwconv1 sparsity: 0.7500
Layer 0.pwconv1 non-zero weights remaining: 147455
Layer 0.pwconv2 sparsity: 0.7500
Layer 0.pwconv2 non-zero weights remaining: 147454
Layer 1.pwconv1 sparsity: 0.7500
Layer 1.pwconv1 non-zero weights remaining: 147455
Layer 1.pwconv2 sparsity: 0.7500
Layer 1.pwconv2 non-zero weights remaining: 147455
Layer 2.pwconv1 sparsity: 0.7500
Layer 2.pwconv1 non-zero weights remaining: 147455
Layer 2.pwconv2 sparsity: 0.7500
Layer 2.pwconv2 non-zero weights remaining: 147455
Layer 3.pwconv1 sparsity: 0.7500
Layer 3.pwconv1 non-zero weights remaining: 147455
Layer 3.pwconv2 sparsity: 0.7500
Layer 3.pwconv2 non-zero weights remaining: 147455
Layer 4.pwconv1 sparsity: 0.7500
Layer 4.pwconv1 non-zero weights remaining: 147455
Layer 4.pwconv2 sparsity: 0.7500
Layer 4.pwconv2 non-zero weights remaining: 147455
Layer 5.pwconv1 sparsity: 0.7500
Layer 5.pwconv1 non-zero weights remaining: 147455
Layer 5.pwconv2 sparsity: 0.7500
Layer 5.pwconv2 non-zero weights remaining: 147455
Layer 6.pwconv1 sparsity: 0.7500
Layer 6.pwconv1 non-zero weights remaining: 147455
Layer 6.pwconv2 sparsity: 0.7500
Layer 6.pwconv2 non-zero weights remaining: 147455
Layer 7.pwconv1 sparsity: 0.7500
Layer 7.pwconv1 non-zero weights remaining: 147455
Layer 7.pwconv2 sparsity: 0.7500
Layer 7.pwconv2 non-zero weights remaining: 147455
Layer 8.pwconv1 sparsity: 0.7500
Layer 8.pwconv1 non-zero weights remaining: 147455
Layer 8.pwconv2 sparsity: 0.7500
Layer 8.pwconv2 non-zero weights remaining: 147455
Processing block 3
Layer 0.pwconv1 sparsity: 0.7500
Layer 0.pwconv1 non-zero weights remaining: 589823
Layer 0.pwconv2 sparsity: 0.7500
Layer 0.pwconv2 non-zero weights remaining: 589823
Layer 1.pwconv1 sparsity: 0.7500
Layer 1.pwconv1 non-zero weights remaining: 589823
Layer 1.pwconv2 sparsity: 0.7500
Layer 1.pwconv2 non-zero weights remaining: 589823
Layer 2.pwconv1 sparsity: 0.7500
Layer 2.pwconv1 non-zero weights remaining: 589822
Layer 2.pwconv2 sparsity: 0.7500
Layer 2.pwconv2 non-zero weights remaining: 589823
Overall model sparsity: 0.2500
Sparsity after pruning: 0.6957
Non-zero weights after pruning: 8488094

Epoch 1/1
Epoch: [0]  [  0/196]  eta: 0:13:56  lr: 0.000000  min_lr: 0.000000  loss: 4.4842 (4.4842)  class_acc: 0.0312 (0.0312)  weight_decay: 0.0500 (0.0500)  time: 4.2685  data: 2.1022  max mem: 1752
Epoch: [0]  [ 10/196]  eta: 0:07:47  lr: 0.000004  min_lr: 0.000004  loss: 4.4832 (4.4824)  class_acc: 0.0391 (0.0394)  weight_decay: 0.0499 (0.0499)  time: 2.5159  data: 0.1961  max mem: 1752
Epoch: [0]  [ 20/196]  eta: 0:07:07  lr: 0.000008  min_lr: 0.000008  loss: 4.4700 (4.4787)  class_acc: 0.0430 (0.0407)  weight_decay: 0.0496 (0.0496)  time: 2.3395  data: 0.0045  max mem: 1752
Epoch: [0]  [ 30/196]  eta: 0:06:38  lr: 0.000012  min_lr: 0.000012  loss: 4.4589 (4.4763)  class_acc: 0.0469 (0.0410)  weight_decay: 0.0486 (0.0490)  time: 2.3380  data: 0.0038  max mem: 1752
Epoch: [0]  [ 40/196]  eta: 0:06:12  lr: 0.000016  min_lr: 0.000016  loss: 4.4817 (4.4821)  class_acc: 0.0391 (0.0400)  weight_decay: 0.0469 (0.0483)  time: 2.3379  data: 0.0045  max mem: 1752
Epoch: [0]  [ 50/196]  eta: 0:05:49  lr: 0.000021  min_lr: 0.000021  loss: 4.4987 (4.4825)  class_acc: 0.0391 (0.0413)  weight_decay: 0.0447 (0.0474)  time: 2.3756  data: 0.0060  max mem: 1752
Epoch: [0]  [ 60/196]  eta: 0:05:27  lr: 0.000025  min_lr: 0.000025  loss: 4.4455 (4.4741)  class_acc: 0.0469 (0.0437)  weight_decay: 0.0420 (0.0463)  time: 2.4607  data: 0.0057  max mem: 1752
Epoch: [0]  [ 70/196]  eta: 0:05:03  lr: 0.000029  min_lr: 0.000029  loss: 4.4377 (4.4730)  class_acc: 0.0469 (0.0441)  weight_decay: 0.0389 (0.0450)  time: 2.4660  data: 0.0056  max mem: 1752
Epoch: [0]  [ 80/196]  eta: 0:04:41  lr: 0.000033  min_lr: 0.000033  loss: 4.4480 (4.4701)  class_acc: 0.0430 (0.0442)  weight_decay: 0.0354 (0.0436)  time: 2.4743  data: 0.0066  max mem: 1752
Epoch: [0]  [ 90/196]  eta: 0:04:20  lr: 0.000037  min_lr: 0.000037  loss: 4.4533 (4.4688)  class_acc: 0.0391 (0.0441)  weight_decay: 0.0316 (0.0421)  time: 2.6150  data: 0.0058  max mem: 1752
Epoch: [0]  [100/196]  eta: 0:03:56  lr: 0.000041  min_lr: 0.000041  loss: 4.4358 (4.4649)  class_acc: 0.0469 (0.0449)  weight_decay: 0.0276 (0.0405)  time: 2.6142  data: 0.0051  max mem: 1752
Epoch: [0]  [110/196]  eta: 0:03:31  lr: 0.000045  min_lr: 0.000045  loss: 4.4435 (4.4641)  class_acc: 0.0469 (0.0452)  weight_decay: 0.0236 (0.0388)  time: 2.4707  data: 0.0047  max mem: 1752
Epoch: [0]  [120/196]  eta: 0:03:07  lr: 0.000049  min_lr: 0.000049  loss: 4.4488 (4.4623)  class_acc: 0.0469 (0.0458)  weight_decay: 0.0196 (0.0371)  time: 2.4620  data: 0.0048  max mem: 1752
Epoch: [0]  [130/196]  eta: 0:02:42  lr: 0.000053  min_lr: 0.000053  loss: 4.4469 (4.4612)  class_acc: 0.0430 (0.0457)  weight_decay: 0.0158 (0.0353)  time: 2.4925  data: 0.0065  max mem: 1752
Epoch: [0]  [140/196]  eta: 0:02:18  lr: 0.000057  min_lr: 0.000057  loss: 4.4229 (4.4585)  class_acc: 0.0508 (0.0463)  weight_decay: 0.0122 (0.0336)  time: 2.5223  data: 0.0074  max mem: 1752
Epoch: [0]  [150/196]  eta: 0:01:53  lr: 0.000062  min_lr: 0.000062  loss: 4.4264 (4.4587)  class_acc: 0.0469 (0.0461)  weight_decay: 0.0089 (0.0318)  time: 2.5591  data: 0.0055  max mem: 1752
Epoch: [0]  [160/196]  eta: 0:01:29  lr: 0.000066  min_lr: 0.000066  loss: 4.4331 (4.4565)  class_acc: 0.0469 (0.0467)  weight_decay: 0.0060 (0.0302)  time: 2.5879  data: 0.0045  max mem: 1752
Epoch: [0]  [170/196]  eta: 0:01:04  lr: 0.000070  min_lr: 0.000070  loss: 4.4330 (4.4550)  class_acc: 0.0547 (0.0468)  weight_decay: 0.0037 (0.0286)  time: 2.5750  data: 0.0055  max mem: 1752
Epoch: [0]  [180/196]  eta: 0:00:39  lr: 0.000074  min_lr: 0.000074  loss: 4.4330 (4.4525)  class_acc: 0.0508 (0.0470)  weight_decay: 0.0018 (0.0271)  time: 2.5349  data: 0.0076  max mem: 1752
Epoch: [0]  [190/196]  eta: 0:00:15  lr: 0.000078  min_lr: 0.000078  loss: 4.4080 (4.4502)  class_acc: 0.0508 (0.0477)  weight_decay: 0.0006 (0.0257)  time: 2.6547  data: 0.0067  max mem: 1752
Epoch: [0]  [195/196]  eta: 0:00:02  lr: 0.000080  min_lr: 0.000080  loss: 4.4013 (4.4489)  class_acc: 0.0508 (0.0479)  weight_decay: 0.0003 (0.0251)  time: 2.4723  data: 0.0047  max mem: 1752
Epoch: [0] Total time: 0:08:08 (2.4916 s / it)
Averaged stats: lr: 0.000080  min_lr: 0.000080  loss: 4.4013 (4.4489)  class_acc: 0.0508 (0.0479)  weight_decay: 0.0003 (0.0251)
Test:  [  0/196]  eta: 0:09:56  loss: 4.3539 (4.3539)  acc1: 4.6875 (4.6875)  acc5: 16.7969 (16.7969)  time: 3.0410  data: 2.8239  max mem: 1752
Test:  [ 10/196]  eta: 0:02:30  loss: 4.4053 (4.4016)  acc1: 5.8594 (5.4332)  acc5: 18.3594 (18.3949)  time: 0.8084  data: 0.2590  max mem: 1752
Test:  [ 20/196]  eta: 0:02:05  loss: 4.4209 (4.4200)  acc1: 5.4688 (5.2455)  acc5: 17.9688 (17.8571)  time: 0.5951  data: 0.0031  max mem: 1752
Test:  [ 30/196]  eta: 0:01:52  loss: 4.4118 (4.4142)  acc1: 5.0781 (5.4435)  acc5: 17.5781 (18.0192)  time: 0.6052  data: 0.0037  max mem: 1752
Test:  [ 40/196]  eta: 0:01:43  loss: 4.3927 (4.4144)  acc1: 5.0781 (5.3068)  acc5: 18.3594 (17.9402)  time: 0.6076  data: 0.0051  max mem: 1752
Test:  [ 50/196]  eta: 0:01:34  loss: 4.4080 (4.4163)  acc1: 5.0781 (5.3156)  acc5: 16.7969 (17.6164)  time: 0.6091  data: 0.0056  max mem: 1752
Test:  [ 60/196]  eta: 0:01:27  loss: 4.4063 (4.4156)  acc1: 5.8594 (5.3471)  acc5: 16.7969 (17.8343)  time: 0.6085  data: 0.0047  max mem: 1752
Test:  [ 70/196]  eta: 0:01:20  loss: 4.4063 (4.4166)  acc1: 5.4688 (5.3807)  acc5: 17.5781 (17.7652)  time: 0.6078  data: 0.0047  max mem: 1752
Test:  [ 80/196]  eta: 0:01:13  loss: 4.4186 (4.4180)  acc1: 5.4688 (5.3819)  acc5: 17.1875 (17.7373)  time: 0.6071  data: 0.0048  max mem: 1752
Test:  [ 90/196]  eta: 0:01:06  loss: 4.4154 (4.4156)  acc1: 5.0781 (5.4044)  acc5: 17.1875 (17.7928)  time: 0.6071  data: 0.0047  max mem: 1752
Test:  [100/196]  eta: 0:01:00  loss: 4.4048 (4.4152)  acc1: 5.4688 (5.4301)  acc5: 17.5781 (17.8489)  time: 0.6075  data: 0.0050  max mem: 1752
Test:  [110/196]  eta: 0:00:53  loss: 4.4112 (4.4154)  acc1: 5.0781 (5.3737)  acc5: 17.5781 (17.7787)  time: 0.6059  data: 0.0045  max mem: 1752
Test:  [120/196]  eta: 0:00:47  loss: 4.4285 (4.4174)  acc1: 4.6875 (5.3525)  acc5: 16.4062 (17.7686)  time: 0.6038  data: 0.0033  max mem: 1752
Test:  [130/196]  eta: 0:00:41  loss: 4.4247 (4.4172)  acc1: 5.4688 (5.3554)  acc5: 17.1875 (17.7839)  time: 0.6063  data: 0.0042  max mem: 1752
Test:  [140/196]  eta: 0:00:34  loss: 4.4247 (4.4183)  acc1: 4.6875 (5.3385)  acc5: 17.1875 (17.7416)  time: 0.6087  data: 0.0056  max mem: 1752
Test:  [150/196]  eta: 0:00:28  loss: 4.4037 (4.4170)  acc1: 4.6875 (5.3498)  acc5: 17.5781 (17.7489)  time: 0.6089  data: 0.0061  max mem: 1752
Test:  [160/196]  eta: 0:00:21  loss: 4.3983 (4.4179)  acc1: 5.0781 (5.3474)  acc5: 17.5781 (17.7140)  time: 0.4974  data: 0.0073  max mem: 1752
Test:  [170/196]  eta: 0:00:15  loss: 4.3983 (4.4169)  acc1: 5.4688 (5.3728)  acc5: 17.1875 (17.6992)  time: 0.4231  data: 0.0084  max mem: 1752
Test:  [180/196]  eta: 0:00:09  loss: 4.4446 (4.4187)  acc1: 5.0781 (5.3371)  acc5: 16.7969 (17.6342)  time: 0.4497  data: 0.0091  max mem: 1752
Test:  [190/196]  eta: 0:00:03  loss: 4.4541 (4.4202)  acc1: 5.0781 (5.3338)  acc5: 16.4062 (17.6108)  time: 0.4932  data: 0.0072  max mem: 1752
Test:  [195/196]  eta: 0:00:00  loss: 4.4535 (4.4200)  acc1: 5.0781 (5.3440)  acc5: 17.1875 (17.6060)  time: 0.4828  data: 0.0047  max mem: 1752
Test: Total time: 0:01:55 (0.5882 s / it)
* Acc@1 5.344 Acc@5 17.606 loss 4.420
Test:  [ 0/40]  eta: 0:01:13  loss: 4.0440 (4.0440)  acc1: 9.7656 (9.7656)  acc5: 25.3906 (25.3906)  time: 1.8471  data: 1.6339  max mem: 1752
Test:  [10/40]  eta: 0:00:20  loss: 4.1020 (4.0807)  acc1: 9.7656 (9.5170)  acc5: 26.5625 (26.5980)  time: 0.6674  data: 0.1548  max mem: 1752
Test:  [20/40]  eta: 0:00:12  loss: 4.1020 (4.0861)  acc1: 8.9844 (9.4494)  acc5: 26.9531 (27.1205)  time: 0.5762  data: 0.0046  max mem: 1752
Test:  [30/40]  eta: 0:00:06  loss: 4.1000 (4.0873)  acc1: 8.9844 (9.5262)  acc5: 26.1719 (26.7515)  time: 0.6026  data: 0.0025  max mem: 1752
Test:  [39/40]  eta: 0:00:00  loss: 4.0875 (4.0915)  acc1: 9.7656 (9.6300)  acc5: 26.1719 (26.8000)  time: 0.5767  data: 0.0030  max mem: 1752
Test: Total time: 0:00:24 (0.6161 s / it)
* Acc@1 9.630 Acc@5 26.800 loss 4.092
Training Accuracy: 0.00%
Testing Accuracy: 0.00%

Final sparsity after training: 0.4638
Final non-zero weights: 14957726
**************Prune Round 3**********************

Current sparsity level: 0.463823673260105
Non-zero weights before pruning: 14957726
Processing block 0
Layer 0.pwconv1 sparsity: 0.7500
Layer 0.pwconv1 non-zero weights remaining: 9215
Layer 0.pwconv2 sparsity: 0.7500
Layer 0.pwconv2 non-zero weights remaining: 9215
Layer 1.pwconv1 sparsity: 0.7500
Layer 1.pwconv1 non-zero weights remaining: 9215
Layer 1.pwconv2 sparsity: 0.7500
Layer 1.pwconv2 non-zero weights remaining: 9215
Layer 2.pwconv1 sparsity: 0.7500
Layer 2.pwconv1 non-zero weights remaining: 9215
Layer 2.pwconv2 sparsity: 0.7500
Layer 2.pwconv2 non-zero weights remaining: 9215
Processing block 1
Layer 0.pwconv1 sparsity: 0.7500
Layer 0.pwconv1 non-zero weights remaining: 36863
Layer 0.pwconv2 sparsity: 0.7500
Layer 0.pwconv2 non-zero weights remaining: 36863
Layer 1.pwconv1 sparsity: 0.7500
Layer 1.pwconv1 non-zero weights remaining: 36863
Layer 1.pwconv2 sparsity: 0.7500
Layer 1.pwconv2 non-zero weights remaining: 36863
Layer 2.pwconv1 sparsity: 0.7500
Layer 2.pwconv1 non-zero weights remaining: 36863
Layer 2.pwconv2 sparsity: 0.7500
Layer 2.pwconv2 non-zero weights remaining: 36863
Processing block 2
Layer 0.pwconv1 sparsity: 0.7500
Layer 0.pwconv1 non-zero weights remaining: 147455
Layer 0.pwconv2 sparsity: 0.7500
Layer 0.pwconv2 non-zero weights remaining: 147454
Layer 1.pwconv1 sparsity: 0.7500
Layer 1.pwconv1 non-zero weights remaining: 147455
Layer 1.pwconv2 sparsity: 0.7500
Layer 1.pwconv2 non-zero weights remaining: 147455
Layer 2.pwconv1 sparsity: 0.7500
Layer 2.pwconv1 non-zero weights remaining: 147455
Layer 2.pwconv2 sparsity: 0.7500
Layer 2.pwconv2 non-zero weights remaining: 147455
Layer 3.pwconv1 sparsity: 0.7500
Layer 3.pwconv1 non-zero weights remaining: 147455
Layer 3.pwconv2 sparsity: 0.7500
Layer 3.pwconv2 non-zero weights remaining: 147455
Layer 4.pwconv1 sparsity: 0.7500
Layer 4.pwconv1 non-zero weights remaining: 147455
Layer 4.pwconv2 sparsity: 0.7500
Layer 4.pwconv2 non-zero weights remaining: 147455
Layer 5.pwconv1 sparsity: 0.7500
Layer 5.pwconv1 non-zero weights remaining: 147455
Layer 5.pwconv2 sparsity: 0.7500
Layer 5.pwconv2 non-zero weights remaining: 147455
Layer 6.pwconv1 sparsity: 0.7500
Layer 6.pwconv1 non-zero weights remaining: 147455
Layer 6.pwconv2 sparsity: 0.7500
Layer 6.pwconv2 non-zero weights remaining: 147455
Layer 7.pwconv1 sparsity: 0.7500
Layer 7.pwconv1 non-zero weights remaining: 147455
Layer 7.pwconv2 sparsity: 0.7500
Layer 7.pwconv2 non-zero weights remaining: 147455
Layer 8.pwconv1 sparsity: 0.7500
Layer 8.pwconv1 non-zero weights remaining: 147455
Layer 8.pwconv2 sparsity: 0.7500
Layer 8.pwconv2 non-zero weights remaining: 147455
Processing block 3
Layer 0.pwconv1 sparsity: 0.7500
Layer 0.pwconv1 non-zero weights remaining: 589823
Layer 0.pwconv2 sparsity: 0.7500
Layer 0.pwconv2 non-zero weights remaining: 589823
Layer 1.pwconv1 sparsity: 0.7500
Layer 1.pwconv1 non-zero weights remaining: 589823
Layer 1.pwconv2 sparsity: 0.7500
Layer 1.pwconv2 non-zero weights remaining: 589823
Layer 2.pwconv1 sparsity: 0.7500
Layer 2.pwconv1 non-zero weights remaining: 589822
Layer 2.pwconv2 sparsity: 0.7500
Layer 2.pwconv2 non-zero weights remaining: 589823
Overall model sparsity: 0.2500
Sparsity after pruning: 0.6957
Non-zero weights after pruning: 8488094

Epoch 1/1
Epoch: [0]  [  0/196]  eta: 0:22:01  lr: 0.000000  min_lr: 0.000000  loss: 4.4467 (4.4467)  class_acc: 0.0820 (0.0820)  weight_decay: 0.0500 (0.0500)  time: 6.7429  data: 4.2873  max mem: 1752
Epoch: [0]  [ 10/196]  eta: 0:08:29  lr: 0.000004  min_lr: 0.000004  loss: 4.4177 (4.4089)  class_acc: 0.0547 (0.0565)  weight_decay: 0.0499 (0.0499)  time: 2.7414  data: 0.3946  max mem: 1752
Epoch: [0]  [ 20/196]  eta: 0:07:28  lr: 0.000008  min_lr: 0.000008  loss: 4.4297 (4.4240)  class_acc: 0.0508 (0.0536)  weight_decay: 0.0496 (0.0496)  time: 2.3390  data: 0.0037  max mem: 1752
Epoch: [0]  [ 30/196]  eta: 0:06:56  lr: 0.000012  min_lr: 0.000012  loss: 4.4297 (4.4216)  class_acc: 0.0508 (0.0548)  weight_decay: 0.0486 (0.0490)  time: 2.3799  data: 0.0051  max mem: 1752
Epoch: [0]  [ 40/196]  eta: 0:06:32  lr: 0.000016  min_lr: 0.000016  loss: 4.4193 (4.4226)  class_acc: 0.0547 (0.0532)  weight_decay: 0.0469 (0.0483)  time: 2.4784  data: 0.0068  max mem: 1752
Epoch: [0]  [ 50/196]  eta: 0:06:08  lr: 0.000021  min_lr: 0.000021  loss: 4.4231 (4.4234)  class_acc: 0.0508 (0.0522)  weight_decay: 0.0447 (0.0474)  time: 2.5486  data: 0.0056  max mem: 1752
Epoch: [0]  [ 60/196]  eta: 0:05:43  lr: 0.000025  min_lr: 0.000025  loss: 4.4131 (4.4224)  class_acc: 0.0508 (0.0519)  weight_decay: 0.0420 (0.0463)  time: 2.5538  data: 0.0051  max mem: 1752
Epoch: [0]  [ 70/196]  eta: 0:05:20  lr: 0.000029  min_lr: 0.000029  loss: 4.4011 (4.4196)  class_acc: 0.0547 (0.0524)  weight_decay: 0.0389 (0.0450)  time: 2.5839  data: 0.0048  max mem: 1752
Epoch: [0]  [ 80/196]  eta: 0:04:56  lr: 0.000033  min_lr: 0.000033  loss: 4.4066 (4.4188)  class_acc: 0.0547 (0.0531)  weight_decay: 0.0354 (0.0436)  time: 2.6326  data: 0.0047  max mem: 1752
Epoch: [0]  [ 90/196]  eta: 0:04:30  lr: 0.000037  min_lr: 0.000037  loss: 4.4063 (4.4178)  class_acc: 0.0586 (0.0536)  weight_decay: 0.0316 (0.0421)  time: 2.5913  data: 0.0044  max mem: 1752
Epoch: [0]  [100/196]  eta: 0:04:04  lr: 0.000041  min_lr: 0.000041  loss: 4.4063 (4.4161)  class_acc: 0.0508 (0.0533)  weight_decay: 0.0276 (0.0405)  time: 2.5418  data: 0.0046  max mem: 1752
Epoch: [0]  [110/196]  eta: 0:03:39  lr: 0.000045  min_lr: 0.000045  loss: 4.4043 (4.4141)  class_acc: 0.0586 (0.0543)  weight_decay: 0.0236 (0.0388)  time: 2.5394  data: 0.0065  max mem: 1752
Epoch: [0]  [120/196]  eta: 0:03:14  lr: 0.000049  min_lr: 0.000049  loss: 4.3815 (4.4115)  class_acc: 0.0625 (0.0549)  weight_decay: 0.0196 (0.0371)  time: 2.5941  data: 0.0053  max mem: 1752
Epoch: [0]  [130/196]  eta: 0:02:48  lr: 0.000053  min_lr: 0.000053  loss: 4.3887 (4.4128)  class_acc: 0.0586 (0.0552)  weight_decay: 0.0158 (0.0353)  time: 2.6094  data: 0.0020  max mem: 1752
Epoch: [0]  [140/196]  eta: 0:02:23  lr: 0.000057  min_lr: 0.000057  loss: 4.4014 (4.4104)  class_acc: 0.0508 (0.0550)  weight_decay: 0.0122 (0.0336)  time: 2.5669  data: 0.0017  max mem: 1752
Epoch: [0]  [150/196]  eta: 0:01:57  lr: 0.000062  min_lr: 0.000062  loss: 4.3928 (4.4097)  class_acc: 0.0508 (0.0550)  weight_decay: 0.0089 (0.0318)  time: 2.5125  data: 0.0020  max mem: 1752
Epoch: [0]  [160/196]  eta: 0:01:31  lr: 0.000066  min_lr: 0.000066  loss: 4.4070 (4.4095)  class_acc: 0.0508 (0.0549)  weight_decay: 0.0060 (0.0302)  time: 2.4809  data: 0.0037  max mem: 1752
Epoch: [0]  [170/196]  eta: 0:01:06  lr: 0.000070  min_lr: 0.000070  loss: 4.4058 (4.4093)  class_acc: 0.0547 (0.0547)  weight_decay: 0.0037 (0.0286)  time: 2.4590  data: 0.0055  max mem: 1752
Epoch: [0]  [180/196]  eta: 0:00:40  lr: 0.000074  min_lr: 0.000074  loss: 4.3813 (4.4077)  class_acc: 0.0547 (0.0551)  weight_decay: 0.0018 (0.0271)  time: 2.4073  data: 0.0051  max mem: 1752
Epoch: [0]  [190/196]  eta: 0:00:15  lr: 0.000078  min_lr: 0.000078  loss: 4.3786 (4.4069)  class_acc: 0.0586 (0.0551)  weight_decay: 0.0006 (0.0257)  time: 2.4680  data: 0.0040  max mem: 1752
Epoch: [0]  [195/196]  eta: 0:00:02  lr: 0.000080  min_lr: 0.000080  loss: 4.3776 (4.4064)  class_acc: 0.0508 (0.0551)  weight_decay: 0.0003 (0.0251)  time: 2.3142  data: 0.0031  max mem: 1752
Epoch: [0] Total time: 0:08:13 (2.5188 s / it)
Averaged stats: lr: 0.000080  min_lr: 0.000080  loss: 4.3776 (4.4064)  class_acc: 0.0508 (0.0551)  weight_decay: 0.0003 (0.0251)
Test:  [  0/196]  eta: 0:09:51  loss: 4.4274 (4.4274)  acc1: 2.7344 (2.7344)  acc5: 17.1875 (17.1875)  time: 3.0204  data: 2.7913  max mem: 1752
Test:  [ 10/196]  eta: 0:02:30  loss: 4.3795 (4.4081)  acc1: 5.4688 (4.7230)  acc5: 17.5781 (17.3295)  time: 0.8093  data: 0.2636  max mem: 1752
Test:  [ 20/196]  eta: 0:02:05  loss: 4.3790 (4.3882)  acc1: 5.4688 (5.1711)  acc5: 18.7500 (18.6012)  time: 0.5977  data: 0.0067  max mem: 1752
Test:  [ 30/196]  eta: 0:01:52  loss: 4.3710 (4.3909)  acc1: 5.4688 (5.1411)  acc5: 18.3594 (18.3216)  time: 0.6050  data: 0.0029  max mem: 1752
Test:  [ 40/196]  eta: 0:01:38  loss: 4.4292 (4.4036)  acc1: 4.6875 (5.0781)  acc5: 17.5781 (17.9688)  time: 0.5395  data: 0.0077  max mem: 1752
Test:  [ 50/196]  eta: 0:01:27  loss: 4.4344 (4.3998)  acc1: 4.6875 (5.1854)  acc5: 17.5781 (18.1909)  time: 0.4770  data: 0.0140  max mem: 1752
Test:  [ 60/196]  eta: 0:01:21  loss: 4.3864 (4.3930)  acc1: 5.8594 (5.3663)  acc5: 17.9688 (18.3210)  time: 0.5447  data: 0.0117  max mem: 1752
Test:  [ 70/196]  eta: 0:01:15  loss: 4.3866 (4.3923)  acc1: 5.8594 (5.3917)  acc5: 18.3594 (18.4639)  time: 0.6117  data: 0.0076  max mem: 1752
Test:  [ 80/196]  eta: 0:01:10  loss: 4.3909 (4.3899)  acc1: 5.8594 (5.4832)  acc5: 19.9219 (18.6825)  time: 0.6118  data: 0.0074  max mem: 1752
Test:  [ 90/196]  eta: 0:01:04  loss: 4.3895 (4.3885)  acc1: 5.8594 (5.5074)  acc5: 19.9219 (18.8015)  time: 0.6114  data: 0.0070  max mem: 1752
Test:  [100/196]  eta: 0:00:56  loss: 4.4062 (4.3906)  acc1: 5.4688 (5.5693)  acc5: 17.9688 (18.6726)  time: 0.5119  data: 0.0084  max mem: 1752
Test:  [110/196]  eta: 0:00:48  loss: 4.4203 (4.3924)  acc1: 5.0781 (5.5180)  acc5: 17.5781 (18.6057)  time: 0.3729  data: 0.0145  max mem: 1752
Test:  [120/196]  eta: 0:00:42  loss: 4.3960 (4.3928)  acc1: 5.0781 (5.5591)  acc5: 17.5781 (18.5369)  time: 0.4283  data: 0.0132  max mem: 1752
Test:  [130/196]  eta: 0:00:37  loss: 4.3776 (4.3920)  acc1: 5.8594 (5.5880)  acc5: 17.5781 (18.5532)  time: 0.5654  data: 0.0060  max mem: 1752
Test:  [140/196]  eta: 0:00:31  loss: 4.4004 (4.3927)  acc1: 5.8594 (5.5851)  acc5: 17.5781 (18.5893)  time: 0.6065  data: 0.0042  max mem: 1752
Test:  [150/196]  eta: 0:00:26  loss: 4.4056 (4.3938)  acc1: 6.2500 (5.6136)  acc5: 19.5312 (18.5974)  time: 0.5829  data: 0.0039  max mem: 1752
Test:  [160/196]  eta: 0:00:20  loss: 4.4150 (4.3954)  acc1: 5.8594 (5.6070)  acc5: 17.9688 (18.5583)  time: 0.4812  data: 0.0046  max mem: 1752
Test:  [170/196]  eta: 0:00:14  loss: 4.4158 (4.3960)  acc1: 5.0781 (5.5990)  acc5: 17.1875 (18.5216)  time: 0.5058  data: 0.0058  max mem: 1752
Test:  [180/196]  eta: 0:00:08  loss: 4.4035 (4.3967)  acc1: 5.8594 (5.6328)  acc5: 17.1875 (18.4781)  time: 0.6098  data: 0.0064  max mem: 1752
Test:  [190/196]  eta: 0:00:03  loss: 4.4162 (4.3973)  acc1: 5.4688 (5.5955)  acc5: 17.5781 (18.4432)  time: 0.5854  data: 0.0061  max mem: 1752
Test:  [195/196]  eta: 0:00:00  loss: 4.4090 (4.3969)  acc1: 5.0781 (5.6000)  acc5: 17.9688 (18.4500)  time: 0.4992  data: 0.0055  max mem: 1752
Test: Total time: 0:01:48 (0.5561 s / it)
* Acc@1 5.600 Acc@5 18.450 loss 4.397
Test:  [ 0/40]  eta: 0:01:08  loss: 3.9864 (3.9864)  acc1: 11.7188 (11.7188)  acc5: 27.3438 (27.3438)  time: 1.7106  data: 1.4755  max mem: 1752
Test:  [10/40]  eta: 0:00:18  loss: 4.0341 (4.0242)  acc1: 9.7656 (9.7301)  acc5: 28.5156 (29.0128)  time: 0.6310  data: 0.1751  max mem: 1752
Test:  [20/40]  eta: 0:00:12  loss: 4.0368 (4.0327)  acc1: 9.7656 (9.8214)  acc5: 28.5156 (28.8132)  time: 0.5649  data: 0.0244  max mem: 1752
Test:  [30/40]  eta: 0:00:06  loss: 4.0406 (4.0365)  acc1: 9.3750 (9.8664)  acc5: 28.1250 (28.6164)  time: 0.6065  data: 0.0038  max mem: 1752
Test:  [39/40]  eta: 0:00:00  loss: 4.0302 (4.0393)  acc1: 10.1562 (9.7700)  acc5: 28.5156 (28.6800)  time: 0.5574  data: 0.0045  max mem: 1752
Test: Total time: 0:00:23 (0.5963 s / it)
* Acc@1 9.770 Acc@5 28.680 loss 4.039
Training Accuracy: 0.00%
Testing Accuracy: 0.00%

Final sparsity after training: 0.4638
Final non-zero weights: 14957726
**************Prune Round 4**********************

Current sparsity level: 0.463823673260105
Non-zero weights before pruning: 14957726
Processing block 0
Layer 0.pwconv1 sparsity: 0.7500
Layer 0.pwconv1 non-zero weights remaining: 9215
Layer 0.pwconv2 sparsity: 0.7500
Layer 0.pwconv2 non-zero weights remaining: 9215
Layer 1.pwconv1 sparsity: 0.7500
Layer 1.pwconv1 non-zero weights remaining: 9215
Layer 1.pwconv2 sparsity: 0.7500
Layer 1.pwconv2 non-zero weights remaining: 9215
Layer 2.pwconv1 sparsity: 0.7500
Layer 2.pwconv1 non-zero weights remaining: 9215
Layer 2.pwconv2 sparsity: 0.7500
Layer 2.pwconv2 non-zero weights remaining: 9215
Processing block 1
Layer 0.pwconv1 sparsity: 0.7500
Layer 0.pwconv1 non-zero weights remaining: 36863
Layer 0.pwconv2 sparsity: 0.7500
Layer 0.pwconv2 non-zero weights remaining: 36863
Layer 1.pwconv1 sparsity: 0.7500
Layer 1.pwconv1 non-zero weights remaining: 36863
Layer 1.pwconv2 sparsity: 0.7500
Layer 1.pwconv2 non-zero weights remaining: 36863
Layer 2.pwconv1 sparsity: 0.7500
Layer 2.pwconv1 non-zero weights remaining: 36863
Layer 2.pwconv2 sparsity: 0.7500
Layer 2.pwconv2 non-zero weights remaining: 36863
Processing block 2
Layer 0.pwconv1 sparsity: 0.7500
Layer 0.pwconv1 non-zero weights remaining: 147455
Layer 0.pwconv2 sparsity: 0.7500
Layer 0.pwconv2 non-zero weights remaining: 147454
Layer 1.pwconv1 sparsity: 0.7500
Layer 1.pwconv1 non-zero weights remaining: 147455
Layer 1.pwconv2 sparsity: 0.7500
Layer 1.pwconv2 non-zero weights remaining: 147455
Layer 2.pwconv1 sparsity: 0.7500
Layer 2.pwconv1 non-zero weights remaining: 147455
Layer 2.pwconv2 sparsity: 0.7500
Layer 2.pwconv2 non-zero weights remaining: 147455
Layer 3.pwconv1 sparsity: 0.7500
Layer 3.pwconv1 non-zero weights remaining: 147455
Layer 3.pwconv2 sparsity: 0.7500
Layer 3.pwconv2 non-zero weights remaining: 147455
Layer 4.pwconv1 sparsity: 0.7500
Layer 4.pwconv1 non-zero weights remaining: 147455
Layer 4.pwconv2 sparsity: 0.7500
Layer 4.pwconv2 non-zero weights remaining: 147455
Layer 5.pwconv1 sparsity: 0.7500
Layer 5.pwconv1 non-zero weights remaining: 147455
Layer 5.pwconv2 sparsity: 0.7500
Layer 5.pwconv2 non-zero weights remaining: 147455
Layer 6.pwconv1 sparsity: 0.7500
Layer 6.pwconv1 non-zero weights remaining: 147455
Layer 6.pwconv2 sparsity: 0.7500
Layer 6.pwconv2 non-zero weights remaining: 147455
Layer 7.pwconv1 sparsity: 0.7500
Layer 7.pwconv1 non-zero weights remaining: 147455
Layer 7.pwconv2 sparsity: 0.7500
Layer 7.pwconv2 non-zero weights remaining: 147455
Layer 8.pwconv1 sparsity: 0.7500
Layer 8.pwconv1 non-zero weights remaining: 147455
Layer 8.pwconv2 sparsity: 0.7500
Layer 8.pwconv2 non-zero weights remaining: 147455
Processing block 3
Layer 0.pwconv1 sparsity: 0.7500
Layer 0.pwconv1 non-zero weights remaining: 589823
Layer 0.pwconv2 sparsity: 0.7500
Layer 0.pwconv2 non-zero weights remaining: 589823
Layer 1.pwconv1 sparsity: 0.7500
Layer 1.pwconv1 non-zero weights remaining: 589823
Layer 1.pwconv2 sparsity: 0.7500
Layer 1.pwconv2 non-zero weights remaining: 589823
Layer 2.pwconv1 sparsity: 0.7500
Layer 2.pwconv1 non-zero weights remaining: 589822
Layer 2.pwconv2 sparsity: 0.7500
Layer 2.pwconv2 non-zero weights remaining: 589823
Overall model sparsity: 0.2500
Sparsity after pruning: 0.6957
Non-zero weights after pruning: 8488094

Epoch 1/1
Epoch: [0]  [  0/196]  eta: 0:17:53  lr: 0.000000  min_lr: 0.000000  loss: 4.2959 (4.2959)  class_acc: 0.0703 (0.0703)  weight_decay: 0.0500 (0.0500)  time: 5.4779  data: 3.5302  max mem: 1752
Epoch: [0]  [ 10/196]  eta: 0:08:09  lr: 0.000004  min_lr: 0.000004  loss: 4.4036 (4.3910)  class_acc: 0.0508 (0.0558)  weight_decay: 0.0499 (0.0499)  time: 2.6330  data: 0.3412  max mem: 1752
Epoch: [0]  [ 20/196]  eta: 0:07:17  lr: 0.000008  min_lr: 0.000008  loss: 4.4036 (4.3984)  class_acc: 0.0508 (0.0528)  weight_decay: 0.0496 (0.0496)  time: 2.3358  data: 0.0116  max mem: 1752
