Set warmup steps = 9750
Set warmup steps = 0
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.dwconv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.dwconv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.dwconv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.dwconv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.dwconv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.dwconv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.dwconv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.dwconv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.dwconv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.dwconv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.dwconv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.dwconv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.dwconv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.dwconv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.dwconv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.dwconv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.dwconv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.dwconv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.dwconv.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.dwconv.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.dwconv.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.dwconv.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.dwconv.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.dwconv.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.dwconv.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.dwconv.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.dwconv.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.dwconv.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.dwconv.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.dwconv.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.dwconv.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.dwconv.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.dwconv.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.dwconv.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.dwconv.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.dwconv.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
/home/shahanahmed/.local/lib/python3.10/site-packages/timm/utils/cuda.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
**************Prune Round 1**********************

Current sparsity level: 0.0017724827175138512
Target sparsity for this round: 0.0000
Processing block 0
Block 0 - Current sparsity level: 1.0000
Processing block 1
Block 1 - Current sparsity level: 1.0000
Processing block 2
Block 2 - Current sparsity level: 1.0000
Processing block 3
Block 3 - Current sparsity level: 1.0000
Actual sparsity after pruning: 0.929416997394848
/home/shahanahmed/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch: [0]  [  0/196]  eta: 1:11:46  lr: 0.000000  min_lr: 0.000000  loss: 4.7563 (4.7563)  class_acc: 0.0039 (0.0039)  weight_decay: 0.0500 (0.0500)  time: 21.9706  data: 4.7944  max mem: 1751
Epoch: [0]  [ 10/196]  eta: 0:11:45  lr: 0.000004  min_lr: 0.000004  loss: 4.7563 (4.7499)  class_acc: 0.0078 (0.0067)  weight_decay: 0.0499 (0.0499)  time: 3.7951  data: 0.4391  max mem: 1751
Epoch: [0]  [ 20/196]  eta: 0:08:54  lr: 0.000008  min_lr: 0.000008  loss: 4.7347 (4.7369)  class_acc: 0.0078 (0.0086)  weight_decay: 0.0496 (0.0496)  time: 2.0886  data: 0.0030  max mem: 1751
Epoch: [0]  [ 30/196]  eta: 0:07:46  lr: 0.000012  min_lr: 0.000012  loss: 4.7102 (4.7234)  class_acc: 0.0078 (0.0091)  weight_decay: 0.0486 (0.0490)  time: 2.2699  data: 0.0040  max mem: 1751
Epoch: [0]  [ 40/196]  eta: 0:07:00  lr: 0.000016  min_lr: 0.000016  loss: 4.6796 (4.7099)  class_acc: 0.0117 (0.0105)  weight_decay: 0.0469 (0.0483)  time: 2.3419  data: 0.0065  max mem: 1751
Epoch: [0]  [ 50/196]  eta: 0:06:23  lr: 0.000021  min_lr: 0.000021  loss: 4.6585 (4.7000)  class_acc: 0.0156 (0.0119)  weight_decay: 0.0447 (0.0474)  time: 2.3430  data: 0.0073  max mem: 1751
Epoch: [0]  [ 60/196]  eta: 0:05:50  lr: 0.000025  min_lr: 0.000025  loss: 4.6425 (4.6873)  class_acc: 0.0156 (0.0137)  weight_decay: 0.0420 (0.0463)  time: 2.3417  data: 0.0063  max mem: 1751
Epoch: [0]  [ 70/196]  eta: 0:05:21  lr: 0.000029  min_lr: 0.000029  loss: 4.6201 (4.6764)  class_acc: 0.0195 (0.0153)  weight_decay: 0.0389 (0.0450)  time: 2.3455  data: 0.0071  max mem: 1751
Epoch: [0]  [ 80/196]  eta: 0:04:52  lr: 0.000033  min_lr: 0.000033  loss: 4.6036 (4.6677)  class_acc: 0.0195 (0.0163)  weight_decay: 0.0354 (0.0436)  time: 2.3463  data: 0.0077  max mem: 1751
Epoch: [0]  [ 90/196]  eta: 0:04:25  lr: 0.000037  min_lr: 0.000037  loss: 4.6006 (4.6608)  class_acc: 0.0195 (0.0169)  weight_decay: 0.0316 (0.0421)  time: 2.3419  data: 0.0065  max mem: 1751
Epoch: [0]  [100/196]  eta: 0:03:58  lr: 0.000041  min_lr: 0.000041  loss: 4.5705 (4.6513)  class_acc: 0.0234 (0.0179)  weight_decay: 0.0276 (0.0405)  time: 2.3417  data: 0.0065  max mem: 1751
Epoch: [0]  [110/196]  eta: 0:03:32  lr: 0.000045  min_lr: 0.000045  loss: 4.5605 (4.6419)  class_acc: 0.0312 (0.0197)  weight_decay: 0.0236 (0.0388)  time: 2.3475  data: 0.0076  max mem: 1751
Epoch: [0]  [120/196]  eta: 0:03:07  lr: 0.000049  min_lr: 0.000049  loss: 4.5418 (4.6324)  class_acc: 0.0352 (0.0213)  weight_decay: 0.0196 (0.0371)  time: 2.3447  data: 0.0072  max mem: 1751
Epoch: [0]  [130/196]  eta: 0:02:41  lr: 0.000053  min_lr: 0.000053  loss: 4.5237 (4.6228)  class_acc: 0.0352 (0.0225)  weight_decay: 0.0158 (0.0353)  time: 2.3348  data: 0.0049  max mem: 1751
Epoch: [0]  [140/196]  eta: 0:02:16  lr: 0.000057  min_lr: 0.000057  loss: 4.5150 (4.6156)  class_acc: 0.0352 (0.0235)  weight_decay: 0.0122 (0.0336)  time: 2.3334  data: 0.0041  max mem: 1751
Epoch: [0]  [150/196]  eta: 0:01:52  lr: 0.000062  min_lr: 0.000062  loss: 4.5150 (4.6094)  class_acc: 0.0312 (0.0240)  weight_decay: 0.0089 (0.0318)  time: 2.3355  data: 0.0050  max mem: 1751
Epoch: [0]  [160/196]  eta: 0:01:27  lr: 0.000066  min_lr: 0.000066  loss: 4.5044 (4.6016)  class_acc: 0.0352 (0.0254)  weight_decay: 0.0060 (0.0302)  time: 2.3341  data: 0.0044  max mem: 1751
Epoch: [0]  [170/196]  eta: 0:01:03  lr: 0.000070  min_lr: 0.000070  loss: 4.4817 (4.5947)  class_acc: 0.0430 (0.0264)  weight_decay: 0.0037 (0.0286)  time: 2.3305  data: 0.0032  max mem: 1751
Epoch: [0]  [180/196]  eta: 0:00:38  lr: 0.000074  min_lr: 0.000074  loss: 4.5000 (4.5891)  class_acc: 0.0391 (0.0272)  weight_decay: 0.0018 (0.0271)  time: 2.3342  data: 0.0043  max mem: 1751
Epoch: [0]  [190/196]  eta: 0:00:14  lr: 0.000078  min_lr: 0.000078  loss: 4.4848 (4.5830)  class_acc: 0.0430 (0.0284)  weight_decay: 0.0006 (0.0257)  time: 2.3460  data: 0.0076  max mem: 1751
Epoch: [0]  [195/196]  eta: 0:00:02  lr: 0.000080  min_lr: 0.000080  loss: 4.4827 (4.5808)  class_acc: 0.0508 (0.0287)  weight_decay: 0.0003 (0.0251)  time: 2.2309  data: 0.0081  max mem: 1751
Epoch: [0] Total time: 0:07:51 (2.4054 s / it)
Averaged stats: lr: 0.000080  min_lr: 0.000080  loss: 4.4827 (4.5808)  class_acc: 0.0508 (0.0287)  weight_decay: 0.0003 (0.0251)
/home/shahanahmed/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test:  [  0/196]  eta: 0:13:42  loss: 4.4761 (4.4761)  acc1: 4.2969 (4.2969)  acc5: 17.9688 (17.9688)  time: 4.1949  data: 3.7116  max mem: 1751
Test:  [ 10/196]  eta: 0:02:21  loss: 4.5015 (4.4979)  acc1: 4.2969 (4.5099)  acc5: 16.4062 (15.4830)  time: 0.7590  data: 0.3991  max mem: 1751
Test:  [ 20/196]  eta: 0:01:41  loss: 4.4820 (4.4835)  acc1: 4.2969 (4.2225)  acc5: 14.8438 (15.0670)  time: 0.3955  data: 0.0488  max mem: 1751
Test:  [ 30/196]  eta: 0:01:23  loss: 4.4560 (4.4861)  acc1: 3.9062 (4.1331)  acc5: 14.8438 (14.9824)  time: 0.3640  data: 0.0243  max mem: 1751
Test:  [ 40/196]  eta: 0:01:12  loss: 4.4756 (4.4822)  acc1: 4.2969 (4.3350)  acc5: 14.8438 (14.9390)  time: 0.3517  data: 0.0156  max mem: 1751
Test:  [ 50/196]  eta: 0:01:03  loss: 4.4601 (4.4790)  acc1: 4.2969 (4.2892)  acc5: 14.8438 (15.1195)  time: 0.3340  data: 0.0121  max mem: 1751
Test:  [ 60/196]  eta: 0:00:57  loss: 4.4691 (4.4794)  acc1: 3.9062 (4.2969)  acc5: 15.6250 (15.0999)  time: 0.3215  data: 0.0159  max mem: 1751
Test:  [ 70/196]  eta: 0:00:54  loss: 4.4761 (4.4762)  acc1: 4.2969 (4.3299)  acc5: 15.2344 (15.2124)  time: 0.4264  data: 0.0150  max mem: 1751
Test:  [ 80/196]  eta: 0:00:53  loss: 4.4705 (4.4762)  acc1: 4.2969 (4.3499)  acc5: 15.2344 (15.2151)  time: 0.5725  data: 0.0119  max mem: 1751
Test:  [ 90/196]  eta: 0:00:50  loss: 4.4705 (4.4752)  acc1: 4.2969 (4.3312)  acc5: 16.0156 (15.3288)  time: 0.6146  data: 0.0100  max mem: 1751
Test:  [100/196]  eta: 0:00:45  loss: 4.4795 (4.4757)  acc1: 4.2969 (4.3394)  acc5: 16.0156 (15.3349)  time: 0.5260  data: 0.0087  max mem: 1751
Test:  [110/196]  eta: 0:00:39  loss: 4.4585 (4.4749)  acc1: 5.0781 (4.4376)  acc5: 16.0156 (15.4174)  time: 0.3807  data: 0.0126  max mem: 1751
Test:  [120/196]  eta: 0:00:33  loss: 4.4661 (4.4754)  acc1: 4.6875 (4.4163)  acc5: 16.0156 (15.4474)  time: 0.3253  data: 0.0211  max mem: 1751
Test:  [130/196]  eta: 0:00:28  loss: 4.4732 (4.4739)  acc1: 4.2969 (4.4490)  acc5: 15.2344 (15.4193)  time: 0.3355  data: 0.0320  max mem: 1751
Test:  [140/196]  eta: 0:00:24  loss: 4.4602 (4.4742)  acc1: 4.6875 (4.4742)  acc5: 16.0156 (15.4449)  time: 0.3470  data: 0.0401  max mem: 1751
Test:  [150/196]  eta: 0:00:20  loss: 4.4602 (4.4734)  acc1: 4.6875 (4.4909)  acc5: 15.6250 (15.4362)  time: 0.4282  data: 0.0317  max mem: 1751
Test:  [160/196]  eta: 0:00:15  loss: 4.4589 (4.4729)  acc1: 4.2969 (4.5346)  acc5: 15.2344 (15.4163)  time: 0.4167  data: 0.0190  max mem: 1751
Test:  [170/196]  eta: 0:00:11  loss: 4.4589 (4.4735)  acc1: 4.2969 (4.5139)  acc5: 15.2344 (15.3692)  time: 0.4772  data: 0.0177  max mem: 1751
Test:  [180/196]  eta: 0:00:07  loss: 4.4728 (4.4738)  acc1: 3.9062 (4.4738)  acc5: 15.2344 (15.3315)  time: 0.6194  data: 0.0133  max mem: 1751
Test:  [190/196]  eta: 0:00:02  loss: 4.4829 (4.4738)  acc1: 4.2969 (4.4789)  acc5: 15.2344 (15.3387)  time: 0.6137  data: 0.0092  max mem: 1751
Test:  [195/196]  eta: 0:00:00  loss: 4.4886 (4.4749)  acc1: 3.9062 (4.4760)  acc5: 15.2344 (15.3540)  time: 0.6317  data: 0.0078  max mem: 1751
Test: Total time: 0:01:31 (0.4688 s / it)
* Acc@1 4.476 Acc@5 15.354 loss 4.475
Test:  [ 0/40]  eta: 0:01:39  loss: 4.1348 (4.1348)  acc1: 8.2031 (8.2031)  acc5: 26.5625 (26.5625)  time: 2.4993  data: 2.2748  max mem: 1751
Test:  [10/40]  eta: 0:00:15  loss: 4.1954 (4.1791)  acc1: 8.5938 (8.3097)  acc5: 24.6094 (24.1122)  time: 0.5042  data: 0.2142  max mem: 1751
Test:  [20/40]  eta: 0:00:08  loss: 4.1958 (4.1878)  acc1: 8.2031 (8.1473)  acc5: 23.8281 (23.7909)  time: 0.3056  data: 0.0054  max mem: 1751
Test:  [30/40]  eta: 0:00:04  loss: 4.1958 (4.1917)  acc1: 7.8125 (8.1023)  acc5: 22.6562 (23.6139)  time: 0.3534  data: 0.0040  max mem: 1751
Test:  [39/40]  eta: 0:00:00  loss: 4.1817 (4.1960)  acc1: 7.8125 (7.9400)  acc5: 23.4375 (23.5300)  time: 0.4864  data: 0.0062  max mem: 1751
Test: Total time: 0:00:18 (0.4618 s / it)
* Acc@1 7.940 Acc@5 23.530 loss 4.196
Sparsity after training: 0.9291
**************Prune Round 2**********************

Current sparsity level: 0.9290692901050248
Target sparsity for this round: 0.0000
Processing block 0
Block 0 - Current sparsity level: 1.0000
Processing block 1
Block 1 - Current sparsity level: 1.0000
Processing block 2
Block 2 - Current sparsity level: 1.0000
Processing block 3
Block 3 - Current sparsity level: 1.0000
Actual sparsity after pruning: 0.9290692901050248
Epoch: [0]  [  0/196]  eta: 0:18:41  lr: 0.000000  min_lr: 0.000000  loss: 4.4585 (4.4585)  class_acc: 0.0391 (0.0391)  weight_decay: 0.0500 (0.0500)  time: 5.7228  data: 3.5326  max mem: 1751
Epoch: [0]  [ 10/196]  eta: 0:08:13  lr: 0.000004  min_lr: 0.000004  loss: 4.4900 (4.4869)  class_acc: 0.0430 (0.0444)  weight_decay: 0.0499 (0.0499)  time: 2.6509  data: 0.3255  max mem: 1751
Epoch: [0]  [ 20/196]  eta: 0:07:20  lr: 0.000008  min_lr: 0.000008  loss: 4.4729 (4.4815)  class_acc: 0.0430 (0.0433)  weight_decay: 0.0496 (0.0496)  time: 2.3423  data: 0.0048  max mem: 1751
Epoch: [0]  [ 30/196]  eta: 0:06:46  lr: 0.000012  min_lr: 0.000012  loss: 4.4564 (4.4733)  class_acc: 0.0469 (0.0456)  weight_decay: 0.0486 (0.0490)  time: 2.3395  data: 0.0048  max mem: 1751
Epoch: [0]  [ 40/196]  eta: 0:06:18  lr: 0.000016  min_lr: 0.000016  loss: 4.4578 (4.4752)  class_acc: 0.0469 (0.0453)  weight_decay: 0.0469 (0.0483)  time: 2.3411  data: 0.0059  max mem: 1751
Epoch: [0]  [ 50/196]  eta: 0:05:51  lr: 0.000021  min_lr: 0.000021  loss: 4.4578 (4.4703)  class_acc: 0.0469 (0.0456)  weight_decay: 0.0447 (0.0474)  time: 2.3436  data: 0.0071  max mem: 1751
Epoch: [0]  [ 60/196]  eta: 0:05:26  lr: 0.000025  min_lr: 0.000025  loss: 4.4474 (4.4702)  class_acc: 0.0391 (0.0445)  weight_decay: 0.0420 (0.0463)  time: 2.3430  data: 0.0068  max mem: 1751
Epoch: [0]  [ 70/196]  eta: 0:05:01  lr: 0.000029  min_lr: 0.000029  loss: 4.4410 (4.4651)  class_acc: 0.0469 (0.0459)  weight_decay: 0.0389 (0.0450)  time: 2.3422  data: 0.0062  max mem: 1751
