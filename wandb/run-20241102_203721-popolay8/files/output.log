Set warmup steps = 8750
Set warmup steps = 0
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.dwconv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.dwconv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.dwconv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.dwconv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.dwconv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.dwconv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.dwconv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.dwconv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.dwconv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.dwconv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.dwconv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.dwconv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.dwconv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.dwconv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.dwconv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.dwconv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.dwconv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.dwconv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.dwconv.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.dwconv.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.dwconv.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.dwconv.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.dwconv.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.dwconv.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.dwconv.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.dwconv.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.dwconv.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.dwconv.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.dwconv.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.dwconv.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.dwconv.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.dwconv.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.dwconv.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.dwconv.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.dwconv.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.dwconv.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
/home/shahanahmed/.local/lib/python3.10/site-packages/timm/utils/cuda.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
**************Prune Round 1**********************

Current sparsity level: 1.1592622269705604e-07
block 0
block 1
block 2
block 3
Actual sparsity after pruning: 1.0
/home/shahanahmed/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch: [0]  [  0/176]  eta: 0:38:42  lr: 0.000000  min_lr: 0.000000  loss: 7.1341 (7.1341)  class_acc: 0.0000 (0.0000)  weight_decay: 0.0500 (0.0500)  time: 13.1942  data: 2.3829  max mem: 1586
Epoch: [0]  [ 10/176]  eta: 0:06:58  lr: 0.000005  min_lr: 0.000005  loss: 7.0471 (7.0405)  class_acc: 0.0000 (0.0007)  weight_decay: 0.0499 (0.0499)  time: 2.5230  data: 0.2177  max mem: 1586
Epoch: [0]  [ 20/176]  eta: 0:05:13  lr: 0.000009  min_lr: 0.000009  loss: 7.0258 (7.0306)  class_acc: 0.0000 (0.0006)  weight_decay: 0.0495 (0.0495)  time: 1.4528  data: 0.0013  max mem: 1586
Epoch: [0]  [ 30/176]  eta: 0:04:30  lr: 0.000014  min_lr: 0.000014  loss: 6.9927 (7.0096)  class_acc: 0.0000 (0.0005)  weight_decay: 0.0482 (0.0488)  time: 1.4789  data: 0.0024  max mem: 1586
Epoch: [0]  [ 40/176]  eta: 0:04:01  lr: 0.000018  min_lr: 0.000018  loss: 6.9292 (6.9822)  class_acc: 0.0000 (0.0012)  weight_decay: 0.0462 (0.0479)  time: 1.5318  data: 0.0029  max mem: 1586
Epoch: [0]  [ 50/176]  eta: 0:03:40  lr: 0.000023  min_lr: 0.000023  loss: 6.8690 (6.9539)  class_acc: 0.0039 (0.0019)  weight_decay: 0.0435 (0.0467)  time: 1.5923  data: 0.0024  max mem: 1586
Epoch: [0]  [ 60/176]  eta: 0:03:21  lr: 0.000027  min_lr: 0.000027  loss: 6.7550 (6.9112)  class_acc: 0.0039 (0.0028)  weight_decay: 0.0402 (0.0454)  time: 1.6646  data: 0.0021  max mem: 1586
Epoch: [0]  [ 70/176]  eta: 0:03:04  lr: 0.000032  min_lr: 0.000032  loss: 6.6296 (6.8613)  class_acc: 0.0078 (0.0051)  weight_decay: 0.0364 (0.0439)  time: 1.7209  data: 0.0024  max mem: 1586
Epoch: [0]  [ 80/176]  eta: 0:02:47  lr: 0.000037  min_lr: 0.000037  loss: 6.4490 (6.7956)  class_acc: 0.0156 (0.0062)  weight_decay: 0.0323 (0.0422)  time: 1.7683  data: 0.0027  max mem: 1586
Epoch: [0]  [ 90/176]  eta: 0:02:31  lr: 0.000041  min_lr: 0.000041  loss: 6.1824 (6.7191)  class_acc: 0.0195 (0.0082)  weight_decay: 0.0279 (0.0404)  time: 1.8246  data: 0.0022  max mem: 1586
Epoch: [0]  [100/176]  eta: 0:02:14  lr: 0.000046  min_lr: 0.000046  loss: 5.9825 (6.6373)  class_acc: 0.0234 (0.0096)  weight_decay: 0.0234 (0.0385)  time: 1.8731  data: 0.0044  max mem: 1586
Epoch: [0]  [110/176]  eta: 0:01:58  lr: 0.000050  min_lr: 0.000050  loss: 5.7469 (6.5470)  class_acc: 0.0234 (0.0109)  weight_decay: 0.0190 (0.0366)  time: 1.9213  data: 0.0079  max mem: 1586
Epoch: [0]  [120/176]  eta: 0:01:41  lr: 0.000055  min_lr: 0.000055  loss: 5.5011 (6.4526)  class_acc: 0.0234 (0.0123)  weight_decay: 0.0148 (0.0347)  time: 1.9678  data: 0.0073  max mem: 1586
Epoch: [0]  [130/176]  eta: 0:01:23  lr: 0.000059  min_lr: 0.000059  loss: 5.3303 (6.3610)  class_acc: 0.0312 (0.0140)  weight_decay: 0.0109 (0.0327)  time: 2.0126  data: 0.0060  max mem: 1586
Epoch: [0]  [140/176]  eta: 0:01:06  lr: 0.000064  min_lr: 0.000064  loss: 5.1675 (6.2719)  class_acc: 0.0312 (0.0150)  weight_decay: 0.0074 (0.0308)  time: 2.0754  data: 0.0052  max mem: 1586
Epoch: [0]  [150/176]  eta: 0:00:48  lr: 0.000069  min_lr: 0.000069  loss: 5.0264 (6.1873)  class_acc: 0.0273 (0.0159)  weight_decay: 0.0045 (0.0290)  time: 2.1393  data: 0.0045  max mem: 1586
Epoch: [0]  [160/176]  eta: 0:00:30  lr: 0.000073  min_lr: 0.000073  loss: 4.9799 (6.1115)  class_acc: 0.0312 (0.0171)  weight_decay: 0.0023 (0.0273)  time: 2.1761  data: 0.0053  max mem: 1586
Epoch: [0]  [170/176]  eta: 0:00:11  lr: 0.000078  min_lr: 0.000078  loss: 4.8902 (6.0373)  class_acc: 0.0312 (0.0179)  weight_decay: 0.0008 (0.0257)  time: 2.2176  data: 0.0052  max mem: 1586
Epoch: [0]  [175/176]  eta: 0:00:01  lr: 0.000080  min_lr: 0.000080  loss: 4.8182 (6.0083)  class_acc: 0.0352 (0.0185)  weight_decay: 0.0004 (0.0251)  time: 2.1201  data: 0.0044  max mem: 1586
Epoch: [0] Total time: 0:05:35 (1.9063 s / it)
Averaged stats: lr: 0.000080  min_lr: 0.000080  loss: 4.8182 (6.0083)  class_acc: 0.0352 (0.0185)  weight_decay: 0.0004 (0.0251)
/home/shahanahmed/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test:  [  0/176]  eta: 0:12:06  loss: 4.7208 (4.7208)  acc1: 5.4688 (5.4688)  acc5: 12.1094 (12.1094)  time: 4.1262  data: 3.6680  max mem: 1586
Test:  [ 10/176]  eta: 0:01:54  loss: 4.7748 (4.7641)  acc1: 3.9062 (3.9773)  acc5: 14.4531 (14.1690)  time: 0.6902  data: 0.3599  max mem: 1586
Test:  [ 20/176]  eta: 0:01:20  loss: 4.7549 (4.7460)  acc1: 3.9062 (3.9435)  acc5: 14.4531 (14.2671)  time: 0.3372  data: 0.0350  max mem: 1586
Test:  [ 30/176]  eta: 0:01:07  loss: 4.7169 (4.7371)  acc1: 3.9062 (3.8432)  acc5: 13.6719 (13.8231)  time: 0.3351  data: 0.0423  max mem: 1586
Test:  [ 40/176]  eta: 0:00:57  loss: 4.7169 (4.7350)  acc1: 3.9062 (3.8872)  acc5: 13.6719 (13.9291)  time: 0.3240  data: 0.0359  max mem: 1586
Test:  [ 50/176]  eta: 0:00:50  loss: 4.7342 (4.7344)  acc1: 3.9062 (3.9522)  acc5: 14.0625 (14.0395)  time: 0.3171  data: 0.0280  max mem: 1586
Test:  [ 60/176]  eta: 0:00:45  loss: 4.7600 (4.7407)  acc1: 3.5156 (3.8038)  acc5: 14.4531 (14.1457)  time: 0.3297  data: 0.0336  max mem: 1586
Test:  [ 70/176]  eta: 0:00:40  loss: 4.7679 (4.7409)  acc1: 3.5156 (3.8457)  acc5: 14.4531 (14.1340)  time: 0.3324  data: 0.0329  max mem: 1586
Test:  [ 80/176]  eta: 0:00:36  loss: 4.7679 (4.7470)  acc1: 4.2969 (3.8821)  acc5: 13.2812 (14.0577)  time: 0.3338  data: 0.0265  max mem: 1586
Test:  [ 90/176]  eta: 0:00:31  loss: 4.7526 (4.7459)  acc1: 3.5156 (3.8204)  acc5: 13.6719 (14.0325)  time: 0.3253  data: 0.0321  max mem: 1586
Test:  [100/176]  eta: 0:00:27  loss: 4.7240 (4.7457)  acc1: 3.1250 (3.7941)  acc5: 14.4531 (14.0200)  time: 0.3234  data: 0.0304  max mem: 1586
Test:  [110/176]  eta: 0:00:24  loss: 4.7524 (4.7435)  acc1: 3.5156 (3.8112)  acc5: 12.8906 (14.0308)  time: 0.3342  data: 0.0287  max mem: 1586
Test:  [120/176]  eta: 0:00:20  loss: 4.7528 (4.7465)  acc1: 3.5156 (3.7674)  acc5: 12.8906 (13.8688)  time: 0.3526  data: 0.0336  max mem: 1586
Test:  [130/176]  eta: 0:00:16  loss: 4.7528 (4.7446)  acc1: 3.5156 (3.7721)  acc5: 13.2812 (13.8836)  time: 0.3482  data: 0.0315  max mem: 1586
Test:  [140/176]  eta: 0:00:13  loss: 4.7256 (4.7423)  acc1: 3.5156 (3.7400)  acc5: 12.5000 (13.7855)  time: 0.3434  data: 0.0438  max mem: 1586
Test:  [150/176]  eta: 0:00:09  loss: 4.7267 (4.7430)  acc1: 3.1250 (3.7019)  acc5: 12.5000 (13.7210)  time: 0.3494  data: 0.0404  max mem: 1586
Test:  [160/176]  eta: 0:00:05  loss: 4.7372 (4.7427)  acc1: 3.1250 (3.6855)  acc5: 13.6719 (13.7835)  time: 0.3381  data: 0.0259  max mem: 1586
Test:  [170/176]  eta: 0:00:02  loss: 4.7372 (4.7431)  acc1: 3.5156 (3.7098)  acc5: 14.0625 (13.7655)  time: 0.3209  data: 0.0170  max mem: 1586
Test:  [175/176]  eta: 0:00:00  loss: 4.7214 (4.7431)  acc1: 3.5156 (3.6911)  acc5: 14.0000 (13.7644)  time: 0.3753  data: 0.0104  max mem: 1586
Test: Total time: 0:01:04 (0.3636 s / it)
* Acc@1 3.691 Acc@5 13.764 loss 4.743
Test:  [ 0/59]  eta: 0:02:02  loss: 5.3983 (5.3983)  acc1: 16.0156 (16.0156)  acc5: 38.2812 (38.2812)  time: 2.0814  data: 1.8739  max mem: 1586
Test:  [10/59]  eta: 0:00:22  loss: 6.0570 (5.9273)  acc1: 1.1719 (3.4091)  acc5: 7.0312 (12.3580)  time: 0.4617  data: 0.1747  max mem: 1586
Test:  [20/59]  eta: 0:00:15  loss: 6.0208 (5.8920)  acc1: 0.7812 (3.9993)  acc5: 7.0312 (13.3371)  time: 0.3027  data: 0.0042  max mem: 1586
Test:  [30/59]  eta: 0:00:10  loss: 5.9005 (5.8658)  acc1: 1.9531 (4.5741)  acc5: 15.2344 (14.0751)  time: 0.3057  data: 0.0038  max mem: 1586
Test:  [40/59]  eta: 0:00:06  loss: 5.5816 (5.7404)  acc1: 3.5156 (6.0880)  acc5: 16.0156 (17.0351)  time: 0.3059  data: 0.0042  max mem: 1586
Test:  [50/59]  eta: 0:00:03  loss: 5.7132 (5.7461)  acc1: 4.6875 (5.6756)  acc5: 19.1406 (17.2105)  time: 0.3085  data: 0.0061  max mem: 1586
Test:  [58/59]  eta: 0:00:00  loss: 5.5044 (5.7323)  acc1: 1.9531 (5.6198)  acc5: 18.3594 (17.3070)  time: 0.3371  data: 0.0062  max mem: 1586
Test: Total time: 0:00:20 (0.3497 s / it)
* Acc@1 5.620 Acc@5 17.307 loss 5.732
**************Prune Round 2**********************

Current sparsity level: 1.0
block 0
block 1
block 2
block 3
Actual sparsity after pruning: 1.0
Epoch: [0]  [  0/176]  eta: 0:15:46  lr: 0.000000  min_lr: 0.000000  loss: 4.7122 (4.7122)  class_acc: 0.0352 (0.0352)  weight_decay: 0.0500 (0.0500)  time: 5.3801  data: 3.6646  max mem: 1586
Epoch: [0]  [ 10/176]  eta: 0:07:29  lr: 0.000005  min_lr: 0.000005  loss: 4.7122 (4.7275)  class_acc: 0.0352 (0.0359)  weight_decay: 0.0499 (0.0499)  time: 2.7065  data: 0.4945  max mem: 1586
Epoch: [0]  [ 20/176]  eta: 0:06:35  lr: 0.000009  min_lr: 0.000009  loss: 4.7755 (4.7602)  class_acc: 0.0352 (0.0366)  weight_decay: 0.0495 (0.0495)  time: 2.3918  data: 0.0899  max mem: 1586
Epoch: [0]  [ 30/176]  eta: 0:06:01  lr: 0.000014  min_lr: 0.000014  loss: 4.7530 (4.7450)  class_acc: 0.0352 (0.0364)  weight_decay: 0.0482 (0.0488)  time: 2.3460  data: 0.0036  max mem: 1586
Epoch: [0]  [ 40/176]  eta: 0:05:32  lr: 0.000018  min_lr: 0.000018  loss: 4.7132 (4.7387)  class_acc: 0.0352 (0.0371)  weight_decay: 0.0462 (0.0479)  time: 2.3489  data: 0.0053  max mem: 1586
Epoch: [0]  [ 50/176]  eta: 0:05:05  lr: 0.000023  min_lr: 0.000023  loss: 4.6998 (4.7338)  class_acc: 0.0352 (0.0369)  weight_decay: 0.0435 (0.0467)  time: 2.3461  data: 0.0046  max mem: 1586
Epoch: [0]  [ 60/176]  eta: 0:04:39  lr: 0.000027  min_lr: 0.000027  loss: 4.6894 (4.7222)  class_acc: 0.0391 (0.0386)  weight_decay: 0.0402 (0.0454)  time: 2.3427  data: 0.0035  max mem: 1586
Epoch: [0]  [ 70/176]  eta: 0:04:14  lr: 0.000032  min_lr: 0.000032  loss: 4.6676 (4.7151)  class_acc: 0.0469 (0.0398)  weight_decay: 0.0364 (0.0439)  time: 2.3423  data: 0.0035  max mem: 1586
Epoch: [0]  [ 80/176]  eta: 0:03:49  lr: 0.000037  min_lr: 0.000037  loss: 4.5989 (4.7005)  class_acc: 0.0352 (0.0393)  weight_decay: 0.0323 (0.0422)  time: 2.3419  data: 0.0033  max mem: 1586
Epoch: [0]  [ 90/176]  eta: 0:03:25  lr: 0.000041  min_lr: 0.000041  loss: 4.5753 (4.6850)  class_acc: 0.0352 (0.0396)  weight_decay: 0.0279 (0.0404)  time: 2.3425  data: 0.0031  max mem: 1586
Epoch: [0]  [100/176]  eta: 0:03:01  lr: 0.000046  min_lr: 0.000046  loss: 4.5496 (4.6707)  class_acc: 0.0430 (0.0397)  weight_decay: 0.0234 (0.0385)  time: 2.3445  data: 0.0041  max mem: 1586
Epoch: [0]  [110/176]  eta: 0:02:37  lr: 0.000050  min_lr: 0.000050  loss: 4.5275 (4.6584)  class_acc: 0.0430 (0.0397)  weight_decay: 0.0190 (0.0366)  time: 2.3615  data: 0.0101  max mem: 1586
Epoch: [0]  [120/176]  eta: 0:02:13  lr: 0.000055  min_lr: 0.000055  loss: 4.5269 (4.6465)  class_acc: 0.0352 (0.0397)  weight_decay: 0.0148 (0.0347)  time: 2.3595  data: 0.0093  max mem: 1586
Epoch: [0]  [130/176]  eta: 0:01:49  lr: 0.000059  min_lr: 0.000059  loss: 4.5063 (4.6352)  class_acc: 0.0430 (0.0403)  weight_decay: 0.0109 (0.0327)  time: 2.3425  data: 0.0032  max mem: 1586
Epoch: [0]  [140/176]  eta: 0:01:25  lr: 0.000064  min_lr: 0.000064  loss: 4.4907 (4.6246)  class_acc: 0.0469 (0.0408)  weight_decay: 0.0074 (0.0308)  time: 2.3452  data: 0.0043  max mem: 1586
Epoch: [0]  [150/176]  eta: 0:01:01  lr: 0.000069  min_lr: 0.000069  loss: 4.5040 (4.6160)  class_acc: 0.0469 (0.0411)  weight_decay: 0.0045 (0.0290)  time: 2.3439  data: 0.0042  max mem: 1586
Epoch: [0]  [160/176]  eta: 0:00:37  lr: 0.000073  min_lr: 0.000073  loss: 4.4683 (4.6065)  class_acc: 0.0469 (0.0418)  weight_decay: 0.0023 (0.0273)  time: 2.3447  data: 0.0052  max mem: 1586
Epoch: [0]  [170/176]  eta: 0:00:14  lr: 0.000078  min_lr: 0.000078  loss: 4.4640 (4.5984)  class_acc: 0.0508 (0.0421)  weight_decay: 0.0008 (0.0257)  time: 2.3460  data: 0.0060  max mem: 1586
Epoch: [0]  [175/176]  eta: 0:00:02  lr: 0.000080  min_lr: 0.000080  loss: 4.4680 (4.5953)  class_acc: 0.0469 (0.0422)  weight_decay: 0.0004 (0.0251)  time: 2.2285  data: 0.0046  max mem: 1586
Epoch: [0] Total time: 0:06:54 (2.3579 s / it)
Averaged stats: lr: 0.000080  min_lr: 0.000080  loss: 4.4680 (4.5953)  class_acc: 0.0469 (0.0422)  weight_decay: 0.0004 (0.0251)
Test:  [  0/176]  eta: 0:09:07  loss: 4.4061 (4.4061)  acc1: 4.6875 (4.6875)  acc5: 19.1406 (19.1406)  time: 3.1090  data: 2.9015  max mem: 1586
Test:  [ 10/176]  eta: 0:01:41  loss: 4.4391 (4.4540)  acc1: 4.6875 (4.6875)  acc5: 17.5781 (17.0455)  time: 0.6118  data: 0.3307  max mem: 1586
Test:  [ 20/176]  eta: 0:01:14  loss: 4.4594 (4.4601)  acc1: 4.2969 (4.4271)  acc5: 16.7969 (16.8527)  time: 0.3460  data: 0.0534  max mem: 1586
Test:  [ 30/176]  eta: 0:01:01  loss: 4.4487 (4.4571)  acc1: 4.2969 (4.5489)  acc5: 17.1875 (17.0489)  time: 0.3194  data: 0.0218  max mem: 1586
Test:  [ 40/176]  eta: 0:00:53  loss: 4.4487 (4.4580)  acc1: 4.6875 (4.6113)  acc5: 16.4062 (16.9112)  time: 0.3074  data: 0.0072  max mem: 1586
Test:  [ 50/176]  eta: 0:00:47  loss: 4.4501 (4.4591)  acc1: 4.6875 (4.5726)  acc5: 16.0156 (16.8658)  time: 0.3060  data: 0.0047  max mem: 1586
Test:  [ 60/176]  eta: 0:00:42  loss: 4.4652 (4.4602)  acc1: 4.2969 (4.5146)  acc5: 15.6250 (16.6112)  time: 0.3088  data: 0.0064  max mem: 1586
Test:  [ 70/176]  eta: 0:00:38  loss: 4.4471 (4.4581)  acc1: 4.2969 (4.4949)  acc5: 16.0156 (16.7143)  time: 0.3185  data: 0.0145  max mem: 1586
Test:  [ 80/176]  eta: 0:00:34  loss: 4.4448 (4.4586)  acc1: 4.6875 (4.6152)  acc5: 16.7969 (16.6136)  time: 0.3320  data: 0.0234  max mem: 1586
Test:  [ 90/176]  eta: 0:00:30  loss: 4.4583 (4.4580)  acc1: 4.6875 (4.5845)  acc5: 15.2344 (16.4578)  time: 0.3530  data: 0.0360  max mem: 1586
Test:  [100/176]  eta: 0:00:27  loss: 4.4669 (4.4573)  acc1: 4.2969 (4.5908)  acc5: 15.6250 (16.5261)  time: 0.3541  data: 0.0379  max mem: 1586
Test:  [110/176]  eta: 0:00:23  loss: 4.4395 (4.4561)  acc1: 4.6875 (4.6242)  acc5: 17.5781 (16.6948)  time: 0.3302  data: 0.0217  max mem: 1586
Test:  [120/176]  eta: 0:00:20  loss: 4.4471 (4.4569)  acc1: 4.2969 (4.5939)  acc5: 16.7969 (16.6258)  time: 0.3770  data: 0.0107  max mem: 1586
Test:  [130/176]  eta: 0:00:17  loss: 4.4480 (4.4551)  acc1: 4.2969 (4.5921)  acc5: 15.2344 (16.6657)  time: 0.5231  data: 0.0056  max mem: 1586
Test:  [140/176]  eta: 0:00:13  loss: 4.4471 (4.4548)  acc1: 4.2969 (4.5822)  acc5: 17.1875 (16.7221)  time: 0.4968  data: 0.0194  max mem: 1586
Test:  [150/176]  eta: 0:00:09  loss: 4.4497 (4.4550)  acc1: 3.9062 (4.5814)  acc5: 17.5781 (16.7296)  time: 0.3916  data: 0.0476  max mem: 1586
Test:  [160/176]  eta: 0:00:06  loss: 4.4447 (4.4548)  acc1: 4.2969 (4.6074)  acc5: 16.7969 (16.7459)  time: 0.3679  data: 0.0540  max mem: 1586
Test:  [170/176]  eta: 0:00:02  loss: 4.4303 (4.4533)  acc1: 4.6875 (4.6075)  acc5: 17.1875 (16.7900)  time: 0.3197  data: 0.0260  max mem: 1586
Test:  [175/176]  eta: 0:00:00  loss: 4.4391 (4.4543)  acc1: 4.2969 (4.6089)  acc5: 16.4062 (16.7644)  time: 0.3039  data: 0.0043  max mem: 1586
Test: Total time: 0:01:05 (0.3748 s / it)
* Acc@1 4.609 Acc@5 16.764 loss 4.454
Test:  [ 0/59]  eta: 0:03:46  loss: 3.8213 (3.8213)  acc1: 14.8438 (14.8438)  acc5: 45.3125 (45.3125)  time: 3.8341  data: 3.6150  max mem: 1586
Test:  [10/59]  eta: 0:00:31  loss: 4.3002 (4.2295)  acc1: 2.3438 (5.0071)  acc5: 18.3594 (22.6918)  time: 0.6472  data: 0.3418  max mem: 1586
Test:  [20/59]  eta: 0:00:19  loss: 4.2458 (4.2171)  acc1: 3.1250 (7.3475)  acc5: 20.7031 (24.7954)  time: 0.3363  data: 0.0212  max mem: 1586
Test:  [30/59]  eta: 0:00:13  loss: 4.2368 (4.2280)  acc1: 5.0781 (7.7117)  acc5: 23.4375 (24.2566)  time: 0.3518  data: 0.0316  max mem: 1586
Test:  [40/59]  eta: 0:00:08  loss: 4.1647 (4.1693)  acc1: 5.4688 (9.4417)  acc5: 23.4375 (27.7153)  time: 0.3423  data: 0.0269  max mem: 1586
Test:  [50/59]  eta: 0:00:04  loss: 4.0234 (4.1783)  acc1: 5.4688 (8.5401)  acc5: 36.3281 (27.4050)  time: 0.4701  data: 0.0125  max mem: 1586
Test:  [58/59]  eta: 0:00:00  loss: 4.1970 (4.1740)  acc1: 2.7344 (8.4263)  acc5: 23.4375 (27.6445)  time: 0.5690  data: 0.0065  max mem: 1586
Test: Total time: 0:00:28 (0.4830 s / it)
* Acc@1 8.426 Acc@5 27.645 loss 4.174
**************Prune Round 3**********************

Current sparsity level: 1.0
block 0
block 1
block 2
block 3
Actual sparsity after pruning: 1.0
Epoch: [0]  [  0/176]  eta: 0:17:44  lr: 0.000000  min_lr: 0.000000  loss: 4.4329 (4.4329)  class_acc: 0.0352 (0.0352)  weight_decay: 0.0500 (0.0500)  time: 6.0488  data: 3.9013  max mem: 1586
Epoch: [0]  [ 10/176]  eta: 0:07:29  lr: 0.000005  min_lr: 0.000005  loss: 4.4588 (4.4629)  class_acc: 0.0352 (0.0391)  weight_decay: 0.0499 (0.0499)  time: 2.7078  data: 0.3596  max mem: 1586
Epoch: [0]  [ 20/176]  eta: 0:06:35  lr: 0.000009  min_lr: 0.000009  loss: 4.4674 (4.4668)  class_acc: 0.0430 (0.0432)  weight_decay: 0.0495 (0.0495)  time: 2.3568  data: 0.0038  max mem: 1586
Epoch: [0]  [ 30/176]  eta: 0:06:00  lr: 0.000014  min_lr: 0.000014  loss: 4.4674 (4.4612)  class_acc: 0.0508 (0.0455)  weight_decay: 0.0482 (0.0488)  time: 2.3378  data: 0.0020  max mem: 1586
Epoch: [0]  [ 40/176]  eta: 0:05:31  lr: 0.000018  min_lr: 0.000018  loss: 4.4548 (4.4599)  class_acc: 0.0508 (0.0467)  weight_decay: 0.0462 (0.0479)  time: 2.3360  data: 0.0020  max mem: 1586
Epoch: [0]  [ 50/176]  eta: 0:05:04  lr: 0.000023  min_lr: 0.000023  loss: 4.4303 (4.4546)  class_acc: 0.0547 (0.0480)  weight_decay: 0.0435 (0.0467)  time: 2.3356  data: 0.0019  max mem: 1586
Epoch: [0]  [ 60/176]  eta: 0:04:38  lr: 0.000027  min_lr: 0.000027  loss: 4.4155 (4.4460)  class_acc: 0.0469 (0.0476)  weight_decay: 0.0402 (0.0454)  time: 2.3348  data: 0.0017  max mem: 1586
Epoch: [0]  [ 70/176]  eta: 0:04:13  lr: 0.000032  min_lr: 0.000032  loss: 4.4111 (4.4460)  class_acc: 0.0469 (0.0485)  weight_decay: 0.0364 (0.0439)  time: 2.3348  data: 0.0018  max mem: 1586
Epoch: [0]  [ 80/176]  eta: 0:03:49  lr: 0.000037  min_lr: 0.000037  loss: 4.4255 (4.4421)  class_acc: 0.0508 (0.0486)  weight_decay: 0.0323 (0.0422)  time: 2.3351  data: 0.0018  max mem: 1586
Epoch: [0]  [ 90/176]  eta: 0:03:24  lr: 0.000041  min_lr: 0.000041  loss: 4.4191 (4.4417)  class_acc: 0.0508 (0.0494)  weight_decay: 0.0279 (0.0404)  time: 2.3355  data: 0.0019  max mem: 1586
Epoch: [0]  [100/176]  eta: 0:03:00  lr: 0.000046  min_lr: 0.000046  loss: 4.4223 (4.4395)  class_acc: 0.0469 (0.0491)  weight_decay: 0.0234 (0.0385)  time: 2.3358  data: 0.0020  max mem: 1586
Epoch: [0]  [110/176]  eta: 0:02:36  lr: 0.000050  min_lr: 0.000050  loss: 4.4223 (4.4387)  class_acc: 0.0469 (0.0495)  weight_decay: 0.0190 (0.0366)  time: 2.3365  data: 0.0020  max mem: 1586
Epoch: [0]  [120/176]  eta: 0:02:12  lr: 0.000055  min_lr: 0.000055  loss: 4.4174 (4.4358)  class_acc: 0.0547 (0.0500)  weight_decay: 0.0148 (0.0347)  time: 2.3364  data: 0.0019  max mem: 1586
Epoch: [0]  [130/176]  eta: 0:01:48  lr: 0.000059  min_lr: 0.000059  loss: 4.4119 (4.4349)  class_acc: 0.0547 (0.0505)  weight_decay: 0.0109 (0.0327)  time: 2.3365  data: 0.0018  max mem: 1586
Epoch: [0]  [140/176]  eta: 0:01:25  lr: 0.000064  min_lr: 0.000064  loss: 4.4119 (4.4335)  class_acc: 0.0508 (0.0503)  weight_decay: 0.0074 (0.0308)  time: 2.3364  data: 0.0017  max mem: 1586
Epoch: [0]  [150/176]  eta: 0:01:01  lr: 0.000069  min_lr: 0.000069  loss: 4.4054 (4.4310)  class_acc: 0.0508 (0.0509)  weight_decay: 0.0045 (0.0290)  time: 2.3354  data: 0.0017  max mem: 1586
Epoch: [0]  [160/176]  eta: 0:00:37  lr: 0.000073  min_lr: 0.000073  loss: 4.3959 (4.4288)  class_acc: 0.0586 (0.0512)  weight_decay: 0.0023 (0.0273)  time: 2.3355  data: 0.0018  max mem: 1586
Epoch: [0]  [170/176]  eta: 0:00:14  lr: 0.000078  min_lr: 0.000078  loss: 4.3952 (4.4278)  class_acc: 0.0508 (0.0514)  weight_decay: 0.0008 (0.0257)  time: 2.3353  data: 0.0018  max mem: 1586
Epoch: [0]  [175/176]  eta: 0:00:02  lr: 0.000080  min_lr: 0.000080  loss: 4.3885 (4.4269)  class_acc: 0.0508 (0.0517)  weight_decay: 0.0004 (0.0251)  time: 2.2185  data: 0.0017  max mem: 1586
Epoch: [0] Total time: 0:06:53 (2.3473 s / it)
Averaged stats: lr: 0.000080  min_lr: 0.000080  loss: 4.3885 (4.4269)  class_acc: 0.0508 (0.0517)  weight_decay: 0.0004 (0.0251)
Test:  [  0/176]  eta: 0:06:31  loss: 4.5326 (4.5326)  acc1: 4.2969 (4.2969)  acc5: 16.4062 (16.4062)  time: 2.2237  data: 2.0071  max mem: 1586
Test:  [ 10/176]  eta: 0:02:07  loss: 4.4165 (4.4149)  acc1: 5.8594 (5.5753)  acc5: 19.5312 (18.1818)  time: 0.7698  data: 0.2963  max mem: 1586
Test:  [ 20/176]  eta: 0:01:37  loss: 4.4117 (4.4138)  acc1: 5.4688 (5.3013)  acc5: 17.5781 (17.7083)  time: 0.5458  data: 0.0641  max mem: 1586
Test:  [ 30/176]  eta: 0:01:16  loss: 4.4117 (4.4121)  acc1: 5.4688 (5.5192)  acc5: 17.1875 (17.7167)  time: 0.3861  data: 0.0031  max mem: 1586
Test:  [ 40/176]  eta: 0:01:03  loss: 4.4004 (4.4111)  acc1: 5.4688 (5.4402)  acc5: 17.5781 (17.5972)  time: 0.3056  data: 0.0038  max mem: 1586
Test:  [ 50/176]  eta: 0:00:55  loss: 4.4082 (4.4148)  acc1: 5.0781 (5.2926)  acc5: 17.5781 (17.5628)  time: 0.3071  data: 0.0053  max mem: 1586
Test:  [ 60/176]  eta: 0:00:52  loss: 4.4180 (4.4137)  acc1: 5.0781 (5.3663)  acc5: 17.9688 (17.5525)  time: 0.4300  data: 0.0055  max mem: 1586
Test:  [ 70/176]  eta: 0:00:50  loss: 4.3887 (4.4092)  acc1: 5.4688 (5.4632)  acc5: 19.1406 (17.8477)  time: 0.5804  data: 0.0038  max mem: 1586
Test:  [ 80/176]  eta: 0:00:44  loss: 4.3893 (4.4094)  acc1: 5.4688 (5.4495)  acc5: 19.1406 (17.8096)  time: 0.4772  data: 0.0037  max mem: 1586
Test:  [ 90/176]  eta: 0:00:38  loss: 4.3905 (4.4070)  acc1: 5.4688 (5.4430)  acc5: 17.9688 (17.8486)  time: 0.3265  data: 0.0049  max mem: 1586
Test:  [100/176]  eta: 0:00:32  loss: 4.3905 (4.4084)  acc1: 5.0781 (5.4417)  acc5: 17.9688 (17.8218)  time: 0.3065  data: 0.0049  max mem: 1586
Test:  [110/176]  eta: 0:00:28  loss: 4.4096 (4.4077)  acc1: 4.6875 (5.3948)  acc5: 17.9688 (17.9265)  time: 0.3975  data: 0.0037  max mem: 1586
Test:  [120/176]  eta: 0:00:25  loss: 4.4096 (4.4092)  acc1: 4.6875 (5.3816)  acc5: 18.3594 (17.9203)  time: 0.5495  data: 0.0030  max mem: 1586
Test:  [130/176]  eta: 0:00:20  loss: 4.4121 (4.4100)  acc1: 5.8594 (5.4479)  acc5: 18.3594 (17.9419)  time: 0.4772  data: 0.0035  max mem: 1586
Test:  [140/176]  eta: 0:00:15  loss: 4.3993 (4.4082)  acc1: 5.8594 (5.4383)  acc5: 17.9688 (17.9244)  time: 0.3261  data: 0.0042  max mem: 1586
Test:  [150/176]  eta: 0:00:11  loss: 4.3897 (4.4056)  acc1: 5.0781 (5.4584)  acc5: 17.9688 (17.9455)  time: 0.3079  data: 0.0052  max mem: 1586
Test:  [160/176]  eta: 0:00:06  loss: 4.3897 (4.4053)  acc1: 5.4688 (5.4639)  acc5: 17.9688 (17.9760)  time: 0.3530  data: 0.0059  max mem: 1586
Test:  [170/176]  eta: 0:00:02  loss: 4.4258 (4.4077)  acc1: 5.4688 (5.4413)  acc5: 17.5781 (17.8751)  time: 0.5030  data: 0.0041  max mem: 1586
Test:  [175/176]  eta: 0:00:00  loss: 4.4250 (4.4063)  acc1: 5.4688 (5.4467)  acc5: 17.5781 (17.9356)  time: 0.5368  data: 0.0026  max mem: 1586
Test: Total time: 0:01:16 (0.4354 s / it)
* Acc@1 5.447 Acc@5 17.936 loss 4.406
Test:  [ 0/59]  eta: 0:01:40  loss: 3.6002 (3.6002)  acc1: 30.4688 (30.4688)  acc5: 51.1719 (51.1719)  time: 1.7067  data: 1.4988  max mem: 1586
Test:  [10/59]  eta: 0:00:31  loss: 4.2590 (4.1782)  acc1: 4.2969 (6.2145)  acc5: 23.0469 (23.4020)  time: 0.6390  data: 0.1603  max mem: 1586
Test:  [20/59]  eta: 0:00:18  loss: 4.2465 (4.1693)  acc1: 4.2969 (9.0960)  acc5: 19.9219 (25.0558)  time: 0.4183  data: 0.0145  max mem: 1586
Test:  [30/59]  eta: 0:00:12  loss: 4.1093 (4.1443)  acc1: 7.0312 (10.0428)  acc5: 25.3906 (25.8443)  time: 0.3040  data: 0.0027  max mem: 1586
Test:  [40/59]  eta: 0:00:07  loss: 4.0886 (4.1006)  acc1: 6.6406 (11.2138)  acc5: 28.9062 (28.3441)  time: 0.3038  data: 0.0029  max mem: 1586
Test:  [50/59]  eta: 0:00:03  loss: 3.9925 (4.0990)  acc1: 6.6406 (10.8609)  acc5: 37.5000 (28.9828)  time: 0.3669  data: 0.0025  max mem: 1586
Test:  [58/59]  eta: 0:00:00  loss: 4.0886 (4.1010)  acc1: 3.1250 (10.0234)  acc5: 31.2500 (29.1413)  time: 0.4727  data: 0.0022  max mem: 1586
Test: Total time: 0:00:25 (0.4260 s / it)
* Acc@1 10.023 Acc@5 29.141 loss 4.101
**************Prune Round 4**********************

Current sparsity level: 1.0
block 0
block 1
block 2
block 3
Actual sparsity after pruning: 1.0
Epoch: [0]  [  0/176]  eta: 0:11:41  lr: 0.000000  min_lr: 0.000000  loss: 4.3757 (4.3757)  class_acc: 0.0586 (0.0586)  weight_decay: 0.0500 (0.0500)  time: 3.9878  data: 2.1507  max mem: 1586
Epoch: [0]  [ 10/176]  eta: 0:06:53  lr: 0.000005  min_lr: 0.000005  loss: 4.3663 (4.3800)  class_acc: 0.0586 (0.0568)  weight_decay: 0.0499 (0.0499)  time: 2.4911  data: 0.2015  max mem: 1586
Epoch: [0]  [ 20/176]  eta: 0:06:17  lr: 0.000009  min_lr: 0.000009  loss: 4.3767 (4.3868)  class_acc: 0.0508 (0.0556)  weight_decay: 0.0495 (0.0495)  time: 2.3417  data: 0.0047  max mem: 1586
Epoch: [0]  [ 30/176]  eta: 0:05:49  lr: 0.000014  min_lr: 0.000014  loss: 4.3644 (4.3831)  class_acc: 0.0508 (0.0562)  weight_decay: 0.0482 (0.0488)  time: 2.3424  data: 0.0030  max mem: 1586
Epoch: [0]  [ 40/176]  eta: 0:05:24  lr: 0.000018  min_lr: 0.000018  loss: 4.3624 (4.3769)  class_acc: 0.0625 (0.0581)  weight_decay: 0.0462 (0.0479)  time: 2.3576  data: 0.0073  max mem: 1586
Epoch: [0]  [ 50/176]  eta: 0:05:00  lr: 0.000023  min_lr: 0.000023  loss: 4.3656 (4.3799)  class_acc: 0.0586 (0.0583)  weight_decay: 0.0435 (0.0467)  time: 2.3654  data: 0.0096  max mem: 1586
Epoch: [0]  [ 60/176]  eta: 0:04:35  lr: 0.000027  min_lr: 0.000027  loss: 4.3903 (4.3801)  class_acc: 0.0547 (0.0583)  weight_decay: 0.0402 (0.0454)  time: 2.3502  data: 0.0055  max mem: 1586
Epoch: [0]  [ 70/176]  eta: 0:04:11  lr: 0.000032  min_lr: 0.000032  loss: 4.3849 (4.3792)  class_acc: 0.0547 (0.0585)  weight_decay: 0.0364 (0.0439)  time: 2.3444  data: 0.0036  max mem: 1586
Epoch: [0]  [ 80/176]  eta: 0:03:47  lr: 0.000037  min_lr: 0.000037  loss: 4.3849 (4.3817)  class_acc: 0.0625 (0.0593)  weight_decay: 0.0323 (0.0422)  time: 2.3546  data: 0.0058  max mem: 1586
Epoch: [0]  [ 90/176]  eta: 0:03:23  lr: 0.000041  min_lr: 0.000041  loss: 4.3875 (4.3826)  class_acc: 0.0586 (0.0586)  weight_decay: 0.0279 (0.0404)  time: 2.3513  data: 0.0049  max mem: 1586
Epoch: [0]  [100/176]  eta: 0:02:59  lr: 0.000046  min_lr: 0.000046  loss: 4.3927 (4.3868)  class_acc: 0.0508 (0.0579)  weight_decay: 0.0234 (0.0385)  time: 2.3476  data: 0.0039  max mem: 1586
Traceback (most recent call last):
  File "/media/shahanahmed/b8c6fb5d-b937-4730-bb0f-ac0eba675d7e/Wanda_pruning_for_image_classification/main.py", line 533, in <module>
    main(args)
  File "/media/shahanahmed/b8c6fb5d-b937-4730-bb0f-ac0eba675d7e/Wanda_pruning_for_image_classification/main.py", line 526, in main
    train_with_pruning(model,dataset_train, dataset_val,device,args)
  File "/media/shahanahmed/b8c6fb5d-b937-4730-bb0f-ac0eba675d7e/Wanda_pruning_for_image_classification/main.py", line 357, in train_with_pruning
    train_stats = train_one_epoch(
  File "/media/shahanahmed/b8c6fb5d-b937-4730-bb0f-ac0eba675d7e/Wanda_pruning_for_image_classification/engine.py", line 106, in train_one_epoch
    torch.cuda.synchronize()
  File "/home/shahanahmed/.local/lib/python3.10/site-packages/torch/cuda/__init__.py", line 892, in synchronize
    return torch._C._cuda_synchronize()
KeyboardInterrupt
Traceback (most recent call last):
  File "/media/shahanahmed/b8c6fb5d-b937-4730-bb0f-ac0eba675d7e/Wanda_pruning_for_image_classification/main.py", line 533, in <module>
    main(args)
  File "/media/shahanahmed/b8c6fb5d-b937-4730-bb0f-ac0eba675d7e/Wanda_pruning_for_image_classification/main.py", line 526, in main
    train_with_pruning(model,dataset_train, dataset_val,device,args)
  File "/media/shahanahmed/b8c6fb5d-b937-4730-bb0f-ac0eba675d7e/Wanda_pruning_for_image_classification/main.py", line 357, in train_with_pruning
    train_stats = train_one_epoch(
  File "/media/shahanahmed/b8c6fb5d-b937-4730-bb0f-ac0eba675d7e/Wanda_pruning_for_image_classification/engine.py", line 106, in train_one_epoch
    torch.cuda.synchronize()
  File "/home/shahanahmed/.local/lib/python3.10/site-packages/torch/cuda/__init__.py", line 892, in synchronize
    return torch._C._cuda_synchronize()
KeyboardInterrupt
