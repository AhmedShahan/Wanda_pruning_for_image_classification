Set warmup steps = 8750
Set warmup steps = 0
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.dwconv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.dwconv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.dwconv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.dwconv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.dwconv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.dwconv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.dwconv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.dwconv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.dwconv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.dwconv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.dwconv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.dwconv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.dwconv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.dwconv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.dwconv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.dwconv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.dwconv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.dwconv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.dwconv.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.dwconv.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.dwconv.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.dwconv.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.dwconv.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.dwconv.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.dwconv.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.dwconv.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.dwconv.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.dwconv.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.dwconv.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.dwconv.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.dwconv.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.dwconv.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.dwconv.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.dwconv.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.dwconv.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.dwconv.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
/home/shahanahmed/.local/lib/python3.10/site-packages/timm/utils/cuda.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
**************Prune Round 1**********************

Current sparsity level: 0.00176105406222953
Actual sparsity after pruning: 0.00176105406222953
/home/shahanahmed/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test:  [  0/176]  eta: 0:13:02  loss: 7.0340 (7.0340)  acc1: 0.0000 (0.0000)  acc5: 0.3906 (0.3906)  time: 4.4468  data: 1.8749  max mem: 1095
Test:  [ 10/176]  eta: 0:01:52  loss: 7.0729 (7.0739)  acc1: 0.0000 (0.0355)  acc5: 0.3906 (0.3551)  time: 0.6805  data: 0.1728  max mem: 1095
Test:  [ 20/176]  eta: 0:01:18  loss: 7.0729 (7.0724)  acc1: 0.0000 (0.0372)  acc5: 0.3906 (0.3906)  time: 0.3029  data: 0.0018  max mem: 1095
Test:  [ 30/176]  eta: 0:01:09  loss: 7.0671 (7.0676)  acc1: 0.0000 (0.0378)  acc5: 0.3906 (0.3528)  time: 0.3618  data: 0.0022  max mem: 1095
Test:  [ 40/176]  eta: 0:01:00  loss: 7.0543 (7.0598)  acc1: 0.0000 (0.0381)  acc5: 0.3906 (0.3811)  time: 0.3848  data: 0.0033  max mem: 1095
Test:  [ 50/176]  eta: 0:00:55  loss: 7.0481 (7.0609)  acc1: 0.0000 (0.0460)  acc5: 0.3906 (0.3906)  time: 0.3951  data: 0.0026  max mem: 1095
Test:  [ 60/176]  eta: 0:00:51  loss: 7.0592 (7.0587)  acc1: 0.0000 (0.0384)  acc5: 0.3906 (0.3778)  time: 0.4516  data: 0.0039  max mem: 1095
Test:  [ 70/176]  eta: 0:00:47  loss: 7.0512 (7.0571)  acc1: 0.0000 (0.0330)  acc5: 0.3906 (0.3741)  time: 0.4450  data: 0.0046  max mem: 1095
Test:  [ 80/176]  eta: 0:00:42  loss: 7.0489 (7.0562)  acc1: 0.0000 (0.0434)  acc5: 0.3906 (0.4051)  time: 0.4473  data: 0.0029  max mem: 1095
Test:  [ 90/176]  eta: 0:00:39  loss: 7.0506 (7.0553)  acc1: 0.0000 (0.0429)  acc5: 0.3906 (0.4164)  time: 0.5167  data: 0.0023  max mem: 1095
Test:  [100/176]  eta: 0:00:34  loss: 7.0570 (7.0544)  acc1: 0.0000 (0.0464)  acc5: 0.3906 (0.4177)  time: 0.4890  data: 0.0028  max mem: 1095
Test:  [110/176]  eta: 0:00:30  loss: 7.0378 (7.0528)  acc1: 0.0000 (0.0528)  acc5: 0.3906 (0.4329)  time: 0.5087  data: 0.0030  max mem: 1095
Test:  [120/176]  eta: 0:00:25  loss: 7.0305 (7.0516)  acc1: 0.0000 (0.0517)  acc5: 0.3906 (0.4358)  time: 0.4974  data: 0.0028  max mem: 1095
Test:  [130/176]  eta: 0:00:21  loss: 7.0390 (7.0509)  acc1: 0.0000 (0.0596)  acc5: 0.7812 (0.4562)  time: 0.4981  data: 0.0031  max mem: 1095
Test:  [140/176]  eta: 0:00:17  loss: 7.0396 (7.0502)  acc1: 0.0000 (0.0554)  acc5: 0.3906 (0.4405)  time: 0.6080  data: 0.0026  max mem: 1095
Test:  [150/176]  eta: 0:00:12  loss: 7.0368 (7.0492)  acc1: 0.0000 (0.0543)  acc5: 0.3906 (0.4424)  time: 0.5248  data: 0.0033  max mem: 1095
Test:  [160/176]  eta: 0:00:07  loss: 7.0366 (7.0483)  acc1: 0.0000 (0.0534)  acc5: 0.3906 (0.4367)  time: 0.4703  data: 0.0083  max mem: 1095
Test:  [170/176]  eta: 0:00:02  loss: 7.0381 (7.0476)  acc1: 0.0000 (0.0571)  acc5: 0.0000 (0.4340)  time: 0.5139  data: 0.0082  max mem: 1095
Test:  [175/176]  eta: 0:00:00  loss: 7.0333 (7.0478)  acc1: 0.0000 (0.0556)  acc5: 0.0000 (0.4311)  time: 0.5988  data: 0.0043  max mem: 1095
Test: Total time: 0:01:26 (0.4922 s / it)
* Acc@1 0.056 Acc@5 0.431 loss 7.048
Test:  [ 0/59]  eta: 0:01:54  loss: 6.9202 (6.9202)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 1.9392  data: 1.7309  max mem: 1095
Test:  [10/59]  eta: 0:00:34  loss: 7.0491 (7.0902)  acc1: 0.0000 (0.0710)  acc5: 0.3906 (0.2841)  time: 0.7004  data: 0.1615  max mem: 1095
Test:  [20/59]  eta: 0:00:24  loss: 7.0641 (7.0802)  acc1: 0.0000 (0.0372)  acc5: 0.0000 (0.2418)  time: 0.5556  data: 0.0031  max mem: 1095
Test:  [30/59]  eta: 0:00:17  loss: 7.0641 (7.0885)  acc1: 0.0000 (0.0756)  acc5: 0.0000 (0.3654)  time: 0.5262  data: 0.0018  max mem: 1095
Test:  [40/59]  eta: 0:00:11  loss: 7.0390 (7.0809)  acc1: 0.0000 (0.0857)  acc5: 0.3906 (0.4097)  time: 0.5534  data: 0.0020  max mem: 1095
Test:  [50/59]  eta: 0:00:05  loss: 7.0283 (7.0703)  acc1: 0.0000 (0.0919)  acc5: 0.3906 (0.4366)  time: 0.5081  data: 0.0026  max mem: 1095
Test:  [58/59]  eta: 0:00:00  loss: 7.0639 (7.0748)  acc1: 0.0000 (0.0802)  acc5: 0.0000 (0.3876)  time: 0.5295  data: 0.0026  max mem: 1095
Test: Total time: 0:00:33 (0.5745 s / it)
* Acc@1 0.080 Acc@5 0.388 loss 7.075
**************Prune Round 2**********************

Current sparsity level: 0.00176105406222953
Actual sparsity after pruning: 0.00176105406222953
Test:  [  0/176]  eta: 0:02:39  loss: 7.0628 (7.0628)  acc1: 0.0000 (0.0000)  acc5: 0.3906 (0.3906)  time: 0.9058  data: 0.7049  max mem: 1095
Test:  [ 10/176]  eta: 0:01:29  loss: 7.0403 (7.0548)  acc1: 0.0000 (0.0710)  acc5: 0.3906 (0.4616)  time: 0.5366  data: 0.0720  max mem: 1095
Test:  [ 20/176]  eta: 0:01:12  loss: 7.0403 (7.0505)  acc1: 0.0000 (0.1116)  acc5: 0.3906 (0.4650)  time: 0.4439  data: 0.0049  max mem: 1095
Test:  [ 30/176]  eta: 0:01:14  loss: 7.0418 (7.0494)  acc1: 0.0000 (0.0756)  acc5: 0.3906 (0.4032)  time: 0.4973  data: 0.0018  max mem: 1095
Test:  [ 40/176]  eta: 0:01:06  loss: 7.0436 (7.0519)  acc1: 0.0000 (0.0857)  acc5: 0.3906 (0.4097)  time: 0.5092  data: 0.0033  max mem: 1095
Test:  [ 50/176]  eta: 0:01:01  loss: 7.0425 (7.0464)  acc1: 0.0000 (0.0766)  acc5: 0.3906 (0.4519)  time: 0.4549  data: 0.0034  max mem: 1095
Test:  [ 60/176]  eta: 0:00:57  loss: 7.0287 (7.0452)  acc1: 0.0000 (0.0832)  acc5: 0.7812 (0.5123)  time: 0.5071  data: 0.0023  max mem: 1095
Test:  [ 70/176]  eta: 0:00:51  loss: 7.0349 (7.0447)  acc1: 0.0000 (0.0770)  acc5: 0.3906 (0.5062)  time: 0.4930  data: 0.0022  max mem: 1095
Test:  [ 80/176]  eta: 0:00:47  loss: 7.0374 (7.0454)  acc1: 0.0000 (0.0820)  acc5: 0.3906 (0.5112)  time: 0.5070  data: 0.0025  max mem: 1095
Test:  [ 90/176]  eta: 0:00:43  loss: 7.0432 (7.0465)  acc1: 0.0000 (0.0773)  acc5: 0.3906 (0.4936)  time: 0.5760  data: 0.0024  max mem: 1095
Test:  [100/176]  eta: 0:00:37  loss: 7.0484 (7.0465)  acc1: 0.0000 (0.0696)  acc5: 0.3906 (0.4873)  time: 0.5082  data: 0.0029  max mem: 1095
Test:  [110/176]  eta: 0:00:33  loss: 7.0458 (7.0464)  acc1: 0.0000 (0.0704)  acc5: 0.3906 (0.4962)  time: 0.5083  data: 0.0029  max mem: 1095
Test:  [120/176]  eta: 0:00:28  loss: 7.0202 (7.0441)  acc1: 0.0000 (0.0678)  acc5: 0.3906 (0.4939)  time: 0.6078  data: 0.0023  max mem: 1095
Test:  [130/176]  eta: 0:00:23  loss: 7.0222 (7.0436)  acc1: 0.0000 (0.0656)  acc5: 0.3906 (0.4801)  time: 0.5371  data: 0.0024  max mem: 1095
Test:  [140/176]  eta: 0:00:18  loss: 7.0366 (7.0438)  acc1: 0.0000 (0.0609)  acc5: 0.3906 (0.4737)  time: 0.5370  data: 0.0026  max mem: 1095
Test:  [150/176]  eta: 0:00:13  loss: 7.0447 (7.0448)  acc1: 0.0000 (0.0621)  acc5: 0.3906 (0.4863)  time: 0.6072  data: 0.0023  max mem: 1095
Test:  [160/176]  eta: 0:00:08  loss: 7.0434 (7.0443)  acc1: 0.0000 (0.0631)  acc5: 0.3906 (0.4852)  time: 0.5096  data: 0.0032  max mem: 1095
Test:  [170/176]  eta: 0:00:03  loss: 7.0412 (7.0440)  acc1: 0.0000 (0.0617)  acc5: 0.3906 (0.4683)  time: 0.5096  data: 0.0031  max mem: 1095
Test:  [175/176]  eta: 0:00:00  loss: 7.0337 (7.0437)  acc1: 0.0000 (0.0644)  acc5: 0.3906 (0.4800)  time: 0.5216  data: 0.0023  max mem: 1095
Test: Total time: 0:01:31 (0.5210 s / it)
* Acc@1 0.064 Acc@5 0.480 loss 7.044
Test:  [ 0/59]  eta: 0:01:29  loss: 6.9202 (6.9202)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 1.5199  data: 1.2950  max mem: 1095
Test:  [10/59]  eta: 0:00:32  loss: 7.0491 (7.0902)  acc1: 0.0000 (0.0710)  acc5: 0.3906 (0.2841)  time: 0.6608  data: 0.1191  max mem: 1095
Test:  [20/59]  eta: 0:00:23  loss: 7.0641 (7.0802)  acc1: 0.0000 (0.0372)  acc5: 0.0000 (0.2418)  time: 0.5514  data: 0.0014  max mem: 1095
Test:  [30/59]  eta: 0:00:16  loss: 7.0641 (7.0885)  acc1: 0.0000 (0.0756)  acc5: 0.0000 (0.3654)  time: 0.5366  data: 0.0019  max mem: 1095
Test:  [40/59]  eta: 0:00:10  loss: 7.0390 (7.0809)  acc1: 0.0000 (0.0857)  acc5: 0.3906 (0.4097)  time: 0.4780  data: 0.0029  max mem: 1095
Test:  [50/59]  eta: 0:00:04  loss: 7.0283 (7.0703)  acc1: 0.0000 (0.0919)  acc5: 0.3906 (0.4366)  time: 0.5086  data: 0.0024  max mem: 1095
Test:  [58/59]  eta: 0:00:00  loss: 7.0639 (7.0748)  acc1: 0.0000 (0.0802)  acc5: 0.0000 (0.3876)  time: 0.5907  data: 0.0018  max mem: 1095
Test: Total time: 0:00:32 (0.5567 s / it)
* Acc@1 0.080 Acc@5 0.388 loss 7.075
**************Prune Round 3**********************

Current sparsity level: 0.00176105406222953
Actual sparsity after pruning: 0.00176105406222953
Test:  [  0/176]  eta: 0:05:35  loss: 7.0724 (7.0724)  acc1: 0.0000 (0.0000)  acc5: 0.3906 (0.3906)  time: 1.9073  data: 1.7111  max mem: 1095
Test:  [ 10/176]  eta: 0:01:55  loss: 7.0457 (7.0503)  acc1: 0.0000 (0.0710)  acc5: 0.3906 (0.2841)  time: 0.6936  data: 0.1608  max mem: 1095
Test:  [ 20/176]  eta: 0:01:37  loss: 7.0417 (7.0482)  acc1: 0.0000 (0.0930)  acc5: 0.3906 (0.3906)  time: 0.5611  data: 0.0035  max mem: 1095
Test:  [ 30/176]  eta: 0:01:21  loss: 7.0461 (7.0459)  acc1: 0.0000 (0.0882)  acc5: 0.3906 (0.3906)  time: 0.4870  data: 0.0020  max mem: 1095
Test:  [ 40/176]  eta: 0:01:17  loss: 7.0471 (7.0466)  acc1: 0.0000 (0.0667)  acc5: 0.3906 (0.3906)  time: 0.5156  data: 0.0025  max mem: 1095
Test:  [ 50/176]  eta: 0:01:10  loss: 7.0458 (7.0448)  acc1: 0.0000 (0.0613)  acc5: 0.3906 (0.3753)  time: 0.5533  data: 0.0023  max mem: 1095
Test:  [ 60/176]  eta: 0:01:03  loss: 7.0458 (7.0475)  acc1: 0.0000 (0.0640)  acc5: 0.3906 (0.3650)  time: 0.5077  data: 0.0024  max mem: 1095
Test:  [ 70/176]  eta: 0:00:59  loss: 7.0389 (7.0455)  acc1: 0.0000 (0.0660)  acc5: 0.3906 (0.3906)  time: 0.5617  data: 0.0025  max mem: 1095
Test:  [ 80/176]  eta: 0:00:53  loss: 7.0389 (7.0455)  acc1: 0.0000 (0.0675)  acc5: 0.3906 (0.3858)  time: 0.5678  data: 0.0025  max mem: 1095
Test:  [ 90/176]  eta: 0:00:47  loss: 7.0505 (7.0468)  acc1: 0.0000 (0.0730)  acc5: 0.3906 (0.3863)  time: 0.5086  data: 0.0028  max mem: 1095
Test:  [100/176]  eta: 0:00:42  loss: 7.0604 (7.0482)  acc1: 0.0000 (0.0774)  acc5: 0.3906 (0.3713)  time: 0.5482  data: 0.0026  max mem: 1095
Test:  [110/176]  eta: 0:00:35  loss: 7.0614 (7.0488)  acc1: 0.0000 (0.0739)  acc5: 0.3906 (0.3765)  time: 0.5187  data: 0.0029  max mem: 1095
Test:  [120/176]  eta: 0:00:30  loss: 7.0399 (7.0488)  acc1: 0.0000 (0.0678)  acc5: 0.3906 (0.3680)  time: 0.5188  data: 0.0030  max mem: 1095
Test:  [130/176]  eta: 0:00:24  loss: 7.0403 (7.0486)  acc1: 0.0000 (0.0656)  acc5: 0.3906 (0.3698)  time: 0.5284  data: 0.0027  max mem: 1095
Test:  [140/176]  eta: 0:00:19  loss: 7.0478 (7.0493)  acc1: 0.0000 (0.0637)  acc5: 0.3906 (0.3768)  time: 0.5283  data: 0.0027  max mem: 1095
Test:  [150/176]  eta: 0:00:14  loss: 7.0478 (7.0492)  acc1: 0.0000 (0.0647)  acc5: 0.3906 (0.3699)  time: 0.5829  data: 0.0023  max mem: 1095
Test:  [160/176]  eta: 0:00:08  loss: 7.0499 (7.0498)  acc1: 0.0000 (0.0728)  acc5: 0.3906 (0.3736)  time: 0.4903  data: 0.0034  max mem: 1095
Test:  [170/176]  eta: 0:00:03  loss: 7.0505 (7.0496)  acc1: 0.0000 (0.0754)  acc5: 0.3906 (0.3792)  time: 0.5143  data: 0.0032  max mem: 1095
Test:  [175/176]  eta: 0:00:00  loss: 7.0505 (7.0489)  acc1: 0.0000 (0.0756)  acc5: 0.3906 (0.3800)  time: 0.5842  data: 0.0020  max mem: 1095
Test: Total time: 0:01:35 (0.5443 s / it)
* Acc@1 0.076 Acc@5 0.380 loss 7.049
Test:  [ 0/59]  eta: 0:01:05  loss: 6.9202 (6.9202)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 1.1056  data: 0.8882  max mem: 1095
Test:  [10/59]  eta: 0:00:29  loss: 7.0491 (7.0902)  acc1: 0.0000 (0.0710)  acc5: 0.3906 (0.2841)  time: 0.6052  data: 0.0933  max mem: 1095
Test:  [20/59]  eta: 0:00:20  loss: 7.0641 (7.0802)  acc1: 0.0000 (0.0372)  acc5: 0.0000 (0.2418)  time: 0.5101  data: 0.0077  max mem: 1095
Test:  [30/59]  eta: 0:00:15  loss: 7.0641 (7.0885)  acc1: 0.0000 (0.0756)  acc5: 0.0000 (0.3654)  time: 0.5104  data: 0.0018  max mem: 1095
Test:  [40/59]  eta: 0:00:10  loss: 7.0390 (7.0809)  acc1: 0.0000 (0.0857)  acc5: 0.3906 (0.4097)  time: 0.5368  data: 0.0021  max mem: 1095
Test:  [50/59]  eta: 0:00:04  loss: 7.0283 (7.0703)  acc1: 0.0000 (0.0919)  acc5: 0.3906 (0.4366)  time: 0.5626  data: 0.0022  max mem: 1095
Test:  [58/59]  eta: 0:00:00  loss: 7.0639 (7.0748)  acc1: 0.0000 (0.0802)  acc5: 0.0000 (0.3876)  time: 0.5020  data: 0.0026  max mem: 1095
Test: Total time: 0:00:31 (0.5260 s / it)
* Acc@1 0.080 Acc@5 0.388 loss 7.075
**************Prune Round 4**********************

Current sparsity level: 0.00176105406222953
Actual sparsity after pruning: 0.00176105406222953
Test:  [  0/176]  eta: 0:06:24  loss: 6.9916 (6.9916)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 2.1875  data: 1.9815  max mem: 1095
Test:  [ 10/176]  eta: 0:01:53  loss: 7.0381 (7.0466)  acc1: 0.0000 (0.0710)  acc5: 0.3906 (0.2841)  time: 0.6809  data: 0.1816  max mem: 1095
Test:  [ 20/176]  eta: 0:01:26  loss: 7.0638 (7.0583)  acc1: 0.0000 (0.0372)  acc5: 0.3906 (0.3348)  time: 0.4754  data: 0.0013  max mem: 1095
Test:  [ 30/176]  eta: 0:01:23  loss: 7.0546 (7.0485)  acc1: 0.0000 (0.0630)  acc5: 0.3906 (0.4032)  time: 0.5134  data: 0.0010  max mem: 1095
Test:  [ 40/176]  eta: 0:01:12  loss: 7.0282 (7.0438)  acc1: 0.0000 (0.0476)  acc5: 0.3906 (0.4002)  time: 0.5081  data: 0.0023  max mem: 1095
Test:  [ 50/176]  eta: 0:01:06  loss: 7.0463 (7.0480)  acc1: 0.0000 (0.0613)  acc5: 0.3906 (0.4059)  time: 0.4533  data: 0.0028  max mem: 1095
Test:  [ 60/176]  eta: 0:01:01  loss: 7.0497 (7.0479)  acc1: 0.0000 (0.0640)  acc5: 0.3906 (0.4162)  time: 0.5270  data: 0.0023  max mem: 1095
Test:  [ 70/176]  eta: 0:00:57  loss: 7.0386 (7.0457)  acc1: 0.0000 (0.0825)  acc5: 0.3906 (0.4456)  time: 0.5829  data: 0.0027  max mem: 1095
Test:  [ 80/176]  eta: 0:00:51  loss: 7.0452 (7.0470)  acc1: 0.0000 (0.0965)  acc5: 0.3906 (0.4533)  time: 0.5718  data: 0.0026  max mem: 1095
Test:  [ 90/176]  eta: 0:00:45  loss: 7.0507 (7.0493)  acc1: 0.0000 (0.0944)  acc5: 0.3906 (0.4421)  time: 0.4984  data: 0.0026  max mem: 1095
Test:  [100/176]  eta: 0:00:41  loss: 7.0441 (7.0484)  acc1: 0.0000 (0.0928)  acc5: 0.3906 (0.4409)  time: 0.5343  data: 0.0025  max mem: 1095
Test:  [110/176]  eta: 0:00:34  loss: 7.0262 (7.0477)  acc1: 0.0000 (0.0915)  acc5: 0.3906 (0.4505)  time: 0.5169  data: 0.0026  max mem: 1095
Test:  [120/176]  eta: 0:00:30  loss: 7.0262 (7.0480)  acc1: 0.0000 (0.0904)  acc5: 0.3906 (0.4455)  time: 0.5169  data: 0.0025  max mem: 1095
Test:  [130/176]  eta: 0:00:24  loss: 7.0693 (7.0490)  acc1: 0.0000 (0.0835)  acc5: 0.0000 (0.4294)  time: 0.6077  data: 0.0024  max mem: 1095
Test:  [140/176]  eta: 0:00:19  loss: 7.0340 (7.0478)  acc1: 0.0000 (0.0803)  acc5: 0.0000 (0.4377)  time: 0.5700  data: 0.0027  max mem: 1095
Test:  [150/176]  eta: 0:00:13  loss: 7.0340 (7.0476)  acc1: 0.0000 (0.0776)  acc5: 0.3906 (0.4268)  time: 0.5104  data: 0.0034  max mem: 1095
Test:  [160/176]  eta: 0:00:08  loss: 7.0478 (7.0478)  acc1: 0.0000 (0.0776)  acc5: 0.3906 (0.4270)  time: 0.5482  data: 0.0036  max mem: 1095
Test:  [170/176]  eta: 0:00:03  loss: 7.0432 (7.0476)  acc1: 0.0000 (0.0754)  acc5: 0.3906 (0.4180)  time: 0.5080  data: 0.0028  max mem: 1095
Test:  [175/176]  eta: 0:00:00  loss: 7.0573 (7.0480)  acc1: 0.0000 (0.0778)  acc5: 0.3906 (0.4267)  time: 0.5032  data: 0.0025  max mem: 1095
Test: Total time: 0:01:34 (0.5366 s / it)
* Acc@1 0.078 Acc@5 0.427 loss 7.048
Test:  [ 0/59]  eta: 0:00:54  loss: 6.9202 (6.9202)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.9257  data: 0.6976  max mem: 1095
Test:  [10/59]  eta: 0:00:28  loss: 7.0491 (7.0902)  acc1: 0.0000 (0.0710)  acc5: 0.3906 (0.2841)  time: 0.5826  data: 0.0822  max mem: 1095
Test:  [20/59]  eta: 0:00:23  loss: 7.0641 (7.0802)  acc1: 0.0000 (0.0372)  acc5: 0.0000 (0.2418)  time: 0.5775  data: 0.0114  max mem: 1095
Test:  [30/59]  eta: 0:00:15  loss: 7.0641 (7.0885)  acc1: 0.0000 (0.0756)  acc5: 0.0000 (0.3654)  time: 0.5063  data: 0.0020  max mem: 1095
Test:  [40/59]  eta: 0:00:10  loss: 7.0390 (7.0809)  acc1: 0.0000 (0.0857)  acc5: 0.3906 (0.4097)  time: 0.5063  data: 0.0021  max mem: 1095
Test:  [50/59]  eta: 0:00:04  loss: 7.0283 (7.0703)  acc1: 0.0000 (0.0919)  acc5: 0.3906 (0.4366)  time: 0.5160  data: 0.0021  max mem: 1095
Test:  [58/59]  eta: 0:00:00  loss: 7.0639 (7.0748)  acc1: 0.0000 (0.0802)  acc5: 0.0000 (0.3876)  time: 0.5002  data: 0.0020  max mem: 1095
Test: Total time: 0:00:31 (0.5350 s / it)
* Acc@1 0.080 Acc@5 0.388 loss 7.075
**************Prune Round 5**********************

Current sparsity level: 0.00176105406222953
Actual sparsity after pruning: 0.00176105406222953
Test:  [  0/176]  eta: 0:06:44  loss: 7.0288 (7.0288)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 2.2968  data: 2.0986  max mem: 1095
Test:  [ 10/176]  eta: 0:01:34  loss: 7.0298 (7.0451)  acc1: 0.0000 (0.1065)  acc5: 0.3906 (0.2841)  time: 0.5703  data: 0.2102  max mem: 1095
Test:  [ 20/176]  eta: 0:01:31  loss: 7.0397 (7.0486)  acc1: 0.0000 (0.0744)  acc5: 0.3906 (0.3162)  time: 0.5024  data: 0.0116  max mem: 1095
Test:  [ 30/176]  eta: 0:01:20  loss: 7.0471 (7.0512)  acc1: 0.0000 (0.0882)  acc5: 0.3906 (0.3906)  time: 0.5368  data: 0.0019  max mem: 1095
Test:  [ 40/176]  eta: 0:01:16  loss: 7.0471 (7.0512)  acc1: 0.0000 (0.0667)  acc5: 0.3906 (0.3620)  time: 0.5370  data: 0.0021  max mem: 1095
Test:  [ 50/176]  eta: 0:01:08  loss: 7.0320 (7.0478)  acc1: 0.0000 (0.0689)  acc5: 0.0000 (0.3370)  time: 0.5275  data: 0.0026  max mem: 1095
Test:  [ 60/176]  eta: 0:01:01  loss: 7.0507 (7.0494)  acc1: 0.0000 (0.0576)  acc5: 0.3906 (0.3330)  time: 0.4719  data: 0.0026  max mem: 1095
Test:  [ 70/176]  eta: 0:00:56  loss: 7.0546 (7.0476)  acc1: 0.0000 (0.0550)  acc5: 0.3906 (0.3356)  time: 0.5069  data: 0.0024  max mem: 1095
Test:  [ 80/176]  eta: 0:00:51  loss: 7.0308 (7.0460)  acc1: 0.0000 (0.0579)  acc5: 0.3906 (0.3376)  time: 0.5537  data: 0.0024  max mem: 1095
Test:  [ 90/176]  eta: 0:00:45  loss: 7.0347 (7.0468)  acc1: 0.0000 (0.0558)  acc5: 0.3906 (0.3348)  time: 0.5384  data: 0.0028  max mem: 1095
Test:  [100/176]  eta: 0:00:39  loss: 7.0531 (7.0469)  acc1: 0.0000 (0.0503)  acc5: 0.3906 (0.3326)  time: 0.4773  data: 0.0031  max mem: 1095
Test:  [110/176]  eta: 0:00:35  loss: 7.0446 (7.0460)  acc1: 0.0000 (0.0493)  acc5: 0.3906 (0.3484)  time: 0.5376  data: 0.0028  max mem: 1095
Test:  [120/176]  eta: 0:00:30  loss: 7.0381 (7.0457)  acc1: 0.0000 (0.0517)  acc5: 0.3906 (0.3519)  time: 0.6076  data: 0.0024  max mem: 1095
Test:  [130/176]  eta: 0:00:24  loss: 7.0453 (7.0473)  acc1: 0.0000 (0.0507)  acc5: 0.3906 (0.3489)  time: 0.5685  data: 0.0024  max mem: 1095
Test:  [140/176]  eta: 0:00:19  loss: 7.0603 (7.0485)  acc1: 0.0000 (0.0554)  acc5: 0.0000 (0.3435)  time: 0.5083  data: 0.0029  max mem: 1095
Test:  [150/176]  eta: 0:00:13  loss: 7.0521 (7.0479)  acc1: 0.0000 (0.0595)  acc5: 0.3906 (0.3622)  time: 0.5342  data: 0.0029  max mem: 1095
Test:  [160/176]  eta: 0:00:08  loss: 7.0521 (7.0489)  acc1: 0.0000 (0.0582)  acc5: 0.3906 (0.3664)  time: 0.4999  data: 0.0032  max mem: 1095
Test:  [170/176]  eta: 0:00:03  loss: 7.0572 (7.0489)  acc1: 0.0000 (0.0594)  acc5: 0.3906 (0.3632)  time: 0.5127  data: 0.0029  max mem: 1095
Test:  [175/176]  eta: 0:00:00  loss: 7.0572 (7.0489)  acc1: 0.0000 (0.0578)  acc5: 0.3906 (0.3689)  time: 0.5830  data: 0.0022  max mem: 1095
Test: Total time: 0:01:34 (0.5377 s / it)
* Acc@1 0.058 Acc@5 0.369 loss 7.049
Test:  [ 0/59]  eta: 0:00:53  loss: 6.9202 (6.9202)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.8997  data: 0.6851  max mem: 1095
Test:  [10/59]  eta: 0:00:28  loss: 7.0491 (7.0902)  acc1: 0.0000 (0.0710)  acc5: 0.3906 (0.2841)  time: 0.5740  data: 0.0820  max mem: 1095
Test:  [20/59]  eta: 0:00:20  loss: 7.0641 (7.0802)  acc1: 0.0000 (0.0372)  acc5: 0.0000 (0.2418)  time: 0.4945  data: 0.0124  max mem: 1095
Test:  [30/59]  eta: 0:00:15  loss: 7.0641 (7.0885)  acc1: 0.0000 (0.0756)  acc5: 0.0000 (0.3654)  time: 0.5272  data: 0.0024  max mem: 1095
Test:  [40/59]  eta: 0:00:09  loss: 7.0390 (7.0809)  acc1: 0.0000 (0.0857)  acc5: 0.3906 (0.4097)  time: 0.5176  data: 0.0022  max mem: 1095
Test:  [50/59]  eta: 0:00:04  loss: 7.0283 (7.0703)  acc1: 0.0000 (0.0919)  acc5: 0.3906 (0.4366)  time: 0.5176  data: 0.0023  max mem: 1095
Test:  [58/59]  eta: 0:00:00  loss: 7.0639 (7.0748)  acc1: 0.0000 (0.0802)  acc5: 0.0000 (0.3876)  time: 0.4915  data: 0.0024  max mem: 1095
Test: Total time: 0:00:29 (0.5070 s / it)
* Acc@1 0.080 Acc@5 0.388 loss 7.075
**************Prune Round 6**********************

Current sparsity level: 0.00176105406222953
Actual sparsity after pruning: 0.00176105406222953
Test:  [  0/176]  eta: 0:05:10  loss: 7.0332 (7.0332)  acc1: 0.0000 (0.0000)  acc5: 0.7812 (0.7812)  time: 1.7632  data: 1.5594  max mem: 1095
Test:  [ 10/176]  eta: 0:01:50  loss: 7.0682 (7.0605)  acc1: 0.0000 (0.0000)  acc5: 0.3906 (0.3906)  time: 0.6652  data: 0.1540  max mem: 1095
Test:  [ 20/176]  eta: 0:01:39  loss: 7.0490 (7.0534)  acc1: 0.0000 (0.0558)  acc5: 0.3906 (0.3720)  time: 0.5807  data: 0.0073  max mem: 1095
Test:  [ 30/176]  eta: 0:01:22  loss: 7.0468 (7.0505)  acc1: 0.0000 (0.0756)  acc5: 0.3906 (0.4536)  time: 0.5059  data: 0.0015  max mem: 1095
Test:  [ 40/176]  eta: 0:01:17  loss: 7.0327 (7.0475)  acc1: 0.0000 (0.0762)  acc5: 0.3906 (0.4478)  time: 0.5063  data: 0.0019  max mem: 1095
Test:  [ 50/176]  eta: 0:01:08  loss: 7.0387 (7.0492)  acc1: 0.0000 (0.0766)  acc5: 0.3906 (0.4289)  time: 0.5174  data: 0.0023  max mem: 1095
Test:  [ 60/176]  eta: 0:01:02  loss: 7.0432 (7.0473)  acc1: 0.0000 (0.0897)  acc5: 0.3906 (0.4483)  time: 0.4782  data: 0.0024  max mem: 1095
Test:  [ 70/176]  eta: 0:00:56  loss: 7.0432 (7.0458)  acc1: 0.0000 (0.0825)  acc5: 0.3906 (0.4401)  time: 0.5083  data: 0.0025  max mem: 1095
Test:  [ 80/176]  eta: 0:00:52  loss: 7.0556 (7.0473)  acc1: 0.0000 (0.0868)  acc5: 0.3906 (0.4437)  time: 0.5472  data: 0.0023  max mem: 1095
Test:  [ 90/176]  eta: 0:00:45  loss: 7.0625 (7.0485)  acc1: 0.0000 (0.0816)  acc5: 0.3906 (0.4293)  time: 0.4853  data: 0.0048  max mem: 1095
Test:  [100/176]  eta: 0:00:40  loss: 7.0471 (7.0487)  acc1: 0.0000 (0.0735)  acc5: 0.3906 (0.4293)  time: 0.4874  data: 0.0060  max mem: 1095
Test:  [110/176]  eta: 0:00:35  loss: 7.0442 (7.0487)  acc1: 0.0000 (0.0739)  acc5: 0.3906 (0.4293)  time: 0.6005  data: 0.0037  max mem: 1095
Test:  [120/176]  eta: 0:00:29  loss: 7.0442 (7.0492)  acc1: 0.0000 (0.0710)  acc5: 0.3906 (0.4326)  time: 0.4957  data: 0.0054  max mem: 1095
Test:  [130/176]  eta: 0:00:24  loss: 7.0502 (7.0504)  acc1: 0.0000 (0.0745)  acc5: 0.3906 (0.4354)  time: 0.5044  data: 0.0050  max mem: 1095
Test:  [140/176]  eta: 0:00:18  loss: 7.0492 (7.0503)  acc1: 0.0000 (0.0748)  acc5: 0.3906 (0.4294)  time: 0.5081  data: 0.0023  max mem: 1095
Test:  [150/176]  eta: 0:00:13  loss: 7.0375 (7.0498)  acc1: 0.0000 (0.0724)  acc5: 0.3906 (0.4320)  time: 0.5068  data: 0.0025  max mem: 1095
Test:  [160/176]  eta: 0:00:08  loss: 7.0407 (7.0496)  acc1: 0.0000 (0.0679)  acc5: 0.3906 (0.4319)  time: 0.6061  data: 0.0025  max mem: 1095
Test:  [170/176]  eta: 0:00:03  loss: 7.0502 (7.0500)  acc1: 0.0000 (0.0662)  acc5: 0.3906 (0.4317)  time: 0.5004  data: 0.0032  max mem: 1095
Test:  [175/176]  eta: 0:00:00  loss: 7.0502 (7.0498)  acc1: 0.0000 (0.0644)  acc5: 0.3906 (0.4267)  time: 0.4961  data: 0.0033  max mem: 1095
Test: Total time: 0:01:33 (0.5285 s / it)
* Acc@1 0.064 Acc@5 0.427 loss 7.050
Test:  [ 0/59]  eta: 0:01:16  loss: 6.9202 (6.9202)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 1.2979  data: 1.0732  max mem: 1095
Test:  [10/59]  eta: 0:00:29  loss: 7.0491 (7.0902)  acc1: 0.0000 (0.0710)  acc5: 0.3906 (0.2841)  time: 0.5920  data: 0.1143  max mem: 1095
Test:  [20/59]  eta: 0:00:23  loss: 7.0641 (7.0802)  acc1: 0.0000 (0.0372)  acc5: 0.0000 (0.2418)  time: 0.5559  data: 0.0103  max mem: 1095
Test:  [30/59]  eta: 0:00:15  loss: 7.0641 (7.0885)  acc1: 0.0000 (0.0756)  acc5: 0.0000 (0.3654)  time: 0.5078  data: 0.0023  max mem: 1095
Test:  [40/59]  eta: 0:00:10  loss: 7.0390 (7.0809)  acc1: 0.0000 (0.0857)  acc5: 0.3906 (0.4097)  time: 0.4637  data: 0.0024  max mem: 1095
Test:  [50/59]  eta: 0:00:04  loss: 7.0283 (7.0703)  acc1: 0.0000 (0.0919)  acc5: 0.3906 (0.4366)  time: 0.4779  data: 0.0025  max mem: 1095
Test:  [58/59]  eta: 0:00:00  loss: 7.0639 (7.0748)  acc1: 0.0000 (0.0802)  acc5: 0.0000 (0.3876)  time: 0.4837  data: 0.0023  max mem: 1095
Test: Total time: 0:00:30 (0.5234 s / it)
* Acc@1 0.080 Acc@5 0.388 loss 7.075
**************Prune Round 7**********************

Current sparsity level: 0.00176105406222953
Actual sparsity after pruning: 0.00176105406222953
Test:  [  0/176]  eta: 0:05:27  loss: 7.0741 (7.0741)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 1.8587  data: 1.6592  max mem: 1095
Test:  [ 10/176]  eta: 0:01:42  loss: 7.0357 (7.0427)  acc1: 0.0000 (0.0355)  acc5: 0.0000 (0.2841)  time: 0.6177  data: 0.1672  max mem: 1095
Test:  [ 20/176]  eta: 0:01:24  loss: 7.0462 (7.0505)  acc1: 0.0000 (0.0372)  acc5: 0.0000 (0.3906)  time: 0.4751  data: 0.0101  max mem: 1095
Test:  [ 30/176]  eta: 0:01:16  loss: 7.0615 (7.0543)  acc1: 0.0000 (0.0378)  acc5: 0.3906 (0.3528)  time: 0.4740  data: 0.0022  max mem: 1095
Test:  [ 40/176]  eta: 0:01:10  loss: 7.0692 (7.0558)  acc1: 0.0000 (0.0286)  acc5: 0.3906 (0.3430)  time: 0.4893  data: 0.0027  max mem: 1095
Test:  [ 50/176]  eta: 0:01:05  loss: 7.0509 (7.0524)  acc1: 0.0000 (0.0230)  acc5: 0.3906 (0.3217)  time: 0.5100  data: 0.0029  max mem: 1095
Test:  [ 60/176]  eta: 0:00:59  loss: 7.0323 (7.0500)  acc1: 0.0000 (0.0320)  acc5: 0.3906 (0.3330)  time: 0.5104  data: 0.0030  max mem: 1095
Test:  [ 70/176]  eta: 0:00:53  loss: 7.0442 (7.0509)  acc1: 0.0000 (0.0330)  acc5: 0.0000 (0.3081)  time: 0.4582  data: 0.0035  max mem: 1095
Test:  [ 80/176]  eta: 0:00:49  loss: 7.0401 (7.0490)  acc1: 0.0000 (0.0386)  acc5: 0.0000 (0.3086)  time: 0.5189  data: 0.0033  max mem: 1095
