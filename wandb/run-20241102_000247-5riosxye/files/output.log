Set warmup steps = 8750
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0000040
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.dwconv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.dwconv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.dwconv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.dwconv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.dwconv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.dwconv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.dwconv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.dwconv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.dwconv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.dwconv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.dwconv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.dwconv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.dwconv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.dwconv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.dwconv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.dwconv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.dwconv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.dwconv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.dwconv.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.dwconv.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.dwconv.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.dwconv.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.dwconv.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.dwconv.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.dwconv.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.dwconv.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.dwconv.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.dwconv.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.dwconv.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.dwconv.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.dwconv.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.dwconv.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.dwconv.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.dwconv.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.dwconv.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.dwconv.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
/home/shahanahmed/.local/lib/python3.10/site-packages/timm/utils/cuda.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()

Current sparsity level: 1.1592622269705604e-07
block 0
block 1
block 2
block 3
Actual sparsity after pruning: 1.0
/home/shahanahmed/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch: [0]  [  0/176]  eta: 0:50:47  lr: 0.000000  min_lr: 0.000000  loss: 7.0353 (7.0353)  class_acc: 0.0000 (0.0000)  weight_decay: 0.0500 (0.0500)  time: 17.3138  data: 2.9831  max mem: 1586
Epoch: [0]  [ 10/176]  eta: 0:08:56  lr: 0.000005  min_lr: 0.000005  loss: 7.0473 (7.0531)  class_acc: 0.0000 (0.0007)  weight_decay: 0.0499 (0.0499)  time: 3.2304  data: 0.2751  max mem: 1586
Epoch: [0]  [ 20/176]  eta: 0:06:34  lr: 0.000009  min_lr: 0.000009  loss: 7.0368 (7.0285)  class_acc: 0.0000 (0.0006)  weight_decay: 0.0495 (0.0495)  time: 1.7894  data: 0.0028  max mem: 1586
Epoch: [0]  [ 30/176]  eta: 0:05:37  lr: 0.000014  min_lr: 0.000014  loss: 6.9907 (7.0122)  class_acc: 0.0000 (0.0010)  weight_decay: 0.0482 (0.0488)  time: 1.8048  data: 0.0029  max mem: 1586
Epoch: [0]  [ 40/176]  eta: 0:05:00  lr: 0.000018  min_lr: 0.000018  loss: 6.9356 (6.9822)  class_acc: 0.0000 (0.0013)  weight_decay: 0.0462 (0.0479)  time: 1.8798  data: 0.0047  max mem: 1586
Epoch: [0]  [ 50/176]  eta: 0:04:35  lr: 0.000023  min_lr: 0.000023  loss: 6.8508 (6.9483)  class_acc: 0.0039 (0.0026)  weight_decay: 0.0435 (0.0467)  time: 1.9888  data: 0.0057  max mem: 1586
Epoch: [0]  [ 60/176]  eta: 0:04:12  lr: 0.000027  min_lr: 0.000027  loss: 6.7539 (6.9084)  class_acc: 0.0078 (0.0042)  weight_decay: 0.0402 (0.0454)  time: 2.1006  data: 0.0063  max mem: 1586
Epoch: [0]  [ 70/176]  eta: 0:03:51  lr: 0.000032  min_lr: 0.000032  loss: 6.6253 (6.8594)  class_acc: 0.0117 (0.0055)  weight_decay: 0.0364 (0.0439)  time: 2.1926  data: 0.0054  max mem: 1586
Epoch: [0]  [ 80/176]  eta: 0:03:31  lr: 0.000037  min_lr: 0.000037  loss: 6.4559 (6.7965)  class_acc: 0.0156 (0.0071)  weight_decay: 0.0323 (0.0422)  time: 2.2991  data: 0.0046  max mem: 1586
Epoch: [0]  [ 90/176]  eta: 0:03:11  lr: 0.000041  min_lr: 0.000041  loss: 6.2070 (6.7217)  class_acc: 0.0195 (0.0088)  weight_decay: 0.0279 (0.0404)  time: 2.3429  data: 0.0041  max mem: 1586
Epoch: [0]  [100/176]  eta: 0:02:49  lr: 0.000046  min_lr: 0.000046  loss: 5.9652 (6.6351)  class_acc: 0.0273 (0.0106)  weight_decay: 0.0234 (0.0385)  time: 2.3389  data: 0.0030  max mem: 1586
Epoch: [0]  [110/176]  eta: 0:02:28  lr: 0.000050  min_lr: 0.000050  loss: 5.7292 (6.5431)  class_acc: 0.0273 (0.0121)  weight_decay: 0.0190 (0.0366)  time: 2.3402  data: 0.0034  max mem: 1586
Epoch: [0]  [120/176]  eta: 0:02:06  lr: 0.000055  min_lr: 0.000055  loss: 5.4787 (6.4517)  class_acc: 0.0273 (0.0133)  weight_decay: 0.0148 (0.0347)  time: 2.3440  data: 0.0043  max mem: 1586
Epoch: [0]  [130/176]  eta: 0:01:43  lr: 0.000059  min_lr: 0.000059  loss: 5.2970 (6.3575)  class_acc: 0.0234 (0.0144)  weight_decay: 0.0109 (0.0327)  time: 2.3424  data: 0.0038  max mem: 1586
Epoch: [0]  [140/176]  eta: 0:01:21  lr: 0.000064  min_lr: 0.000064  loss: 5.1723 (6.2699)  class_acc: 0.0273 (0.0153)  weight_decay: 0.0074 (0.0308)  time: 2.3414  data: 0.0034  max mem: 1586
Epoch: [0]  [150/176]  eta: 0:00:58  lr: 0.000069  min_lr: 0.000069  loss: 5.0435 (6.1845)  class_acc: 0.0273 (0.0163)  weight_decay: 0.0045 (0.0290)  time: 2.3418  data: 0.0033  max mem: 1586
Epoch: [0]  [160/176]  eta: 0:00:36  lr: 0.000073  min_lr: 0.000073  loss: 4.9423 (6.1056)  class_acc: 0.0312 (0.0174)  weight_decay: 0.0023 (0.0273)  time: 2.3409  data: 0.0035  max mem: 1586
Epoch: [0]  [170/176]  eta: 0:00:13  lr: 0.000078  min_lr: 0.000078  loss: 4.8875 (6.0335)  class_acc: 0.0312 (0.0183)  weight_decay: 0.0008 (0.0257)  time: 2.3427  data: 0.0042  max mem: 1586
Epoch: [0]  [175/176]  eta: 0:00:02  lr: 0.000080  min_lr: 0.000080  loss: 4.8337 (6.0042)  class_acc: 0.0352 (0.0187)  weight_decay: 0.0004 (0.0251)  time: 2.2273  data: 0.0044  max mem: 1586
Epoch: [0] Total time: 0:06:39 (2.2680 s / it)
Averaged stats: lr: 0.000080  min_lr: 0.000080  loss: 4.8337 (6.0042)  class_acc: 0.0352 (0.0187)  weight_decay: 0.0004 (0.0251)
/home/shahanahmed/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test:  [  0/176]  eta: 0:09:52  loss: 4.6851 (4.6851)  acc1: 2.3438 (2.3438)  acc5: 10.1562 (10.1562)  time: 3.3664  data: 2.7899  max mem: 1586
Test:  [ 10/176]  eta: 0:01:40  loss: 4.7776 (4.7725)  acc1: 3.1250 (3.2670)  acc5: 12.8906 (12.5000)  time: 0.6025  data: 0.2806  max mem: 1586
Test:  [ 20/176]  eta: 0:01:16  loss: 4.7493 (4.7595)  acc1: 4.2969 (3.7574)  acc5: 13.2812 (13.0208)  time: 0.3497  data: 0.0460  max mem: 1586
Test:  [ 30/176]  eta: 0:01:03  loss: 4.7493 (4.7658)  acc1: 3.9062 (3.7298)  acc5: 12.8906 (12.8780)  time: 0.3421  data: 0.0342  max mem: 1586
Test:  [ 40/176]  eta: 0:01:01  loss: 4.7543 (4.7644)  acc1: 3.5156 (3.7443)  acc5: 13.2812 (13.0716)  time: 0.4043  data: 0.0055  max mem: 1586
Test:  [ 50/176]  eta: 0:00:55  loss: 4.7748 (4.7689)  acc1: 3.5156 (3.7301)  acc5: 12.8906 (12.8983)  time: 0.4517  data: 0.0051  max mem: 1586
Test:  [ 60/176]  eta: 0:00:48  loss: 4.7500 (4.7617)  acc1: 3.9062 (3.8038)  acc5: 12.8906 (13.1276)  time: 0.3566  data: 0.0053  max mem: 1586
Test:  [ 70/176]  eta: 0:00:44  loss: 4.7102 (4.7537)  acc1: 4.2969 (3.9118)  acc5: 14.4531 (13.2757)  time: 0.3718  data: 0.0059  max mem: 1586
Test:  [ 80/176]  eta: 0:00:42  loss: 4.7156 (4.7531)  acc1: 3.9062 (3.8194)  acc5: 13.2812 (13.2571)  time: 0.5130  data: 0.0056  max mem: 1586
Test:  [ 90/176]  eta: 0:00:36  loss: 4.7270 (4.7508)  acc1: 3.5156 (3.8204)  acc5: 13.2812 (13.2254)  time: 0.4477  data: 0.0046  max mem: 1586
Test:  [100/176]  eta: 0:00:31  loss: 4.7439 (4.7508)  acc1: 3.5156 (3.8018)  acc5: 13.2812 (13.2464)  time: 0.3068  data: 0.0049  max mem: 1586
Test:  [110/176]  eta: 0:00:28  loss: 4.7613 (4.7533)  acc1: 3.5156 (3.7655)  acc5: 12.5000 (13.1616)  time: 0.4391  data: 0.0052  max mem: 1586
Test:  [120/176]  eta: 0:00:23  loss: 4.7433 (4.7505)  acc1: 3.5156 (3.7836)  acc5: 12.5000 (13.1424)  time: 0.4780  data: 0.0058  max mem: 1586
Test:  [130/176]  eta: 0:00:19  loss: 4.7368 (4.7493)  acc1: 3.9062 (3.7631)  acc5: 12.8906 (13.1650)  time: 0.3537  data: 0.0098  max mem: 1586
Test:  [140/176]  eta: 0:00:14  loss: 4.7554 (4.7513)  acc1: 3.5156 (3.7622)  acc5: 12.8906 (13.1926)  time: 0.3247  data: 0.0122  max mem: 1586
Test:  [150/176]  eta: 0:00:10  loss: 4.7563 (4.7500)  acc1: 3.5156 (3.7484)  acc5: 12.5000 (13.1260)  time: 0.3439  data: 0.0145  max mem: 1586
Test:  [160/176]  eta: 0:00:06  loss: 4.7583 (4.7513)  acc1: 3.5156 (3.7534)  acc5: 12.5000 (13.1090)  time: 0.4609  data: 0.0127  max mem: 1586
Test:  [170/176]  eta: 0:00:02  loss: 4.7410 (4.7488)  acc1: 3.9062 (3.7806)  acc5: 13.2812 (13.1967)  time: 0.5469  data: 0.0054  max mem: 1586
Test:  [175/176]  eta: 0:00:00  loss: 4.7101 (4.7474)  acc1: 4.2969 (3.7978)  acc5: 13.6719 (13.1911)  time: 0.5653  data: 0.0034  max mem: 1586
Test: Total time: 0:01:15 (0.4314 s / it)
* Acc@1 3.798 Acc@5 13.191 loss 4.747
Test:  [ 0/59]  eta: 0:02:22  loss: 5.4050 (5.4050)  acc1: 11.3281 (11.3281)  acc5: 33.5938 (33.5938)  time: 2.4204  data: 2.2190  max mem: 1586
Test:  [10/59]  eta: 0:00:34  loss: 5.9905 (5.8664)  acc1: 1.1719 (3.3381)  acc5: 6.2500 (11.9318)  time: 0.7070  data: 0.2072  max mem: 1586
Test:  [20/59]  eta: 0:00:20  loss: 5.8809 (5.8041)  acc1: 1.5625 (5.0781)  acc5: 7.0312 (14.1369)  time: 0.4221  data: 0.0057  max mem: 1586
Test:  [30/59]  eta: 0:00:13  loss: 5.8530 (5.7657)  acc1: 4.6875 (6.3130)  acc5: 15.2344 (15.1714)  time: 0.3086  data: 0.0050  max mem: 1586
Test:  [40/59]  eta: 0:00:09  loss: 5.6939 (5.6596)  acc1: 5.0781 (6.8407)  acc5: 16.7969 (16.9493)  time: 0.4607  data: 0.0046  max mem: 1586
Test:  [50/59]  eta: 0:00:04  loss: 5.6939 (5.6591)  acc1: 2.3438 (6.4568)  acc5: 17.1875 (17.7083)  time: 0.4865  data: 0.0048  max mem: 1586
Test:  [58/59]  eta: 0:00:00  loss: 5.4857 (5.6543)  acc1: 2.3438 (5.8804)  acc5: 17.9688 (17.2269)  time: 0.4009  data: 0.0043  max mem: 1586
Test: Total time: 0:00:27 (0.4598 s / it)
* Acc@1 5.880 Acc@5 17.227 loss 5.654
Test:  [ 0/59]  eta: 0:02:21  loss: 5.4050 (5.4050)  acc1: 11.3281 (11.3281)  acc5: 33.5938 (33.5938)  time: 2.4047  data: 2.2038  max mem: 1586
Test:  [10/59]  eta: 0:00:34  loss: 5.9905 (5.8664)  acc1: 1.1719 (3.3381)  acc5: 6.2500 (11.9318)  time: 0.7143  data: 0.2027  max mem: 1586
Test:  [20/59]  eta: 0:00:22  loss: 5.8809 (5.8041)  acc1: 1.5625 (5.0781)  acc5: 7.0312 (14.1369)  time: 0.4732  data: 0.0025  max mem: 1586
Test:  [30/59]  eta: 0:00:13  loss: 5.8530 (5.7657)  acc1: 4.6875 (6.3130)  acc5: 15.2344 (15.1714)  time: 0.3533  data: 0.0034  max mem: 1586
Test:  [40/59]  eta: 0:00:09  loss: 5.6939 (5.6596)  acc1: 5.0781 (6.8407)  acc5: 16.7969 (16.9493)  time: 0.4144  data: 0.0052  max mem: 1586
Test:  [50/59]  eta: 0:00:04  loss: 5.6939 (5.6591)  acc1: 2.3438 (6.4568)  acc5: 17.1875 (17.7083)  time: 0.4646  data: 0.0045  max mem: 1586
Test:  [58/59]  eta: 0:00:00  loss: 5.4857 (5.6543)  acc1: 2.3438 (5.8804)  acc5: 17.9688 (17.2269)  time: 0.3778  data: 0.0035  max mem: 1586
Test: Total time: 0:00:26 (0.4538 s / it)
* Acc@1 5.880 Acc@5 17.227 loss 5.654
Traceback (most recent call last):
  File "/media/shahanahmed/b8c6fb5d-b937-4730-bb0f-ac0eba675d7e/Wanda_pruning_for_image_classification/main.py", line 530, in <module>
    main(args)
  File "/media/shahanahmed/b8c6fb5d-b937-4730-bb0f-ac0eba675d7e/Wanda_pruning_for_image_classification/main.py", line 523, in main
    train_with_pruning(model,dataset_train, dataset_val,device,args)
  File "/media/shahanahmed/b8c6fb5d-b937-4730-bb0f-ac0eba675d7e/Wanda_pruning_for_image_classification/main.py", line 383, in train_with_pruning
    test_outputs, test_targets = evaluate(val_loader, model, device, use_amp=args.use_amp)
ValueError: too many values to unpack (expected 2)
Traceback (most recent call last):
  File "/media/shahanahmed/b8c6fb5d-b937-4730-bb0f-ac0eba675d7e/Wanda_pruning_for_image_classification/main.py", line 530, in <module>
    main(args)
  File "/media/shahanahmed/b8c6fb5d-b937-4730-bb0f-ac0eba675d7e/Wanda_pruning_for_image_classification/main.py", line 523, in main
    train_with_pruning(model,dataset_train, dataset_val,device,args)
  File "/media/shahanahmed/b8c6fb5d-b937-4730-bb0f-ac0eba675d7e/Wanda_pruning_for_image_classification/main.py", line 383, in train_with_pruning
    test_outputs, test_targets = evaluate(val_loader, model, device, use_amp=args.use_amp)
ValueError: too many values to unpack (expected 2)
