Set warmup steps = 9750
Set warmup steps = 0
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.dwconv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.dwconv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.dwconv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.dwconv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.dwconv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.dwconv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.dwconv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.dwconv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.dwconv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.dwconv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.dwconv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.dwconv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.dwconv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.dwconv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.dwconv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.dwconv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.dwconv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.dwconv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.dwconv.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.dwconv.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.dwconv.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.dwconv.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.dwconv.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.dwconv.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.dwconv.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.dwconv.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.dwconv.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.dwconv.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.dwconv.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.dwconv.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.dwconv.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.dwconv.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.dwconv.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.dwconv.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.dwconv.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.dwconv.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
/home/shahanahmed/.local/lib/python3.10/site-packages/timm/utils/cuda.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
**************Prune Round 1**********************

Current sparsity level: 0.0017724827175138512
Processing block 0
Block 0 - Current sparsity level: 1.0000
Processing block 1
Block 1 - Current sparsity level: 1.0000
Processing block 2
Block 2 - Current sparsity level: 1.0000
Processing block 3
Block 3 - Current sparsity level: 1.0000
Actual sparsity after pruning: 0.929416997394848
/home/shahanahmed/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch: [0]  [  0/196]  eta: 1:00:54  lr: 0.000000  min_lr: 0.000000  loss: 4.6786 (4.6786)  class_acc: 0.0117 (0.0117)  weight_decay: 0.0500 (0.0500)  time: 18.6432  data: 3.7575  max mem: 1751
Epoch: [0]  [ 10/196]  eta: 0:09:43  lr: 0.000004  min_lr: 0.000004  loss: 4.7172 (4.7212)  class_acc: 0.0117 (0.0146)  weight_decay: 0.0499 (0.0499)  time: 3.1390  data: 0.3451  max mem: 1751
Epoch: [0]  [ 20/196]  eta: 0:07:06  lr: 0.000008  min_lr: 0.000008  loss: 4.7172 (4.7201)  class_acc: 0.0156 (0.0147)  weight_decay: 0.0496 (0.0496)  time: 1.6134  data: 0.0045  max mem: 1751
Epoch: [0]  [ 30/196]  eta: 0:06:03  lr: 0.000012  min_lr: 0.000012  loss: 4.7166 (4.7185)  class_acc: 0.0117 (0.0134)  weight_decay: 0.0486 (0.0490)  time: 1.6669  data: 0.0055  max mem: 1751
Epoch: [0]  [ 40/196]  eta: 0:05:24  lr: 0.000016  min_lr: 0.000016  loss: 4.6877 (4.7092)  class_acc: 0.0117 (0.0135)  weight_decay: 0.0469 (0.0483)  time: 1.7230  data: 0.0064  max mem: 1751
Epoch: [0]  [ 50/196]  eta: 0:04:57  lr: 0.000021  min_lr: 0.000021  loss: 4.6669 (4.7005)  class_acc: 0.0117 (0.0136)  weight_decay: 0.0447 (0.0474)  time: 1.8075  data: 0.0069  max mem: 1751
Epoch: [0]  [ 60/196]  eta: 0:04:33  lr: 0.000025  min_lr: 0.000025  loss: 4.6557 (4.6905)  class_acc: 0.0156 (0.0151)  weight_decay: 0.0420 (0.0463)  time: 1.8759  data: 0.0071  max mem: 1751
Epoch: [0]  [ 70/196]  eta: 0:04:14  lr: 0.000029  min_lr: 0.000029  loss: 4.6321 (4.6824)  class_acc: 0.0195 (0.0160)  weight_decay: 0.0389 (0.0450)  time: 1.9585  data: 0.0077  max mem: 1751
Epoch: [0]  [ 80/196]  eta: 0:03:54  lr: 0.000033  min_lr: 0.000033  loss: 4.5932 (4.6696)  class_acc: 0.0234 (0.0174)  weight_decay: 0.0354 (0.0436)  time: 2.0514  data: 0.0072  max mem: 1751
Epoch: [0]  [ 90/196]  eta: 0:03:37  lr: 0.000037  min_lr: 0.000037  loss: 4.5851 (4.6585)  class_acc: 0.0273 (0.0196)  weight_decay: 0.0316 (0.0421)  time: 2.1713  data: 0.0065  max mem: 1751
Epoch: [0]  [100/196]  eta: 0:03:18  lr: 0.000041  min_lr: 0.000041  loss: 4.5613 (4.6494)  class_acc: 0.0352 (0.0213)  weight_decay: 0.0276 (0.0405)  time: 2.2568  data: 0.0066  max mem: 1751
Epoch: [0]  [110/196]  eta: 0:03:00  lr: 0.000045  min_lr: 0.000045  loss: 4.5554 (4.6399)  class_acc: 0.0352 (0.0224)  weight_decay: 0.0236 (0.0388)  time: 2.2897  data: 0.0057  max mem: 1751
Epoch: [0]  [120/196]  eta: 0:02:40  lr: 0.000049  min_lr: 0.000049  loss: 4.5405 (4.6319)  class_acc: 0.0352 (0.0238)  weight_decay: 0.0196 (0.0371)  time: 2.3370  data: 0.0052  max mem: 1751
Epoch: [0]  [130/196]  eta: 0:02:20  lr: 0.000053  min_lr: 0.000053  loss: 4.5244 (4.6229)  class_acc: 0.0391 (0.0246)  weight_decay: 0.0158 (0.0353)  time: 2.3409  data: 0.0063  max mem: 1751
Epoch: [0]  [140/196]  eta: 0:02:00  lr: 0.000057  min_lr: 0.000057  loss: 4.5086 (4.6145)  class_acc: 0.0352 (0.0255)  weight_decay: 0.0122 (0.0336)  time: 2.3422  data: 0.0068  max mem: 1751
Epoch: [0]  [150/196]  eta: 0:01:39  lr: 0.000062  min_lr: 0.000062  loss: 4.4983 (4.6074)  class_acc: 0.0352 (0.0264)  weight_decay: 0.0089 (0.0318)  time: 2.3425  data: 0.0068  max mem: 1751
Epoch: [0]  [160/196]  eta: 0:01:18  lr: 0.000066  min_lr: 0.000066  loss: 4.4902 (4.6009)  class_acc: 0.0430 (0.0274)  weight_decay: 0.0060 (0.0302)  time: 2.3432  data: 0.0075  max mem: 1751
Epoch: [0]  [170/196]  eta: 0:00:56  lr: 0.000070  min_lr: 0.000070  loss: 4.4892 (4.5946)  class_acc: 0.0430 (0.0282)  weight_decay: 0.0037 (0.0286)  time: 2.3388  data: 0.0062  max mem: 1751
Epoch: [0]  [180/196]  eta: 0:00:35  lr: 0.000074  min_lr: 0.000074  loss: 4.4891 (4.5887)  class_acc: 0.0391 (0.0287)  weight_decay: 0.0018 (0.0271)  time: 2.3310  data: 0.0036  max mem: 1751
Epoch: [0]  [190/196]  eta: 0:00:13  lr: 0.000078  min_lr: 0.000078  loss: 4.4885 (4.5829)  class_acc: 0.0391 (0.0297)  weight_decay: 0.0006 (0.0257)  time: 2.3302  data: 0.0035  max mem: 1751
Epoch: [0]  [195/196]  eta: 0:00:02  lr: 0.000080  min_lr: 0.000080  loss: 4.4885 (4.5809)  class_acc: 0.0352 (0.0298)  weight_decay: 0.0003 (0.0251)  time: 2.2148  data: 0.0040  max mem: 1751
Epoch: [0] Total time: 0:07:09 (2.1891 s / it)
Averaged stats: lr: 0.000080  min_lr: 0.000080  loss: 4.4885 (4.5809)  class_acc: 0.0352 (0.0298)  weight_decay: 0.0003 (0.0251)
/home/shahanahmed/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test:  [  0/196]  eta: 0:14:21  loss: 4.5312 (4.5312)  acc1: 3.1250 (3.1250)  acc5: 12.1094 (12.1094)  time: 4.3979  data: 4.0467  max mem: 1751
Test:  [ 10/196]  eta: 0:02:05  loss: 4.4890 (4.4816)  acc1: 3.9062 (4.1193)  acc5: 14.4531 (14.3111)  time: 0.6767  data: 0.3739  max mem: 1751
Test:  [ 20/196]  eta: 0:01:28  loss: 4.4628 (4.4661)  acc1: 4.2969 (4.4457)  acc5: 15.6250 (15.0484)  time: 0.3109  data: 0.0057  max mem: 1751
Test:  [ 30/196]  eta: 0:01:13  loss: 4.4628 (4.4657)  acc1: 5.0781 (4.5489)  acc5: 15.6250 (15.1840)  time: 0.3115  data: 0.0046  max mem: 1751
Test:  [ 40/196]  eta: 0:01:03  loss: 4.4768 (4.4701)  acc1: 4.2969 (4.5636)  acc5: 14.8438 (15.1963)  time: 0.3098  data: 0.0072  max mem: 1751
Test:  [ 50/196]  eta: 0:00:57  loss: 4.4820 (4.4706)  acc1: 4.2969 (4.4194)  acc5: 14.8438 (15.0888)  time: 0.3184  data: 0.0130  max mem: 1751
Test:  [ 60/196]  eta: 0:00:51  loss: 4.4628 (4.4714)  acc1: 4.6875 (4.5082)  acc5: 15.6250 (15.2088)  time: 0.3173  data: 0.0119  max mem: 1751
Test:  [ 70/196]  eta: 0:00:46  loss: 4.4655 (4.4722)  acc1: 4.6875 (4.5224)  acc5: 15.6250 (15.2234)  time: 0.3097  data: 0.0073  max mem: 1751
Test:  [ 80/196]  eta: 0:00:42  loss: 4.4767 (4.4729)  acc1: 4.2969 (4.4850)  acc5: 15.6250 (15.2103)  time: 0.3153  data: 0.0108  max mem: 1751
Test:  [ 90/196]  eta: 0:00:38  loss: 4.4735 (4.4711)  acc1: 4.2969 (4.4857)  acc5: 16.0156 (15.3674)  time: 0.3341  data: 0.0184  max mem: 1751
Test:  [100/196]  eta: 0:00:34  loss: 4.4481 (4.4695)  acc1: 4.6875 (4.4748)  acc5: 16.4062 (15.4626)  time: 0.3228  data: 0.0122  max mem: 1751
Test:  [110/196]  eta: 0:00:30  loss: 4.4515 (4.4683)  acc1: 4.6875 (4.5115)  acc5: 15.6250 (15.5828)  time: 0.3005  data: 0.0027  max mem: 1751
Test:  [120/196]  eta: 0:00:26  loss: 4.4642 (4.4685)  acc1: 4.6875 (4.5390)  acc5: 15.6250 (15.5701)  time: 0.3001  data: 0.0024  max mem: 1751
Test:  [130/196]  eta: 0:00:22  loss: 4.4733 (4.4696)  acc1: 3.9062 (4.5503)  acc5: 14.4531 (15.5117)  time: 0.3008  data: 0.0023  max mem: 1751
Test:  [140/196]  eta: 0:00:19  loss: 4.4706 (4.4700)  acc1: 4.2969 (4.5490)  acc5: 14.4531 (15.4505)  time: 0.3023  data: 0.0028  max mem: 1751
Test:  [150/196]  eta: 0:00:16  loss: 4.4665 (4.4687)  acc1: 4.2969 (4.5504)  acc5: 15.2344 (15.4491)  time: 0.3926  data: 0.0033  max mem: 1751
Test:  [160/196]  eta: 0:00:12  loss: 4.4566 (4.4689)  acc1: 4.6875 (4.5492)  acc5: 15.2344 (15.4697)  time: 0.4866  data: 0.0033  max mem: 1751
Test:  [170/196]  eta: 0:00:09  loss: 4.4535 (4.4677)  acc1: 4.6875 (4.5733)  acc5: 16.0156 (15.5085)  time: 0.3952  data: 0.0025  max mem: 1751
Test:  [180/196]  eta: 0:00:05  loss: 4.4631 (4.4684)  acc1: 3.9062 (4.5256)  acc5: 16.0156 (15.4782)  time: 0.2996  data: 0.0020  max mem: 1751
Test:  [190/196]  eta: 0:00:02  loss: 4.4607 (4.4679)  acc1: 4.2969 (4.5750)  acc5: 16.0156 (15.5227)  time: 0.2994  data: 0.0019  max mem: 1751
Test:  [195/196]  eta: 0:00:00  loss: 4.4638 (4.4673)  acc1: 5.0781 (4.5900)  acc5: 15.6250 (15.5580)  time: 0.3277  data: 0.0016  max mem: 1751
Test: Total time: 0:01:09 (0.3521 s / it)
* Acc@1 4.590 Acc@5 15.558 loss 4.467
Test:  [ 0/40]  eta: 0:00:32  loss: 4.1340 (4.1340)  acc1: 10.5469 (10.5469)  acc5: 23.4375 (23.4375)  time: 0.8057  data: 0.5938  max mem: 1751
Test:  [10/40]  eta: 0:00:17  loss: 4.1834 (4.1769)  acc1: 9.7656 (9.2685)  acc5: 24.6094 (24.4318)  time: 0.5776  data: 0.0677  max mem: 1751
Test:  [20/40]  eta: 0:00:10  loss: 4.1854 (4.1805)  acc1: 8.5938 (8.7426)  acc5: 25.0000 (24.6838)  time: 0.5040  data: 0.0080  max mem: 1751
Test:  [30/40]  eta: 0:00:04  loss: 4.1854 (4.1867)  acc1: 7.8125 (8.4803)  acc5: 23.4375 (24.0549)  time: 0.3759  data: 0.0012  max mem: 1751
Test:  [39/40]  eta: 0:00:00  loss: 4.1815 (4.1893)  acc1: 8.2031 (8.3700)  acc5: 23.0469 (24.1800)  time: 0.2985  data: 0.0014  max mem: 1751
Test: Total time: 0:00:16 (0.4184 s / it)
* Acc@1 8.370 Acc@5 24.180 loss 4.189
**************Prune Round 2**********************

Current sparsity level: 0.9290692901050248
Processing block 0
Block 0 - Current sparsity level: 1.0000
Processing block 1
Block 1 - Current sparsity level: 1.0000
Processing block 2
Block 2 - Current sparsity level: 1.0000
Processing block 3
Block 3 - Current sparsity level: 1.0000
Actual sparsity after pruning: 0.9290692901050248
Epoch: [0]  [  0/196]  eta: 0:10:09  lr: 0.000000  min_lr: 0.000000  loss: 4.5290 (4.5290)  class_acc: 0.0273 (0.0273)  weight_decay: 0.0500 (0.0500)  time: 3.1119  data: 1.0249  max mem: 1751
Epoch: [0]  [ 10/196]  eta: 0:07:26  lr: 0.000004  min_lr: 0.000004  loss: 4.4646 (4.4665)  class_acc: 0.0430 (0.0405)  weight_decay: 0.0499 (0.0499)  time: 2.4014  data: 0.1004  max mem: 1751
Epoch: [0]  [ 20/196]  eta: 0:06:56  lr: 0.000008  min_lr: 0.000008  loss: 4.4478 (4.4593)  class_acc: 0.0430 (0.0456)  weight_decay: 0.0496 (0.0496)  time: 2.3295  data: 0.0049  max mem: 1751
Epoch: [0]  [ 30/196]  eta: 0:06:31  lr: 0.000012  min_lr: 0.000012  loss: 4.4478 (4.4699)  class_acc: 0.0430 (0.0444)  weight_decay: 0.0486 (0.0490)  time: 2.3307  data: 0.0032  max mem: 1751
Epoch: [0]  [ 40/196]  eta: 0:06:06  lr: 0.000016  min_lr: 0.000016  loss: 4.4585 (4.4652)  class_acc: 0.0469 (0.0454)  weight_decay: 0.0469 (0.0483)  time: 2.3349  data: 0.0051  max mem: 1751
Epoch: [0]  [ 50/196]  eta: 0:05:42  lr: 0.000021  min_lr: 0.000021  loss: 4.4525 (4.4643)  class_acc: 0.0469 (0.0460)  weight_decay: 0.0447 (0.0474)  time: 2.3347  data: 0.0049  max mem: 1751
Epoch: [0]  [ 60/196]  eta: 0:05:18  lr: 0.000025  min_lr: 0.000025  loss: 4.4349 (4.4607)  class_acc: 0.0547 (0.0476)  weight_decay: 0.0420 (0.0463)  time: 2.3322  data: 0.0044  max mem: 1751
Epoch: [0]  [ 70/196]  eta: 0:04:55  lr: 0.000029  min_lr: 0.000029  loss: 4.4337 (4.4580)  class_acc: 0.0547 (0.0479)  weight_decay: 0.0389 (0.0450)  time: 2.3328  data: 0.0044  max mem: 1751
Epoch: [0]  [ 80/196]  eta: 0:04:31  lr: 0.000033  min_lr: 0.000033  loss: 4.4455 (4.4572)  class_acc: 0.0469 (0.0473)  weight_decay: 0.0354 (0.0436)  time: 2.3340  data: 0.0045  max mem: 1751
Epoch: [0]  [ 90/196]  eta: 0:04:08  lr: 0.000037  min_lr: 0.000037  loss: 4.4555 (4.4577)  class_acc: 0.0430 (0.0468)  weight_decay: 0.0316 (0.0421)  time: 2.3334  data: 0.0042  max mem: 1751
Epoch: [0]  [100/196]  eta: 0:03:44  lr: 0.000041  min_lr: 0.000041  loss: 4.4441 (4.4553)  class_acc: 0.0430 (0.0474)  weight_decay: 0.0276 (0.0405)  time: 2.3323  data: 0.0039  max mem: 1751
Epoch: [0]  [110/196]  eta: 0:03:21  lr: 0.000045  min_lr: 0.000045  loss: 4.4361 (4.4535)  class_acc: 0.0547 (0.0478)  weight_decay: 0.0236 (0.0388)  time: 2.3298  data: 0.0034  max mem: 1751
Epoch: [0]  [120/196]  eta: 0:02:57  lr: 0.000049  min_lr: 0.000049  loss: 4.4415 (4.4538)  class_acc: 0.0469 (0.0476)  weight_decay: 0.0196 (0.0371)  time: 2.3343  data: 0.0043  max mem: 1751
Epoch: [0]  [130/196]  eta: 0:02:34  lr: 0.000053  min_lr: 0.000053  loss: 4.4408 (4.4521)  class_acc: 0.0430 (0.0477)  weight_decay: 0.0158 (0.0353)  time: 2.3363  data: 0.0046  max mem: 1751
Epoch: [0]  [140/196]  eta: 0:02:10  lr: 0.000057  min_lr: 0.000057  loss: 4.4408 (4.4511)  class_acc: 0.0469 (0.0479)  weight_decay: 0.0122 (0.0336)  time: 2.3380  data: 0.0051  max mem: 1751
Epoch: [0]  [150/196]  eta: 0:01:47  lr: 0.000062  min_lr: 0.000062  loss: 4.4551 (4.4514)  class_acc: 0.0469 (0.0476)  weight_decay: 0.0089 (0.0318)  time: 2.3429  data: 0.0066  max mem: 1751
Epoch: [0]  [160/196]  eta: 0:01:24  lr: 0.000066  min_lr: 0.000066  loss: 4.4581 (4.4521)  class_acc: 0.0469 (0.0476)  weight_decay: 0.0060 (0.0302)  time: 2.3416  data: 0.0056  max mem: 1751
Epoch: [0]  [170/196]  eta: 0:01:00  lr: 0.000070  min_lr: 0.000070  loss: 4.4581 (4.4522)  class_acc: 0.0469 (0.0480)  weight_decay: 0.0037 (0.0286)  time: 2.3448  data: 0.0066  max mem: 1751
Epoch: [0]  [180/196]  eta: 0:00:37  lr: 0.000074  min_lr: 0.000074  loss: 4.4448 (4.4511)  class_acc: 0.0508 (0.0483)  weight_decay: 0.0018 (0.0271)  time: 2.3406  data: 0.0062  max mem: 1751
Epoch: [0]  [190/196]  eta: 0:00:14  lr: 0.000078  min_lr: 0.000078  loss: 4.4128 (4.4491)  class_acc: 0.0508 (0.0486)  weight_decay: 0.0006 (0.0257)  time: 2.3342  data: 0.0045  max mem: 1751
Epoch: [0]  [195/196]  eta: 0:00:02  lr: 0.000080  min_lr: 0.000080  loss: 4.4204 (4.4493)  class_acc: 0.0508 (0.0487)  weight_decay: 0.0003 (0.0251)  time: 2.2186  data: 0.0043  max mem: 1751
Epoch: [0] Total time: 0:07:36 (2.3292 s / it)
Averaged stats: lr: 0.000080  min_lr: 0.000080  loss: 4.4204 (4.4493)  class_acc: 0.0508 (0.0487)  weight_decay: 0.0003 (0.0251)
Test:  [  0/196]  eta: 0:04:26  loss: 4.4098 (4.4098)  acc1: 3.5156 (3.5156)  acc5: 17.1875 (17.1875)  time: 1.3603  data: 1.1499  max mem: 1751
Test:  [ 10/196]  eta: 0:02:16  loss: 4.4098 (4.4238)  acc1: 4.2969 (4.8651)  acc5: 17.1875 (16.9744)  time: 0.7319  data: 0.2236  max mem: 1751
Test:  [ 20/196]  eta: 0:01:58  loss: 4.3946 (4.4130)  acc1: 4.6875 (5.2083)  acc5: 17.1875 (17.2805)  time: 0.6372  data: 0.0679  max mem: 1751
Test:  [ 30/196]  eta: 0:01:48  loss: 4.4391 (4.4213)  acc1: 4.2969 (5.0277)  acc5: 17.1875 (17.2001)  time: 0.6086  data: 0.0058  max mem: 1751
Test:  [ 40/196]  eta: 0:01:30  loss: 4.4459 (4.4238)  acc1: 4.2969 (5.0686)  acc5: 16.4062 (17.1113)  time: 0.4830  data: 0.0114  max mem: 1751
Test:  [ 50/196]  eta: 0:01:18  loss: 4.4346 (4.4250)  acc1: 5.4688 (5.1471)  acc5: 17.5781 (17.2258)  time: 0.3564  data: 0.0218  max mem: 1751
Test:  [ 60/196]  eta: 0:01:08  loss: 4.4137 (4.4257)  acc1: 5.0781 (5.0397)  acc5: 17.5781 (17.1107)  time: 0.3417  data: 0.0216  max mem: 1751
Test:  [ 70/196]  eta: 0:01:05  loss: 4.4184 (4.4251)  acc1: 4.6875 (5.1827)  acc5: 16.4062 (17.0940)  time: 0.4694  data: 0.0127  max mem: 1751
Test:  [ 80/196]  eta: 0:01:01  loss: 4.4168 (4.4232)  acc1: 5.0781 (5.1890)  acc5: 17.1875 (17.1345)  time: 0.6118  data: 0.0085  max mem: 1751
Test:  [ 90/196]  eta: 0:00:54  loss: 4.4106 (4.4235)  acc1: 5.0781 (5.2241)  acc5: 17.1875 (17.1188)  time: 0.5036  data: 0.0120  max mem: 1751
Test:  [100/196]  eta: 0:00:47  loss: 4.4353 (4.4252)  acc1: 5.0781 (5.2212)  acc5: 16.4062 (17.1140)  time: 0.3555  data: 0.0118  max mem: 1751
Test:  [110/196]  eta: 0:00:41  loss: 4.4139 (4.4213)  acc1: 5.4688 (5.2541)  acc5: 17.5781 (17.2508)  time: 0.3123  data: 0.0080  max mem: 1751
Test:  [120/196]  eta: 0:00:36  loss: 4.4028 (4.4205)  acc1: 5.4688 (5.2621)  acc5: 19.1406 (17.3328)  time: 0.4165  data: 0.0297  max mem: 1751
Test:  [130/196]  eta: 0:00:32  loss: 4.4203 (4.4206)  acc1: 5.4688 (5.3077)  acc5: 17.1875 (17.2620)  time: 0.5657  data: 0.0286  max mem: 1751
Test:  [140/196]  eta: 0:00:27  loss: 4.4342 (4.4221)  acc1: 5.4688 (5.2998)  acc5: 16.0156 (17.1847)  time: 0.5057  data: 0.0051  max mem: 1751
Test:  [150/196]  eta: 0:00:21  loss: 4.4411 (4.4229)  acc1: 5.0781 (5.2721)  acc5: 16.4062 (17.1694)  time: 0.3541  data: 0.0042  max mem: 1751
Test:  [160/196]  eta: 0:00:16  loss: 4.4348 (4.4230)  acc1: 4.6875 (5.2722)  acc5: 16.7969 (17.1802)  time: 0.3508  data: 0.0048  max mem: 1751
Test:  [170/196]  eta: 0:00:12  loss: 4.4329 (4.4235)  acc1: 5.0781 (5.2700)  acc5: 17.5781 (17.2081)  time: 0.4997  data: 0.0039  max mem: 1751
Test:  [180/196]  eta: 0:00:07  loss: 4.4329 (4.4237)  acc1: 4.6875 (5.2465)  acc5: 17.1875 (17.1961)  time: 0.6039  data: 0.0040  max mem: 1751
Test:  [190/196]  eta: 0:00:02  loss: 4.4350 (4.4243)  acc1: 4.6875 (5.2438)  acc5: 17.1875 (17.2039)  time: 0.4599  data: 0.0039  max mem: 1751
Test:  [195/196]  eta: 0:00:00  loss: 4.4295 (4.4237)  acc1: 5.4688 (5.2540)  acc5: 17.1875 (17.1960)  time: 0.3763  data: 0.0033  max mem: 1751
Test: Total time: 0:01:32 (0.4718 s / it)
* Acc@1 5.254 Acc@5 17.196 loss 4.424
Test:  [ 0/40]  eta: 0:01:14  loss: 4.0743 (4.0743)  acc1: 10.9375 (10.9375)  acc5: 25.7812 (25.7812)  time: 1.8656  data: 1.5876  max mem: 1751
Test:  [10/40]  eta: 0:00:14  loss: 4.0942 (4.0964)  acc1: 9.7656 (9.3750)  acc5: 26.5625 (26.5980)  time: 0.4867  data: 0.1809  max mem: 1751
Test:  [20/40]  eta: 0:00:08  loss: 4.1166 (4.0998)  acc1: 8.9844 (9.3192)  acc5: 26.5625 (26.8601)  time: 0.3272  data: 0.0218  max mem: 1751
Test:  [30/40]  eta: 0:00:03  loss: 4.1166 (4.1019)  acc1: 8.9844 (9.1356)  acc5: 26.1719 (26.7011)  time: 0.3062  data: 0.0040  max mem: 1751
Test:  [39/40]  eta: 0:00:00  loss: 4.0906 (4.1016)  acc1: 8.2031 (9.0200)  acc5: 25.7812 (26.8000)  time: 0.3729  data: 0.0051  max mem: 1751
Test: Total time: 0:00:15 (0.3977 s / it)
* Acc@1 9.020 Acc@5 26.800 loss 4.102
**************Prune Round 3**********************

Current sparsity level: 0.9290692901050248
Processing block 0
Block 0 - Current sparsity level: 1.0000
Processing block 1
Block 1 - Current sparsity level: 1.0000
Processing block 2
Block 2 - Current sparsity level: 1.0000
Processing block 3
Block 3 - Current sparsity level: 1.0000
Actual sparsity after pruning: 0.9290692901050248
Epoch: [0]  [  0/196]  eta: 0:20:16  lr: 0.000000  min_lr: 0.000000  loss: 4.5218 (4.5218)  class_acc: 0.0352 (0.0352)  weight_decay: 0.0500 (0.0500)  time: 6.2060  data: 4.1575  max mem: 1751
Epoch: [0]  [ 10/196]  eta: 0:08:19  lr: 0.000004  min_lr: 0.000004  loss: 4.4071 (4.4038)  class_acc: 0.0508 (0.0550)  weight_decay: 0.0499 (0.0499)  time: 2.6880  data: 0.3825  max mem: 1751
Epoch: [0]  [ 20/196]  eta: 0:07:23  lr: 0.000008  min_lr: 0.000008  loss: 4.4090 (4.4144)  class_acc: 0.0547 (0.0539)  weight_decay: 0.0496 (0.0496)  time: 2.3369  data: 0.0042  max mem: 1751
Epoch: [0]  [ 30/196]  eta: 0:06:49  lr: 0.000012  min_lr: 0.000012  loss: 4.4240 (4.4192)  class_acc: 0.0547 (0.0548)  weight_decay: 0.0486 (0.0490)  time: 2.3422  data: 0.0054  max mem: 1751
Epoch: [0]  [ 40/196]  eta: 0:06:19  lr: 0.000016  min_lr: 0.000016  loss: 4.4122 (4.4129)  class_acc: 0.0547 (0.0543)  weight_decay: 0.0469 (0.0483)  time: 2.3462  data: 0.0077  max mem: 1751
Epoch: [0]  [ 50/196]  eta: 0:05:52  lr: 0.000021  min_lr: 0.000021  loss: 4.3939 (4.4114)  class_acc: 0.0508 (0.0534)  weight_decay: 0.0447 (0.0474)  time: 2.3422  data: 0.0068  max mem: 1751
Epoch: [0]  [ 60/196]  eta: 0:05:27  lr: 0.000025  min_lr: 0.000025  loss: 4.3989 (4.4117)  class_acc: 0.0547 (0.0546)  weight_decay: 0.0420 (0.0463)  time: 2.3410  data: 0.0062  max mem: 1751
Epoch: [0]  [ 70/196]  eta: 0:05:01  lr: 0.000029  min_lr: 0.000029  loss: 4.3988 (4.4090)  class_acc: 0.0586 (0.0551)  weight_decay: 0.0389 (0.0450)  time: 2.3392  data: 0.0055  max mem: 1751
Epoch: [0]  [ 80/196]  eta: 0:04:37  lr: 0.000033  min_lr: 0.000033  loss: 4.4140 (4.4096)  class_acc: 0.0547 (0.0552)  weight_decay: 0.0354 (0.0436)  time: 2.3420  data: 0.0058  max mem: 1751
Epoch: [0]  [ 90/196]  eta: 0:04:12  lr: 0.000037  min_lr: 0.000037  loss: 4.4353 (4.4126)  class_acc: 0.0508 (0.0550)  weight_decay: 0.0316 (0.0421)  time: 2.3440  data: 0.0069  max mem: 1751
Epoch: [0]  [100/196]  eta: 0:03:48  lr: 0.000041  min_lr: 0.000041  loss: 4.4205 (4.4105)  class_acc: 0.0586 (0.0562)  weight_decay: 0.0276 (0.0405)  time: 2.3362  data: 0.0053  max mem: 1751
Epoch: [0]  [110/196]  eta: 0:03:24  lr: 0.000045  min_lr: 0.000045  loss: 4.3870 (4.4085)  class_acc: 0.0664 (0.0563)  weight_decay: 0.0236 (0.0388)  time: 2.3321  data: 0.0039  max mem: 1751
Epoch: [0]  [120/196]  eta: 0:03:00  lr: 0.000049  min_lr: 0.000049  loss: 4.3926 (4.4076)  class_acc: 0.0547 (0.0558)  weight_decay: 0.0196 (0.0371)  time: 2.3337  data: 0.0037  max mem: 1751
Epoch: [0]  [130/196]  eta: 0:02:36  lr: 0.000053  min_lr: 0.000053  loss: 4.3881 (4.4061)  class_acc: 0.0547 (0.0556)  weight_decay: 0.0158 (0.0353)  time: 2.3386  data: 0.0056  max mem: 1751
