Set warmup steps = 9750
Set warmup steps = 0
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.dwconv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.dwconv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.dwconv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.dwconv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.dwconv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.dwconv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.dwconv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.dwconv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.dwconv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.dwconv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.dwconv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.dwconv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.dwconv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.dwconv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.dwconv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.dwconv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.dwconv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.dwconv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.dwconv.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.dwconv.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.dwconv.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.dwconv.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.dwconv.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.dwconv.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.dwconv.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.dwconv.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.dwconv.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.dwconv.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.dwconv.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.dwconv.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.dwconv.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.dwconv.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.dwconv.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.dwconv.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.dwconv.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.dwconv.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
/home/shahanahmed/.local/lib/python3.10/site-packages/timm/utils/cuda.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
**************Prune Round 1**********************

Current sparsity level: 0.0017724827175138512
Processing block 0
Layer 0.pwconv1 sparsity: 0.5000
Layer 0.pwconv2 sparsity: 0.5000
Layer 1.pwconv1 sparsity: 0.5000
Layer 1.pwconv2 sparsity: 0.5000
Layer 2.pwconv1 sparsity: 0.5000
Layer 2.pwconv2 sparsity: 0.5000
Processing block 1
Layer 0.pwconv1 sparsity: 0.5000
Layer 0.pwconv2 sparsity: 0.5000
Layer 1.pwconv1 sparsity: 0.5000
Layer 1.pwconv2 sparsity: 0.5000
Layer 2.pwconv1 sparsity: 0.5000
Layer 2.pwconv2 sparsity: 0.5000
Processing block 2
Layer 0.pwconv1 sparsity: 0.5000
Layer 0.pwconv2 sparsity: 0.5000
Layer 1.pwconv1 sparsity: 0.5000
Layer 1.pwconv2 sparsity: 0.5000
Layer 2.pwconv1 sparsity: 0.5000
Layer 2.pwconv2 sparsity: 0.5000
Layer 3.pwconv1 sparsity: 0.5000
Layer 3.pwconv2 sparsity: 0.5000
Layer 4.pwconv1 sparsity: 0.5000
Layer 4.pwconv2 sparsity: 0.5000
Layer 5.pwconv1 sparsity: 0.5000
Layer 5.pwconv2 sparsity: 0.5000
Layer 6.pwconv1 sparsity: 0.5000
Layer 6.pwconv2 sparsity: 0.5000
Layer 7.pwconv1 sparsity: 0.5000
Layer 7.pwconv2 sparsity: 0.5000
Layer 8.pwconv1 sparsity: 0.5000
Layer 8.pwconv2 sparsity: 0.5000
Processing block 3
Layer 0.pwconv1 sparsity: 0.5000
Layer 0.pwconv2 sparsity: 0.5000
Layer 1.pwconv1 sparsity: 0.5000
Layer 1.pwconv2 sparsity: 0.5000
Layer 2.pwconv1 sparsity: 0.5000
Layer 2.pwconv2 sparsity: 0.5000
Overall model sparsity: 0.5000
Overall sparsity after pruning: 0.46559475797923705
/home/shahanahmed/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch: [0]  [  0/196]  eta: 0:42:50  lr: 0.000000  min_lr: 0.000000  loss: 4.7706 (4.7706)  class_acc: 0.0117 (0.0117)  weight_decay: 0.0500 (0.0500)  time: 13.1151  data: 1.4910  max mem: 1751
Epoch: [0]  [ 10/196]  eta: 0:07:57  lr: 0.000004  min_lr: 0.000004  loss: 4.7371 (4.7339)  class_acc: 0.0117 (0.0092)  weight_decay: 0.0499 (0.0499)  time: 2.5673  data: 0.1374  max mem: 1751
Epoch: [0]  [ 20/196]  eta: 0:06:11  lr: 0.000008  min_lr: 0.000008  loss: 4.7310 (4.7297)  class_acc: 0.0117 (0.0091)  weight_decay: 0.0496 (0.0496)  time: 1.5634  data: 0.0018  max mem: 1751
Epoch: [0]  [ 30/196]  eta: 0:05:28  lr: 0.000012  min_lr: 0.000012  loss: 4.7134 (4.7221)  class_acc: 0.0117 (0.0100)  weight_decay: 0.0486 (0.0490)  time: 1.6538  data: 0.0023  max mem: 1751
Epoch: [0]  [ 40/196]  eta: 0:05:01  lr: 0.000016  min_lr: 0.000016  loss: 4.6910 (4.7112)  class_acc: 0.0156 (0.0114)  weight_decay: 0.0469 (0.0483)  time: 1.7377  data: 0.0039  max mem: 1751
Epoch: [0]  [ 50/196]  eta: 0:04:40  lr: 0.000021  min_lr: 0.000021  loss: 4.6698 (4.7002)  class_acc: 0.0117 (0.0123)  weight_decay: 0.0447 (0.0474)  time: 1.8254  data: 0.0049  max mem: 1751
Epoch: [0]  [ 60/196]  eta: 0:04:20  lr: 0.000025  min_lr: 0.000025  loss: 4.6302 (4.6883)  class_acc: 0.0117 (0.0134)  weight_decay: 0.0420 (0.0463)  time: 1.8887  data: 0.0060  max mem: 1751
Epoch: [0]  [ 70/196]  eta: 0:04:02  lr: 0.000029  min_lr: 0.000029  loss: 4.6213 (4.6776)  class_acc: 0.0195 (0.0147)  weight_decay: 0.0389 (0.0450)  time: 1.9296  data: 0.0071  max mem: 1751
Epoch: [0]  [ 80/196]  eta: 0:03:45  lr: 0.000033  min_lr: 0.000033  loss: 4.6023 (4.6683)  class_acc: 0.0195 (0.0154)  weight_decay: 0.0354 (0.0436)  time: 2.0412  data: 0.0065  max mem: 1751
Epoch: [0]  [ 90/196]  eta: 0:03:29  lr: 0.000037  min_lr: 0.000037  loss: 4.5946 (4.6599)  class_acc: 0.0234 (0.0169)  weight_decay: 0.0316 (0.0421)  time: 2.1796  data: 0.0068  max mem: 1751
Epoch: [0]  [100/196]  eta: 0:03:13  lr: 0.000041  min_lr: 0.000041  loss: 4.5711 (4.6491)  class_acc: 0.0273 (0.0180)  weight_decay: 0.0276 (0.0405)  time: 2.2853  data: 0.0061  max mem: 1751
Epoch: [0]  [110/196]  eta: 0:02:55  lr: 0.000045  min_lr: 0.000045  loss: 4.5451 (4.6396)  class_acc: 0.0312 (0.0195)  weight_decay: 0.0236 (0.0388)  time: 2.3427  data: 0.0049  max mem: 1751
Epoch: [0]  [120/196]  eta: 0:02:37  lr: 0.000049  min_lr: 0.000049  loss: 4.5451 (4.6324)  class_acc: 0.0352 (0.0209)  weight_decay: 0.0196 (0.0371)  time: 2.3415  data: 0.0051  max mem: 1751
Epoch: [0]  [130/196]  eta: 0:02:17  lr: 0.000053  min_lr: 0.000053  loss: 4.5343 (4.6240)  class_acc: 0.0391 (0.0223)  weight_decay: 0.0158 (0.0353)  time: 2.3399  data: 0.0046  max mem: 1751
Epoch: [0]  [140/196]  eta: 0:01:57  lr: 0.000057  min_lr: 0.000057  loss: 4.5258 (4.6168)  class_acc: 0.0391 (0.0236)  weight_decay: 0.0122 (0.0336)  time: 2.3408  data: 0.0051  max mem: 1751
Epoch: [0]  [150/196]  eta: 0:01:37  lr: 0.000062  min_lr: 0.000062  loss: 4.5132 (4.6094)  class_acc: 0.0430 (0.0248)  weight_decay: 0.0089 (0.0318)  time: 2.3426  data: 0.0056  max mem: 1751
Epoch: [0]  [160/196]  eta: 0:01:16  lr: 0.000066  min_lr: 0.000066  loss: 4.5132 (4.6039)  class_acc: 0.0430 (0.0257)  weight_decay: 0.0060 (0.0302)  time: 2.3437  data: 0.0058  max mem: 1751
Epoch: [0]  [170/196]  eta: 0:00:55  lr: 0.000070  min_lr: 0.000070  loss: 4.5178 (4.5987)  class_acc: 0.0352 (0.0262)  weight_decay: 0.0037 (0.0286)  time: 2.3436  data: 0.0057  max mem: 1751
Epoch: [0]  [180/196]  eta: 0:00:34  lr: 0.000074  min_lr: 0.000074  loss: 4.4948 (4.5924)  class_acc: 0.0391 (0.0269)  weight_decay: 0.0018 (0.0271)  time: 2.3414  data: 0.0052  max mem: 1751
Epoch: [0]  [190/196]  eta: 0:00:13  lr: 0.000078  min_lr: 0.000078  loss: 4.4623 (4.5867)  class_acc: 0.0430 (0.0279)  weight_decay: 0.0006 (0.0257)  time: 2.3429  data: 0.0058  max mem: 1751
Epoch: [0]  [195/196]  eta: 0:00:02  lr: 0.000080  min_lr: 0.000080  loss: 4.4770 (4.5850)  class_acc: 0.0430 (0.0281)  weight_decay: 0.0003 (0.0251)  time: 2.2271  data: 0.0056  max mem: 1751
Epoch: [0] Total time: 0:07:04 (2.1634 s / it)
Averaged stats: lr: 0.000080  min_lr: 0.000080  loss: 4.4770 (4.5850)  class_acc: 0.0430 (0.0281)  weight_decay: 0.0003 (0.0251)
/home/shahanahmed/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test:  [  0/196]  eta: 0:10:42  loss: 4.4590 (4.4590)  acc1: 6.6406 (6.6406)  acc5: 15.6250 (15.6250)  time: 3.2793  data: 2.7334  max mem: 1751
Test:  [ 10/196]  eta: 0:02:43  loss: 4.4736 (4.4701)  acc1: 5.0781 (4.7940)  acc5: 15.2344 (15.4830)  time: 0.8801  data: 0.3473  max mem: 1751
Test:  [ 20/196]  eta: 0:02:11  loss: 4.4750 (4.4689)  acc1: 4.2969 (4.5015)  acc5: 14.8438 (15.3460)  time: 0.6228  data: 0.0565  max mem: 1751
Test:  [ 30/196]  eta: 0:01:56  loss: 4.4791 (4.4709)  acc1: 4.2969 (4.6497)  acc5: 14.8438 (15.5620)  time: 0.6070  data: 0.0054  max mem: 1751
Test:  [ 40/196]  eta: 0:01:37  loss: 4.4804 (4.4713)  acc1: 4.6875 (4.6303)  acc5: 14.8438 (15.5107)  time: 0.4933  data: 0.0086  max mem: 1751
Test:  [ 50/196]  eta: 0:01:26  loss: 4.4938 (4.4776)  acc1: 3.5156 (4.5803)  acc5: 14.4531 (15.2880)  time: 0.4202  data: 0.0104  max mem: 1751
Test:  [ 60/196]  eta: 0:01:20  loss: 4.4911 (4.4801)  acc1: 3.5156 (4.4057)  acc5: 14.0625 (15.0679)  time: 0.5340  data: 0.0079  max mem: 1751
Test:  [ 70/196]  eta: 0:01:10  loss: 4.4800 (4.4774)  acc1: 3.9062 (4.3959)  acc5: 14.0625 (15.0968)  time: 0.4838  data: 0.0072  max mem: 1751
Test:  [ 80/196]  eta: 0:01:04  loss: 4.4739 (4.4785)  acc1: 4.6875 (4.4994)  acc5: 15.2344 (15.1042)  time: 0.4261  data: 0.0088  max mem: 1751
Test:  [ 90/196]  eta: 0:00:59  loss: 4.4869 (4.4788)  acc1: 4.6875 (4.4857)  acc5: 15.2344 (15.2129)  time: 0.5446  data: 0.0062  max mem: 1751
Test:  [100/196]  eta: 0:00:53  loss: 4.4869 (4.4807)  acc1: 4.2969 (4.4748)  acc5: 15.2344 (15.1648)  time: 0.5993  data: 0.0032  max mem: 1751
Test:  [110/196]  eta: 0:00:48  loss: 4.4796 (4.4818)  acc1: 4.2969 (4.4060)  acc5: 14.4531 (14.9986)  time: 0.5988  data: 0.0027  max mem: 1751
Test:  [120/196]  eta: 0:00:41  loss: 4.4577 (4.4800)  acc1: 4.2969 (4.4615)  acc5: 15.2344 (15.0730)  time: 0.4835  data: 0.0030  max mem: 1751
Test:  [130/196]  eta: 0:00:36  loss: 4.4519 (4.4790)  acc1: 5.0781 (4.5026)  acc5: 15.6250 (15.1091)  time: 0.4687  data: 0.0030  max mem: 1751
Test:  [140/196]  eta: 0:00:31  loss: 4.4627 (4.4799)  acc1: 3.9062 (4.4659)  acc5: 15.2344 (15.0682)  time: 0.5900  data: 0.0052  max mem: 1751
Test:  [150/196]  eta: 0:00:25  loss: 4.4876 (4.4801)  acc1: 4.2969 (4.4909)  acc5: 13.6719 (15.0222)  time: 0.6081  data: 0.0066  max mem: 1751
Test:  [160/196]  eta: 0:00:19  loss: 4.4701 (4.4780)  acc1: 4.6875 (4.4885)  acc5: 14.8438 (15.0985)  time: 0.4754  data: 0.0054  max mem: 1751
Test:  [170/196]  eta: 0:00:14  loss: 4.4402 (4.4765)  acc1: 3.9062 (4.4910)  acc5: 16.4062 (15.1407)  time: 0.4451  data: 0.0047  max mem: 1751
Test:  [180/196]  eta: 0:00:08  loss: 4.4714 (4.4776)  acc1: 4.2969 (4.4933)  acc5: 14.4531 (15.1286)  time: 0.5705  data: 0.0029  max mem: 1751
Test:  [190/196]  eta: 0:00:03  loss: 4.4910 (4.4785)  acc1: 4.2969 (4.4932)  acc5: 14.0625 (15.1055)  time: 0.5960  data: 0.0016  max mem: 1751
Test:  [195/196]  eta: 0:00:00  loss: 4.4910 (4.4778)  acc1: 4.2969 (4.4940)  acc5: 13.7500 (15.0940)  time: 0.6155  data: 0.0014  max mem: 1751
Test: Total time: 0:01:48 (0.5547 s / it)
* Acc@1 4.494 Acc@5 15.094 loss 4.478
Test:  [ 0/40]  eta: 0:00:32  loss: 4.1772 (4.1772)  acc1: 7.8125 (7.8125)  acc5: 26.1719 (26.1719)  time: 0.8118  data: 0.6055  max mem: 1751
Test:  [10/40]  eta: 0:00:12  loss: 4.1798 (4.1938)  acc1: 8.2031 (7.7415)  acc5: 23.8281 (23.3665)  time: 0.4238  data: 0.0562  max mem: 1751
Test:  [20/40]  eta: 0:00:10  loss: 4.2001 (4.1972)  acc1: 7.8125 (7.7753)  acc5: 23.0469 (23.3631)  time: 0.4915  data: 0.0011  max mem: 1751
Test:  [30/40]  eta: 0:00:05  loss: 4.2001 (4.2004)  acc1: 7.4219 (7.4597)  acc5: 22.6562 (23.0721)  time: 0.5750  data: 0.0012  max mem: 1751
Test:  [39/40]  eta: 0:00:00  loss: 4.1745 (4.1993)  acc1: 6.6406 (7.5700)  acc5: 24.2188 (23.4800)  time: 0.4415  data: 0.0015  max mem: 1751
Test: Total time: 0:00:19 (0.4760 s / it)
* Acc@1 7.570 Acc@5 23.480 loss 4.199
Training Accuracy: 0.00%
Testing Accuracy: 0.00%
Sparsity after training: 0.4638
**************Prune Round 2**********************

Current sparsity level: 0.4638223828000603
Processing block 0
Layer 0.pwconv1 sparsity: 0.5000
Layer 0.pwconv2 sparsity: 0.5000
Layer 1.pwconv1 sparsity: 0.5000
Layer 1.pwconv2 sparsity: 0.5000
Layer 2.pwconv1 sparsity: 0.5000
Layer 2.pwconv2 sparsity: 0.5000
Processing block 1
Layer 0.pwconv1 sparsity: 0.5000
Layer 0.pwconv2 sparsity: 0.5000
Layer 1.pwconv1 sparsity: 0.5000
Layer 1.pwconv2 sparsity: 0.5000
Layer 2.pwconv1 sparsity: 0.5000
Layer 2.pwconv2 sparsity: 0.5000
Processing block 2
Layer 0.pwconv1 sparsity: 0.5000
Layer 0.pwconv2 sparsity: 0.5000
Layer 1.pwconv1 sparsity: 0.5000
Layer 1.pwconv2 sparsity: 0.5000
Layer 2.pwconv1 sparsity: 0.5000
Layer 2.pwconv2 sparsity: 0.5000
Layer 3.pwconv1 sparsity: 0.5000
Layer 3.pwconv2 sparsity: 0.5000
Layer 4.pwconv1 sparsity: 0.5000
Layer 4.pwconv2 sparsity: 0.5000
Layer 5.pwconv1 sparsity: 0.5000
Layer 5.pwconv2 sparsity: 0.5000
Layer 6.pwconv1 sparsity: 0.5000
Layer 6.pwconv2 sparsity: 0.5000
Layer 7.pwconv1 sparsity: 0.5000
Layer 7.pwconv2 sparsity: 0.5000
Layer 8.pwconv1 sparsity: 0.5000
Layer 8.pwconv2 sparsity: 0.5000
Processing block 3
Layer 0.pwconv1 sparsity: 0.5000
Layer 0.pwconv2 sparsity: 0.5000
Layer 1.pwconv1 sparsity: 0.5000
Layer 1.pwconv2 sparsity: 0.5000
Layer 2.pwconv1 sparsity: 0.5000
Layer 2.pwconv2 sparsity: 0.5000
Overall model sparsity: 0.5000
Overall sparsity after pruning: 0.4638223828000603
Epoch: [0]  [  0/196]  eta: 0:20:52  lr: 0.000000  min_lr: 0.000000  loss: 4.5008 (4.5008)  class_acc: 0.0508 (0.0508)  weight_decay: 0.0500 (0.0500)  time: 6.3901  data: 4.2229  max mem: 1751
Epoch: [0]  [ 10/196]  eta: 0:08:22  lr: 0.000004  min_lr: 0.000004  loss: 4.4660 (4.4515)  class_acc: 0.0508 (0.0501)  weight_decay: 0.0499 (0.0499)  time: 2.7008  data: 0.3859  max mem: 1751
Epoch: [0]  [ 20/196]  eta: 0:07:24  lr: 0.000008  min_lr: 0.000008  loss: 4.4687 (4.4646)  class_acc: 0.0469 (0.0493)  weight_decay: 0.0496 (0.0496)  time: 2.3305  data: 0.0017  max mem: 1751
Epoch: [0]  [ 30/196]  eta: 0:06:48  lr: 0.000012  min_lr: 0.000012  loss: 4.4770 (4.4677)  class_acc: 0.0430 (0.0456)  weight_decay: 0.0486 (0.0490)  time: 2.3308  data: 0.0022  max mem: 1751
Epoch: [0]  [ 40/196]  eta: 0:06:19  lr: 0.000016  min_lr: 0.000016  loss: 4.4765 (4.4730)  class_acc: 0.0430 (0.0469)  weight_decay: 0.0469 (0.0483)  time: 2.3315  data: 0.0030  max mem: 1751
Epoch: [0]  [ 50/196]  eta: 0:05:54  lr: 0.000021  min_lr: 0.000021  loss: 4.4569 (4.4683)  class_acc: 0.0430 (0.0464)  weight_decay: 0.0447 (0.0474)  time: 2.3722  data: 0.0023  max mem: 1751
Epoch: [0]  [ 60/196]  eta: 0:05:31  lr: 0.000025  min_lr: 0.000025  loss: 4.4574 (4.4697)  class_acc: 0.0430 (0.0466)  weight_decay: 0.0420 (0.0463)  time: 2.4483  data: 0.0022  max mem: 1751
Epoch: [0]  [ 70/196]  eta: 0:05:06  lr: 0.000029  min_lr: 0.000029  loss: 4.4741 (4.4685)  class_acc: 0.0430 (0.0462)  weight_decay: 0.0389 (0.0450)  time: 2.4527  data: 0.0030  max mem: 1751
Epoch: [0]  [ 80/196]  eta: 0:04:42  lr: 0.000033  min_lr: 0.000033  loss: 4.4501 (4.4698)  class_acc: 0.0469 (0.0464)  weight_decay: 0.0354 (0.0436)  time: 2.4214  data: 0.0042  max mem: 1751
Epoch: [0]  [ 90/196]  eta: 0:04:18  lr: 0.000037  min_lr: 0.000037  loss: 4.4335 (4.4655)  class_acc: 0.0508 (0.0478)  weight_decay: 0.0316 (0.0421)  time: 2.4703  data: 0.0046  max mem: 1751
Epoch: [0]  [100/196]  eta: 0:03:56  lr: 0.000041  min_lr: 0.000041  loss: 4.4235 (4.4626)  class_acc: 0.0586 (0.0492)  weight_decay: 0.0276 (0.0405)  time: 2.5913  data: 0.0040  max mem: 1751
