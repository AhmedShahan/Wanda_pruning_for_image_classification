Set warmup steps = 9750
Set warmup steps = 0
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.dwconv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.dwconv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.dwconv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.dwconv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.dwconv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.dwconv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.dwconv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.dwconv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.dwconv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.dwconv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.dwconv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.dwconv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.dwconv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.dwconv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.dwconv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.dwconv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.dwconv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.dwconv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.dwconv.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.dwconv.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.dwconv.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.dwconv.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.dwconv.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.dwconv.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.dwconv.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.dwconv.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.dwconv.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.dwconv.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.dwconv.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.dwconv.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.dwconv.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.dwconv.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.dwconv.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.dwconv.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.dwconv.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.dwconv.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
/home/shahanahmed/.local/lib/python3.10/site-packages/timm/utils/cuda.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
**************Prune Round 1**********************

Current sparsity level: 0.0017724827175138512
Non-zero weights before pruning: 27847581
Processing block 0
Layer 0.pwconv1:
  - Previous non-zero weights: 36864
  - Weights pruned this round: 18433
  - Remaining non-zero weights: 18431
  - Layer sparsity: 0.5000
Layer 0.pwconv2:
  - Previous non-zero weights: 36864
  - Weights pruned this round: 18433
  - Remaining non-zero weights: 18431
  - Layer sparsity: 0.5000
Layer 1.pwconv1:
  - Previous non-zero weights: 36864
  - Weights pruned this round: 18433
  - Remaining non-zero weights: 18431
  - Layer sparsity: 0.5000
Layer 1.pwconv2:
  - Previous non-zero weights: 36864
  - Weights pruned this round: 18433
  - Remaining non-zero weights: 18431
  - Layer sparsity: 0.5000
Layer 2.pwconv1:
  - Previous non-zero weights: 36864
  - Weights pruned this round: 18433
  - Remaining non-zero weights: 18431
  - Layer sparsity: 0.5000
Layer 2.pwconv2:
  - Previous non-zero weights: 36864
  - Weights pruned this round: 18433
  - Remaining non-zero weights: 18431
  - Layer sparsity: 0.5000
Processing block 1
Layer 0.pwconv1:
  - Previous non-zero weights: 147456
  - Weights pruned this round: 73729
  - Remaining non-zero weights: 73727
  - Layer sparsity: 0.5000
Layer 0.pwconv2:
  - Previous non-zero weights: 147456
  - Weights pruned this round: 73729
  - Remaining non-zero weights: 73727
  - Layer sparsity: 0.5000
Layer 1.pwconv1:
  - Previous non-zero weights: 147456
  - Weights pruned this round: 73729
  - Remaining non-zero weights: 73727
  - Layer sparsity: 0.5000
Layer 1.pwconv2:
  - Previous non-zero weights: 147456
  - Weights pruned this round: 73729
  - Remaining non-zero weights: 73727
  - Layer sparsity: 0.5000
Layer 2.pwconv1:
  - Previous non-zero weights: 147456
  - Weights pruned this round: 73729
  - Remaining non-zero weights: 73727
  - Layer sparsity: 0.5000
Layer 2.pwconv2:
  - Previous non-zero weights: 147456
  - Weights pruned this round: 73729
  - Remaining non-zero weights: 73727
  - Layer sparsity: 0.5000
Processing block 2
Layer 0.pwconv1:
  - Previous non-zero weights: 589824
  - Weights pruned this round: 294913
  - Remaining non-zero weights: 294911
  - Layer sparsity: 0.5000
Layer 0.pwconv2:
  - Previous non-zero weights: 589824
  - Weights pruned this round: 294914
  - Remaining non-zero weights: 294910
  - Layer sparsity: 0.5000
Layer 1.pwconv1:
  - Previous non-zero weights: 589824
  - Weights pruned this round: 294913
  - Remaining non-zero weights: 294911
  - Layer sparsity: 0.5000
Layer 1.pwconv2:
  - Previous non-zero weights: 589824
  - Weights pruned this round: 294913
  - Remaining non-zero weights: 294911
  - Layer sparsity: 0.5000
Layer 2.pwconv1:
  - Previous non-zero weights: 589824
  - Weights pruned this round: 294913
  - Remaining non-zero weights: 294911
  - Layer sparsity: 0.5000
Layer 2.pwconv2:
  - Previous non-zero weights: 589824
  - Weights pruned this round: 294913
  - Remaining non-zero weights: 294911
  - Layer sparsity: 0.5000
Layer 3.pwconv1:
  - Previous non-zero weights: 589824
  - Weights pruned this round: 294913
  - Remaining non-zero weights: 294911
  - Layer sparsity: 0.5000
Layer 3.pwconv2:
  - Previous non-zero weights: 589824
  - Weights pruned this round: 294913
  - Remaining non-zero weights: 294911
  - Layer sparsity: 0.5000
Layer 4.pwconv1:
  - Previous non-zero weights: 589824
  - Weights pruned this round: 294913
  - Remaining non-zero weights: 294911
  - Layer sparsity: 0.5000
Layer 4.pwconv2:
  - Previous non-zero weights: 589824
  - Weights pruned this round: 294913
  - Remaining non-zero weights: 294911
  - Layer sparsity: 0.5000
Layer 5.pwconv1:
  - Previous non-zero weights: 589824
  - Weights pruned this round: 294913
  - Remaining non-zero weights: 294911
  - Layer sparsity: 0.5000
Layer 5.pwconv2:
  - Previous non-zero weights: 589824
  - Weights pruned this round: 294913
  - Remaining non-zero weights: 294911
  - Layer sparsity: 0.5000
Layer 6.pwconv1:
  - Previous non-zero weights: 589823
  - Weights pruned this round: 294913
  - Remaining non-zero weights: 294910
  - Layer sparsity: 0.5000
Layer 6.pwconv2:
  - Previous non-zero weights: 589824
  - Weights pruned this round: 294913
  - Remaining non-zero weights: 294911
  - Layer sparsity: 0.5000
Layer 7.pwconv1:
  - Previous non-zero weights: 589823
  - Weights pruned this round: 294913
  - Remaining non-zero weights: 294910
  - Layer sparsity: 0.5000
Layer 7.pwconv2:
  - Previous non-zero weights: 589824
  - Weights pruned this round: 294913
  - Remaining non-zero weights: 294911
  - Layer sparsity: 0.5000
Layer 8.pwconv1:
  - Previous non-zero weights: 589824
  - Weights pruned this round: 294913
  - Remaining non-zero weights: 294911
  - Layer sparsity: 0.5000
Layer 8.pwconv2:
  - Previous non-zero weights: 589824
  - Weights pruned this round: 294913
  - Remaining non-zero weights: 294911
  - Layer sparsity: 0.5000
Processing block 3
Layer 0.pwconv1:
  - Previous non-zero weights: 2359296
  - Weights pruned this round: 1179649
  - Remaining non-zero weights: 1179647
  - Layer sparsity: 0.5000
Layer 0.pwconv2:
  - Previous non-zero weights: 2359296
  - Weights pruned this round: 1179649
  - Remaining non-zero weights: 1179647
  - Layer sparsity: 0.5000
Layer 1.pwconv1:
  - Previous non-zero weights: 2359296
  - Weights pruned this round: 1179649
  - Remaining non-zero weights: 1179647
  - Layer sparsity: 0.5000
Layer 1.pwconv2:
  - Previous non-zero weights: 2359296
  - Weights pruned this round: 1179649
  - Remaining non-zero weights: 1179647
  - Layer sparsity: 0.5000
Layer 2.pwconv1:
  - Previous non-zero weights: 2359295
  - Weights pruned this round: 1179649
  - Remaining non-zero weights: 1179646
  - Layer sparsity: 0.5000
Layer 2.pwconv2:
  - Previous non-zero weights: 2359296
  - Weights pruned this round: 1179649
  - Remaining non-zero weights: 1179647
  - Layer sparsity: 0.5000
Overall model sparsity: 0.5000
Sparsity after pruning: 0.4656
Non-zero weights after pruning: 14908280

Epoch 1/1
/home/shahanahmed/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch: [0]  [  0/196]  eta: 0:43:54  lr: 0.000000  min_lr: 0.000000  loss: 4.7326 (4.7326)  class_acc: 0.0117 (0.0117)  weight_decay: 0.0500 (0.0500)  time: 13.4403  data: 0.8277  max mem: 1754
Epoch: [0]  [ 10/196]  eta: 0:08:46  lr: 0.000004  min_lr: 0.000004  loss: 4.7293 (4.7316)  class_acc: 0.0117 (0.0107)  weight_decay: 0.0499 (0.0499)  time: 2.8291  data: 0.0795  max mem: 1754
Epoch: [0]  [ 20/196]  eta: 0:06:57  lr: 0.000008  min_lr: 0.000008  loss: 4.7293 (4.7388)  class_acc: 0.0117 (0.0099)  weight_decay: 0.0496 (0.0496)  time: 1.8201  data: 0.0028  max mem: 1754
Epoch: [0]  [ 30/196]  eta: 0:06:24  lr: 0.000012  min_lr: 0.000012  loss: 4.7268 (4.7300)  class_acc: 0.0078 (0.0097)  weight_decay: 0.0486 (0.0490)  time: 2.0303  data: 0.0011  max mem: 1754
Epoch: [0]  [ 40/196]  eta: 0:06:01  lr: 0.000016  min_lr: 0.000016  loss: 4.6866 (4.7208)  class_acc: 0.0117 (0.0103)  weight_decay: 0.0469 (0.0483)  time: 2.2539  data: 0.0013  max mem: 1754
Epoch: [0]  [ 50/196]  eta: 0:05:38  lr: 0.000021  min_lr: 0.000021  loss: 4.6660 (4.7090)  class_acc: 0.0156 (0.0119)  weight_decay: 0.0447 (0.0474)  time: 2.3193  data: 0.0014  max mem: 1754
Epoch: [0]  [ 60/196]  eta: 0:05:15  lr: 0.000025  min_lr: 0.000025  loss: 4.6542 (4.6973)  class_acc: 0.0195 (0.0135)  weight_decay: 0.0420 (0.0463)  time: 2.3201  data: 0.0014  max mem: 1754
Epoch: [0]  [ 70/196]  eta: 0:04:51  lr: 0.000029  min_lr: 0.000029  loss: 4.6192 (4.6843)  class_acc: 0.0273 (0.0157)  weight_decay: 0.0389 (0.0450)  time: 2.3202  data: 0.0014  max mem: 1754
Epoch: [0]  [ 80/196]  eta: 0:04:28  lr: 0.000033  min_lr: 0.000033  loss: 4.5974 (4.6734)  class_acc: 0.0273 (0.0169)  weight_decay: 0.0354 (0.0436)  time: 2.3196  data: 0.0013  max mem: 1754
Epoch: [0]  [ 90/196]  eta: 0:04:05  lr: 0.000037  min_lr: 0.000037  loss: 4.5872 (4.6648)  class_acc: 0.0273 (0.0185)  weight_decay: 0.0316 (0.0421)  time: 2.3204  data: 0.0013  max mem: 1754
Epoch: [0]  [100/196]  eta: 0:03:42  lr: 0.000041  min_lr: 0.000041  loss: 4.5921 (4.6577)  class_acc: 0.0234 (0.0190)  weight_decay: 0.0276 (0.0405)  time: 2.3213  data: 0.0014  max mem: 1754
Epoch: [0]  [110/196]  eta: 0:03:19  lr: 0.000045  min_lr: 0.000045  loss: 4.5778 (4.6491)  class_acc: 0.0273 (0.0201)  weight_decay: 0.0236 (0.0388)  time: 2.3215  data: 0.0014  max mem: 1754
Epoch: [0]  [120/196]  eta: 0:02:56  lr: 0.000049  min_lr: 0.000049  loss: 4.5728 (4.6414)  class_acc: 0.0273 (0.0208)  weight_decay: 0.0196 (0.0371)  time: 2.3215  data: 0.0013  max mem: 1754
Epoch: [0]  [130/196]  eta: 0:02:33  lr: 0.000053  min_lr: 0.000053  loss: 4.5353 (4.6308)  class_acc: 0.0312 (0.0219)  weight_decay: 0.0158 (0.0353)  time: 2.3246  data: 0.0017  max mem: 1754
Epoch: [0]  [140/196]  eta: 0:02:10  lr: 0.000057  min_lr: 0.000057  loss: 4.5205 (4.6236)  class_acc: 0.0352 (0.0230)  weight_decay: 0.0122 (0.0336)  time: 2.3695  data: 0.0028  max mem: 1754
Epoch: [0]  [150/196]  eta: 0:01:47  lr: 0.000062  min_lr: 0.000062  loss: 4.5205 (4.6163)  class_acc: 0.0391 (0.0245)  weight_decay: 0.0089 (0.0318)  time: 2.4120  data: 0.0046  max mem: 1754
Epoch: [0]  [160/196]  eta: 0:01:24  lr: 0.000066  min_lr: 0.000066  loss: 4.5081 (4.6095)  class_acc: 0.0430 (0.0254)  weight_decay: 0.0060 (0.0302)  time: 2.4174  data: 0.0063  max mem: 1754
Epoch: [0]  [170/196]  eta: 0:01:00  lr: 0.000070  min_lr: 0.000070  loss: 4.5072 (4.6033)  class_acc: 0.0391 (0.0260)  weight_decay: 0.0037 (0.0286)  time: 2.4215  data: 0.0064  max mem: 1754
Epoch: [0]  [180/196]  eta: 0:00:37  lr: 0.000074  min_lr: 0.000074  loss: 4.5002 (4.5971)  class_acc: 0.0391 (0.0270)  weight_decay: 0.0018 (0.0271)  time: 2.3798  data: 0.0063  max mem: 1754
Epoch: [0]  [190/196]  eta: 0:00:14  lr: 0.000078  min_lr: 0.000078  loss: 4.4819 (4.5906)  class_acc: 0.0430 (0.0278)  weight_decay: 0.0006 (0.0257)  time: 2.4600  data: 0.0060  max mem: 1754
Epoch: [0]  [195/196]  eta: 0:00:02  lr: 0.000080  min_lr: 0.000080  loss: 4.4819 (4.5888)  class_acc: 0.0430 (0.0283)  weight_decay: 0.0003 (0.0251)  time: 2.3430  data: 0.0055  max mem: 1754
Epoch: [0] Total time: 0:07:39 (2.3438 s / it)
Averaged stats: lr: 0.000080  min_lr: 0.000080  loss: 4.4819 (4.5888)  class_acc: 0.0430 (0.0283)  weight_decay: 0.0003 (0.0251)
/home/shahanahmed/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test:  [  0/196]  eta: 0:11:11  loss: 4.3967 (4.3967)  acc1: 5.4688 (5.4688)  acc5: 18.7500 (18.7500)  time: 3.4250  data: 2.4702  max mem: 1754
Test:  [ 10/196]  eta: 0:02:18  loss: 4.4758 (4.4658)  acc1: 4.6875 (4.5099)  acc5: 15.2344 (15.1634)  time: 0.7456  data: 0.2823  max mem: 1754
Test:  [ 20/196]  eta: 0:01:59  loss: 4.4768 (4.4812)  acc1: 3.9062 (4.1481)  acc5: 14.0625 (14.5089)  time: 0.5420  data: 0.0338  max mem: 1754
Test:  [ 30/196]  eta: 0:01:48  loss: 4.4674 (4.4689)  acc1: 3.9062 (4.2969)  acc5: 15.2344 (15.1084)  time: 0.6072  data: 0.0046  max mem: 1754
Test:  [ 40/196]  eta: 0:01:40  loss: 4.4628 (4.4739)  acc1: 4.2969 (4.1635)  acc5: 15.6250 (15.0248)  time: 0.6086  data: 0.0057  max mem: 1754
Test:  [ 50/196]  eta: 0:01:33  loss: 4.4840 (4.4763)  acc1: 3.9062 (4.1667)  acc5: 14.0625 (14.9969)  time: 0.6092  data: 0.0065  max mem: 1754
Test:  [ 60/196]  eta: 0:01:26  loss: 4.4799 (4.4782)  acc1: 4.2969 (4.1176)  acc5: 14.0625 (14.9398)  time: 0.6091  data: 0.0067  max mem: 1754
Test:  [ 70/196]  eta: 0:01:19  loss: 4.4722 (4.4780)  acc1: 4.2969 (4.2419)  acc5: 14.0625 (14.9263)  time: 0.6103  data: 0.0076  max mem: 1754
Test:  [ 80/196]  eta: 0:01:12  loss: 4.4667 (4.4787)  acc1: 4.6875 (4.2969)  acc5: 14.0625 (14.8630)  time: 0.6105  data: 0.0072  max mem: 1754
Test:  [ 90/196]  eta: 0:01:06  loss: 4.4787 (4.4794)  acc1: 4.2969 (4.2926)  acc5: 14.4531 (14.7879)  time: 0.6090  data: 0.0060  max mem: 1754
Test:  [100/196]  eta: 0:00:59  loss: 4.4806 (4.4799)  acc1: 3.9062 (4.3007)  acc5: 14.0625 (14.8476)  time: 0.6056  data: 0.0048  max mem: 1754
Test:  [110/196]  eta: 0:00:52  loss: 4.4806 (4.4808)  acc1: 4.6875 (4.3074)  acc5: 14.4531 (14.8578)  time: 0.5681  data: 0.0035  max mem: 1754
Test:  [120/196]  eta: 0:00:44  loss: 4.4708 (4.4817)  acc1: 4.2969 (4.3259)  acc5: 14.8438 (14.9212)  time: 0.4235  data: 0.0067  max mem: 1754
Test:  [130/196]  eta: 0:00:38  loss: 4.4716 (4.4816)  acc1: 4.2969 (4.3088)  acc5: 15.2344 (14.9541)  time: 0.4332  data: 0.0072  max mem: 1754
Test:  [140/196]  eta: 0:00:32  loss: 4.4713 (4.4804)  acc1: 3.9062 (4.2941)  acc5: 15.2344 (14.9656)  time: 0.5815  data: 0.0055  max mem: 1754
Test:  [150/196]  eta: 0:00:27  loss: 4.4551 (4.4803)  acc1: 3.9062 (4.2865)  acc5: 14.8438 (14.9421)  time: 0.6107  data: 0.0075  max mem: 1754
Test:  [160/196]  eta: 0:00:20  loss: 4.4665 (4.4805)  acc1: 3.9062 (4.2484)  acc5: 14.4531 (14.9044)  time: 0.4668  data: 0.0085  max mem: 1754
Test:  [170/196]  eta: 0:00:14  loss: 4.4776 (4.4802)  acc1: 3.9062 (4.2512)  acc5: 14.8438 (14.9009)  time: 0.3941  data: 0.0092  max mem: 1754
Test:  [180/196]  eta: 0:00:09  loss: 4.4813 (4.4806)  acc1: 4.2969 (4.2667)  acc5: 14.4531 (14.8653)  time: 0.5385  data: 0.0091  max mem: 1754
Test:  [190/196]  eta: 0:00:03  loss: 4.4865 (4.4813)  acc1: 4.2969 (4.2764)  acc5: 14.4531 (14.8785)  time: 0.6090  data: 0.0070  max mem: 1754
Test:  [195/196]  eta: 0:00:00  loss: 4.4865 (4.4811)  acc1: 4.2969 (4.2720)  acc5: 14.8438 (14.9120)  time: 0.6290  data: 0.0055  max mem: 1754
Test: Total time: 0:01:53 (0.5774 s / it)
* Acc@1 4.272 Acc@5 14.912 loss 4.481
Test:  [ 0/40]  eta: 0:01:15  loss: 4.1450 (4.1450)  acc1: 10.5469 (10.5469)  acc5: 25.7812 (25.7812)  time: 1.8787  data: 1.5857  max mem: 1754
Test:  [10/40]  eta: 0:00:14  loss: 4.1940 (4.1824)  acc1: 8.5938 (8.3097)  acc5: 24.2188 (24.5739)  time: 0.4733  data: 0.1781  max mem: 1754
Test:  [20/40]  eta: 0:00:10  loss: 4.1940 (4.1874)  acc1: 7.8125 (7.9613)  acc5: 24.2188 (24.2932)  time: 0.4481  data: 0.0206  max mem: 1754
Test:  [30/40]  eta: 0:00:05  loss: 4.1904 (4.1922)  acc1: 7.4219 (7.7999)  acc5: 23.8281 (24.0801)  time: 0.5848  data: 0.0039  max mem: 1754
Test:  [39/40]  eta: 0:00:00  loss: 4.1902 (4.1930)  acc1: 7.4219 (7.9900)  acc5: 24.6094 (24.3100)  time: 0.5983  data: 0.0046  max mem: 1754
Test: Total time: 0:00:22 (0.5654 s / it)
* Acc@1 7.990 Acc@5 24.310 loss 4.193
Training Accuracy: 0.00%
Testing Accuracy: 0.00%

Final sparsity after training: 0.4638
Final non-zero weights: 14957724
**************Prune Round 2**********************

Current sparsity level: 0.4638237449523297
Non-zero weights before pruning: 14957724
Processing block 0
Layer 0.pwconv1:
  - Previous non-zero weights: 18431
  - Weights pruned this round: 9217
  - Remaining non-zero weights: 9214
  - Layer sparsity: 0.7501
Layer 0.pwconv2:
  - Previous non-zero weights: 18431
  - Weights pruned this round: 9217
  - Remaining non-zero weights: 9214
  - Layer sparsity: 0.7501
Layer 1.pwconv1:
  - Previous non-zero weights: 18431
  - Weights pruned this round: 9217
  - Remaining non-zero weights: 9214
  - Layer sparsity: 0.7501
Layer 1.pwconv2:
  - Previous non-zero weights: 18431
  - Weights pruned this round: 9217
  - Remaining non-zero weights: 9214
  - Layer sparsity: 0.7501
Layer 2.pwconv1:
  - Previous non-zero weights: 18431
  - Weights pruned this round: 9217
  - Remaining non-zero weights: 9214
  - Layer sparsity: 0.7501
Layer 2.pwconv2:
  - Previous non-zero weights: 18431
  - Weights pruned this round: 9217
  - Remaining non-zero weights: 9214
  - Layer sparsity: 0.7501
Processing block 1
Layer 0.pwconv1:
  - Previous non-zero weights: 73727
  - Weights pruned this round: 36865
  - Remaining non-zero weights: 36862
  - Layer sparsity: 0.7500
Layer 0.pwconv2:
  - Previous non-zero weights: 73727
  - Weights pruned this round: 36865
  - Remaining non-zero weights: 36862
  - Layer sparsity: 0.7500
Layer 1.pwconv1:
  - Previous non-zero weights: 73727
  - Weights pruned this round: 36865
  - Remaining non-zero weights: 36862
  - Layer sparsity: 0.7500
Layer 1.pwconv2:
  - Previous non-zero weights: 73727
  - Weights pruned this round: 36865
  - Remaining non-zero weights: 36862
  - Layer sparsity: 0.7500
Layer 2.pwconv1:
  - Previous non-zero weights: 73727
  - Weights pruned this round: 36865
  - Remaining non-zero weights: 36862
  - Layer sparsity: 0.7500
Layer 2.pwconv2:
  - Previous non-zero weights: 73727
  - Weights pruned this round: 36865
  - Remaining non-zero weights: 36862
  - Layer sparsity: 0.7500
Processing block 2
Layer 0.pwconv1:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 0.pwconv2:
  - Previous non-zero weights: 294910
  - Weights pruned this round: 147456
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 1.pwconv1:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 1.pwconv2:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 2.pwconv1:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 2.pwconv2:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 3.pwconv1:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 3.pwconv2:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 4.pwconv1:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 4.pwconv2:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 5.pwconv1:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 5.pwconv2:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 6.pwconv1:
  - Previous non-zero weights: 294910
  - Weights pruned this round: 147456
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 6.pwconv2:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 7.pwconv1:
  - Previous non-zero weights: 294910
  - Weights pruned this round: 147456
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 7.pwconv2:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 8.pwconv1:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 8.pwconv2:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Processing block 3
Layer 0.pwconv1:
  - Previous non-zero weights: 1179647
  - Weights pruned this round: 589825
  - Remaining non-zero weights: 589822
  - Layer sparsity: 0.7500
Layer 0.pwconv2:
  - Previous non-zero weights: 1179647
  - Weights pruned this round: 589825
  - Remaining non-zero weights: 589822
  - Layer sparsity: 0.7500
Layer 1.pwconv1:
  - Previous non-zero weights: 1179647
  - Weights pruned this round: 589825
  - Remaining non-zero weights: 589822
  - Layer sparsity: 0.7500
Layer 1.pwconv2:
  - Previous non-zero weights: 1179647
  - Weights pruned this round: 589825
  - Remaining non-zero weights: 589822
  - Layer sparsity: 0.7500
Layer 2.pwconv1:
  - Previous non-zero weights: 1179646
  - Weights pruned this round: 589824
  - Remaining non-zero weights: 589822
  - Layer sparsity: 0.7500
Layer 2.pwconv2:
  - Previous non-zero weights: 1179647
  - Weights pruned this round: 589825
  - Remaining non-zero weights: 589822
  - Layer sparsity: 0.7500
Overall model sparsity: 0.2500
Sparsity after pruning: 0.6957
Non-zero weights after pruning: 8488060

Epoch 1/1
Epoch: [0]  [  0/196]  eta: 0:18:53  lr: 0.000000  min_lr: 0.000000  loss: 4.5212 (4.5212)  class_acc: 0.0430 (0.0430)  weight_decay: 0.0500 (0.0500)  time: 5.7851  data: 3.4921  max mem: 1754
Epoch: [0]  [ 10/196]  eta: 0:08:14  lr: 0.000004  min_lr: 0.000004  loss: 4.4890 (4.5023)  class_acc: 0.0430 (0.0398)  weight_decay: 0.0499 (0.0499)  time: 2.6578  data: 0.3260  max mem: 1754
Epoch: [0]  [ 20/196]  eta: 0:07:20  lr: 0.000008  min_lr: 0.000008  loss: 4.4886 (4.4891)  class_acc: 0.0391 (0.0398)  weight_decay: 0.0496 (0.0496)  time: 2.3406  data: 0.0056  max mem: 1754
Epoch: [0]  [ 30/196]  eta: 0:06:46  lr: 0.000012  min_lr: 0.000012  loss: 4.4660 (4.4796)  class_acc: 0.0391 (0.0403)  weight_decay: 0.0486 (0.0490)  time: 2.3385  data: 0.0037  max mem: 1754
Epoch: [0]  [ 40/196]  eta: 0:06:18  lr: 0.000016  min_lr: 0.000016  loss: 4.4625 (4.4713)  class_acc: 0.0391 (0.0423)  weight_decay: 0.0469 (0.0483)  time: 2.3396  data: 0.0054  max mem: 1754
Epoch: [0]  [ 50/196]  eta: 0:05:53  lr: 0.000021  min_lr: 0.000021  loss: 4.4651 (4.4706)  class_acc: 0.0430 (0.0441)  weight_decay: 0.0447 (0.0474)  time: 2.3707  data: 0.0049  max mem: 1754
Epoch: [0]  [ 60/196]  eta: 0:05:34  lr: 0.000025  min_lr: 0.000025  loss: 4.4648 (4.4677)  class_acc: 0.0430 (0.0444)  weight_decay: 0.0420 (0.0463)  time: 2.5303  data: 0.0042  max mem: 1754
Epoch: [0]  [ 70/196]  eta: 0:05:09  lr: 0.000029  min_lr: 0.000029  loss: 4.4517 (4.4621)  class_acc: 0.0430 (0.0446)  weight_decay: 0.0389 (0.0450)  time: 2.5452  data: 0.0034  max mem: 1754
Epoch: [0]  [ 80/196]  eta: 0:04:45  lr: 0.000033  min_lr: 0.000033  loss: 4.4408 (4.4616)  class_acc: 0.0469 (0.0451)  weight_decay: 0.0354 (0.0436)  time: 2.4756  data: 0.0036  max mem: 1754
Epoch: [0]  [ 90/196]  eta: 0:04:23  lr: 0.000037  min_lr: 0.000037  loss: 4.4527 (4.4608)  class_acc: 0.0469 (0.0458)  weight_decay: 0.0316 (0.0421)  time: 2.5733  data: 0.0040  max mem: 1754
Epoch: [0]  [100/196]  eta: 0:03:59  lr: 0.000041  min_lr: 0.000041  loss: 4.4650 (4.4610)  class_acc: 0.0469 (0.0452)  weight_decay: 0.0276 (0.0405)  time: 2.6187  data: 0.0041  max mem: 1754
Epoch: [0]  [110/196]  eta: 0:03:34  lr: 0.000045  min_lr: 0.000045  loss: 4.4544 (4.4581)  class_acc: 0.0469 (0.0460)  weight_decay: 0.0236 (0.0388)  time: 2.5564  data: 0.0035  max mem: 1754
Epoch: [0]  [120/196]  eta: 0:03:10  lr: 0.000049  min_lr: 0.000049  loss: 4.4411 (4.4565)  class_acc: 0.0547 (0.0464)  weight_decay: 0.0196 (0.0371)  time: 2.5451  data: 0.0031  max mem: 1754
Epoch: [0]  [130/196]  eta: 0:02:45  lr: 0.000053  min_lr: 0.000053  loss: 4.4458 (4.4572)  class_acc: 0.0469 (0.0460)  weight_decay: 0.0158 (0.0353)  time: 2.5510  data: 0.0034  max mem: 1754
Epoch: [0]  [140/196]  eta: 0:02:20  lr: 0.000057  min_lr: 0.000057  loss: 4.4443 (4.4554)  class_acc: 0.0469 (0.0463)  weight_decay: 0.0122 (0.0336)  time: 2.5526  data: 0.0039  max mem: 1754
Epoch: [0]  [150/196]  eta: 0:01:55  lr: 0.000062  min_lr: 0.000062  loss: 4.4381 (4.4560)  class_acc: 0.0508 (0.0463)  weight_decay: 0.0089 (0.0318)  time: 2.5876  data: 0.0051  max mem: 1754
Epoch: [0]  [160/196]  eta: 0:01:30  lr: 0.000066  min_lr: 0.000066  loss: 4.4401 (4.4542)  class_acc: 0.0508 (0.0470)  weight_decay: 0.0060 (0.0302)  time: 2.5847  data: 0.0049  max mem: 1754
Epoch: [0]  [170/196]  eta: 0:01:05  lr: 0.000070  min_lr: 0.000070  loss: 4.4401 (4.4545)  class_acc: 0.0547 (0.0473)  weight_decay: 0.0037 (0.0286)  time: 2.5583  data: 0.0049  max mem: 1754
Epoch: [0]  [180/196]  eta: 0:00:40  lr: 0.000074  min_lr: 0.000074  loss: 4.4389 (4.4534)  class_acc: 0.0508 (0.0478)  weight_decay: 0.0018 (0.0271)  time: 2.5846  data: 0.0069  max mem: 1754
Epoch: [0]  [190/196]  eta: 0:00:15  lr: 0.000078  min_lr: 0.000078  loss: 4.4354 (4.4520)  class_acc: 0.0508 (0.0480)  weight_decay: 0.0006 (0.0257)  time: 2.6075  data: 0.0059  max mem: 1754
Epoch: [0]  [195/196]  eta: 0:00:02  lr: 0.000080  min_lr: 0.000080  loss: 4.4309 (4.4511)  class_acc: 0.0508 (0.0480)  weight_decay: 0.0003 (0.0251)  time: 2.4818  data: 0.0040  max mem: 1754
Epoch: [0] Total time: 0:08:13 (2.5191 s / it)
Averaged stats: lr: 0.000080  min_lr: 0.000080  loss: 4.4309 (4.4511)  class_acc: 0.0508 (0.0480)  weight_decay: 0.0003 (0.0251)
Test:  [  0/196]  eta: 0:08:45  loss: 4.3755 (4.3755)  acc1: 5.8594 (5.8594)  acc5: 16.7969 (16.7969)  time: 2.6815  data: 2.4513  max mem: 1754
Test:  [ 10/196]  eta: 0:02:26  loss: 4.4187 (4.4101)  acc1: 5.0781 (5.4688)  acc5: 17.5781 (17.4361)  time: 0.7890  data: 0.3750  max mem: 1754
Test:  [ 20/196]  eta: 0:02:05  loss: 4.4398 (4.4303)  acc1: 4.6875 (5.0781)  acc5: 16.7969 (16.8899)  time: 0.6122  data: 0.0932  max mem: 1754
Test:  [ 30/196]  eta: 0:01:52  loss: 4.4504 (4.4342)  acc1: 4.6875 (5.0655)  acc5: 16.7969 (17.2127)  time: 0.6161  data: 0.0118  max mem: 1754
Test:  [ 40/196]  eta: 0:01:43  loss: 4.4341 (4.4295)  acc1: 4.6875 (5.0972)  acc5: 18.3594 (17.2637)  time: 0.6083  data: 0.0053  max mem: 1754
Test:  [ 50/196]  eta: 0:01:34  loss: 4.4116 (4.4284)  acc1: 5.0781 (5.1471)  acc5: 17.1875 (17.2028)  time: 0.6080  data: 0.0057  max mem: 1754
Test:  [ 60/196]  eta: 0:01:27  loss: 4.4081 (4.4255)  acc1: 5.0781 (5.1550)  acc5: 17.1875 (17.2579)  time: 0.6049  data: 0.0046  max mem: 1754
Test:  [ 70/196]  eta: 0:01:20  loss: 4.4087 (4.4256)  acc1: 4.6875 (5.1331)  acc5: 17.5781 (17.1655)  time: 0.6016  data: 0.0034  max mem: 1754
Test:  [ 80/196]  eta: 0:01:13  loss: 4.4087 (4.4213)  acc1: 5.0781 (5.2276)  acc5: 17.5781 (17.3852)  time: 0.6003  data: 0.0028  max mem: 1754
Test:  [ 90/196]  eta: 0:01:06  loss: 4.4266 (4.4229)  acc1: 5.8594 (5.2885)  acc5: 17.5781 (17.2734)  time: 0.6004  data: 0.0028  max mem: 1754
Test:  [100/196]  eta: 0:01:00  loss: 4.4283 (4.4216)  acc1: 6.2500 (5.4069)  acc5: 17.5781 (17.3151)  time: 0.6000  data: 0.0027  max mem: 1754
Test:  [110/196]  eta: 0:00:53  loss: 4.4245 (4.4235)  acc1: 5.8594 (5.3878)  acc5: 17.9688 (17.2755)  time: 0.5998  data: 0.0026  max mem: 1754
Test:  [120/196]  eta: 0:00:47  loss: 4.4098 (4.4222)  acc1: 4.6875 (5.3654)  acc5: 16.4062 (17.2779)  time: 0.6000  data: 0.0027  max mem: 1754
Test:  [130/196]  eta: 0:00:40  loss: 4.4051 (4.4215)  acc1: 5.8594 (5.4479)  acc5: 17.9688 (17.3276)  time: 0.6001  data: 0.0028  max mem: 1754
Test:  [140/196]  eta: 0:00:33  loss: 4.4330 (4.4243)  acc1: 5.8594 (5.4244)  acc5: 17.5781 (17.2817)  time: 0.4874  data: 0.0055  max mem: 1754
Test:  [150/196]  eta: 0:00:27  loss: 4.4426 (4.4231)  acc1: 5.0781 (5.4325)  acc5: 17.9688 (17.3401)  time: 0.4854  data: 0.0054  max mem: 1754
Test:  [160/196]  eta: 0:00:21  loss: 4.4029 (4.4223)  acc1: 5.8594 (5.4542)  acc5: 17.9688 (17.3185)  time: 0.5977  data: 0.0025  max mem: 1754
Test:  [170/196]  eta: 0:00:15  loss: 4.4144 (4.4227)  acc1: 5.4688 (5.4505)  acc5: 17.1875 (17.3520)  time: 0.5997  data: 0.0026  max mem: 1754
Test:  [180/196]  eta: 0:00:09  loss: 4.4144 (4.4231)  acc1: 5.0781 (5.4558)  acc5: 17.5781 (17.3709)  time: 0.4877  data: 0.0033  max mem: 1754
Test:  [190/196]  eta: 0:00:03  loss: 4.4162 (4.4231)  acc1: 5.0781 (5.4503)  acc5: 17.5781 (17.3920)  time: 0.4425  data: 0.0030  max mem: 1754
Test:  [195/196]  eta: 0:00:00  loss: 4.4249 (4.4234)  acc1: 5.0781 (5.4300)  acc5: 17.1875 (17.3660)  time: 0.4605  data: 0.0026  max mem: 1754
Test: Total time: 0:01:54 (0.5851 s / it)
* Acc@1 5.430 Acc@5 17.366 loss 4.423
Test:  [ 0/40]  eta: 0:00:54  loss: 4.0562 (4.0562)  acc1: 10.5469 (10.5469)  acc5: 27.3438 (27.3438)  time: 1.3747  data: 1.0366  max mem: 1754
Test:  [10/40]  eta: 0:00:18  loss: 4.0911 (4.0919)  acc1: 9.3750 (9.6236)  acc5: 27.7344 (27.2372)  time: 0.6151  data: 0.1209  max mem: 1754
Test:  [20/40]  eta: 0:00:12  loss: 4.1264 (4.0997)  acc1: 8.9844 (9.2262)  acc5: 27.7344 (27.0275)  time: 0.5688  data: 0.0153  max mem: 1754
Test:  [30/40]  eta: 0:00:06  loss: 4.1232 (4.1040)  acc1: 8.5938 (9.0978)  acc5: 25.7812 (26.7137)  time: 0.5986  data: 0.0014  max mem: 1754
Test:  [39/40]  eta: 0:00:00  loss: 4.0904 (4.1062)  acc1: 9.3750 (9.2800)  acc5: 26.5625 (27.0000)  time: 0.5727  data: 0.0018  max mem: 1754
Test: Total time: 0:00:23 (0.5960 s / it)
* Acc@1 9.280 Acc@5 27.000 loss 4.106
Training Accuracy: 0.00%
Testing Accuracy: 0.00%

Final sparsity after training: 0.4638
Final non-zero weights: 14957724
**************Prune Round 3**********************

Current sparsity level: 0.4638237449523297
Non-zero weights before pruning: 14957724
Processing block 0
Layer 0.pwconv1:
  - Previous non-zero weights: 18431
  - Weights pruned this round: 9217
  - Remaining non-zero weights: 9214
  - Layer sparsity: 0.7501
Layer 0.pwconv2:
  - Previous non-zero weights: 18431
  - Weights pruned this round: 9217
  - Remaining non-zero weights: 9214
  - Layer sparsity: 0.7501
Layer 1.pwconv1:
  - Previous non-zero weights: 18431
  - Weights pruned this round: 9217
  - Remaining non-zero weights: 9214
  - Layer sparsity: 0.7501
Layer 1.pwconv2:
  - Previous non-zero weights: 18431
  - Weights pruned this round: 9217
  - Remaining non-zero weights: 9214
  - Layer sparsity: 0.7501
Layer 2.pwconv1:
  - Previous non-zero weights: 18431
  - Weights pruned this round: 9217
  - Remaining non-zero weights: 9214
  - Layer sparsity: 0.7501
Layer 2.pwconv2:
  - Previous non-zero weights: 18431
  - Weights pruned this round: 9217
  - Remaining non-zero weights: 9214
  - Layer sparsity: 0.7501
Processing block 1
Layer 0.pwconv1:
  - Previous non-zero weights: 73727
  - Weights pruned this round: 36865
  - Remaining non-zero weights: 36862
  - Layer sparsity: 0.7500
Layer 0.pwconv2:
  - Previous non-zero weights: 73727
  - Weights pruned this round: 36865
  - Remaining non-zero weights: 36862
  - Layer sparsity: 0.7500
Layer 1.pwconv1:
  - Previous non-zero weights: 73727
  - Weights pruned this round: 36865
  - Remaining non-zero weights: 36862
  - Layer sparsity: 0.7500
Layer 1.pwconv2:
  - Previous non-zero weights: 73727
  - Weights pruned this round: 36865
  - Remaining non-zero weights: 36862
  - Layer sparsity: 0.7500
Layer 2.pwconv1:
  - Previous non-zero weights: 73727
  - Weights pruned this round: 36865
  - Remaining non-zero weights: 36862
  - Layer sparsity: 0.7500
Layer 2.pwconv2:
  - Previous non-zero weights: 73727
  - Weights pruned this round: 36865
  - Remaining non-zero weights: 36862
  - Layer sparsity: 0.7500
Processing block 2
Layer 0.pwconv1:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 0.pwconv2:
  - Previous non-zero weights: 294910
  - Weights pruned this round: 147456
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 1.pwconv1:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 1.pwconv2:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 2.pwconv1:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 2.pwconv2:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 3.pwconv1:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 3.pwconv2:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 4.pwconv1:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 4.pwconv2:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 5.pwconv1:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 5.pwconv2:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 6.pwconv1:
  - Previous non-zero weights: 294910
  - Weights pruned this round: 147456
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 6.pwconv2:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 7.pwconv1:
  - Previous non-zero weights: 294910
  - Weights pruned this round: 147456
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 7.pwconv2:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 8.pwconv1:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Layer 8.pwconv2:
  - Previous non-zero weights: 294911
  - Weights pruned this round: 147457
  - Remaining non-zero weights: 147454
  - Layer sparsity: 0.7500
Processing block 3
Layer 0.pwconv1:
  - Previous non-zero weights: 1179647
  - Weights pruned this round: 589825
  - Remaining non-zero weights: 589822
  - Layer sparsity: 0.7500
Layer 0.pwconv2:
  - Previous non-zero weights: 1179647
  - Weights pruned this round: 589825
  - Remaining non-zero weights: 589822
  - Layer sparsity: 0.7500
Layer 1.pwconv1:
  - Previous non-zero weights: 1179647
  - Weights pruned this round: 589825
  - Remaining non-zero weights: 589822
  - Layer sparsity: 0.7500
Layer 1.pwconv2:
  - Previous non-zero weights: 1179647
  - Weights pruned this round: 589825
  - Remaining non-zero weights: 589822
  - Layer sparsity: 0.7500
Layer 2.pwconv1:
  - Previous non-zero weights: 1179646
  - Weights pruned this round: 589824
  - Remaining non-zero weights: 589822
  - Layer sparsity: 0.7500
Layer 2.pwconv2:
  - Previous non-zero weights: 1179647
  - Weights pruned this round: 589825
  - Remaining non-zero weights: 589822
  - Layer sparsity: 0.7500
Overall model sparsity: 0.2500
Sparsity after pruning: 0.6957
Non-zero weights after pruning: 8488060

Epoch 1/1
Epoch: [0]  [  0/196]  eta: 0:14:38  lr: 0.000000  min_lr: 0.000000  loss: 4.4388 (4.4388)  class_acc: 0.0625 (0.0625)  weight_decay: 0.0500 (0.0500)  time: 4.4829  data: 2.3392  max mem: 1754
Epoch: [0]  [ 10/196]  eta: 0:07:55  lr: 0.000004  min_lr: 0.000004  loss: 4.4388 (4.4285)  class_acc: 0.0625 (0.0561)  weight_decay: 0.0499 (0.0499)  time: 2.5568  data: 0.2138  max mem: 1754
Epoch: [0]  [ 20/196]  eta: 0:07:10  lr: 0.000008  min_lr: 0.000008  loss: 4.4228 (4.4267)  class_acc: 0.0547 (0.0545)  weight_decay: 0.0496 (0.0496)  time: 2.3415  data: 0.0009  max mem: 1754
Epoch: [0]  [ 30/196]  eta: 0:06:41  lr: 0.000012  min_lr: 0.000012  loss: 4.4238 (4.4260)  class_acc: 0.0508 (0.0541)  weight_decay: 0.0486 (0.0490)  time: 2.3408  data: 0.0010  max mem: 1754
Epoch: [0]  [ 40/196]  eta: 0:06:19  lr: 0.000016  min_lr: 0.000016  loss: 4.4303 (4.4270)  class_acc: 0.0508 (0.0534)  weight_decay: 0.0469 (0.0483)  time: 2.4159  data: 0.0020  max mem: 1754
Epoch: [0]  [ 50/196]  eta: 0:05:56  lr: 0.000021  min_lr: 0.000021  loss: 4.4037 (4.4223)  class_acc: 0.0586 (0.0545)  weight_decay: 0.0447 (0.0474)  time: 2.4746  data: 0.0025  max mem: 1754
Epoch: [0]  [ 60/196]  eta: 0:05:31  lr: 0.000025  min_lr: 0.000025  loss: 4.4108 (4.4248)  class_acc: 0.0547 (0.0537)  weight_decay: 0.0420 (0.0463)  time: 2.4524  data: 0.0036  max mem: 1754
