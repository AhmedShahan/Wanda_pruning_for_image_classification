Set warmup steps = 9750
Set warmup steps = 0
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.dwconv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.dwconv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.dwconv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.dwconv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.dwconv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.dwconv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.dwconv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.dwconv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.dwconv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.dwconv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.dwconv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.dwconv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.dwconv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.dwconv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.dwconv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.dwconv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.dwconv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.dwconv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.dwconv.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.dwconv.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.dwconv.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.dwconv.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.dwconv.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.dwconv.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.dwconv.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.dwconv.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.dwconv.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.dwconv.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.dwconv.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.dwconv.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.dwconv.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.dwconv.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.dwconv.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.dwconv.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.dwconv.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.dwconv.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
/home/shahanahmed/.local/lib/python3.10/site-packages/timm/utils/cuda.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
**************Prune Round 1**********************

Current sparsity level: 0.0017724827175138512
Non-zero weights before pruning: 1877472
Initial non-zero weights: 1877472

Processing block 0

Layer 0.pwconv1:
  Initial non-zero weights: 36864
  Target non-zero weights: 18432
  Final non-zero weights: 18431
  Weights pruned: 18433

Layer 0.pwconv2:
  Initial non-zero weights: 36864
  Target non-zero weights: 18432
  Final non-zero weights: 18431
  Weights pruned: 18433

Layer 1.pwconv1:
  Initial non-zero weights: 36864
  Target non-zero weights: 18432
  Final non-zero weights: 18431
  Weights pruned: 18433

Layer 1.pwconv2:
  Initial non-zero weights: 36864
  Target non-zero weights: 18432
  Final non-zero weights: 18431
  Weights pruned: 18433

Layer 2.pwconv1:
  Initial non-zero weights: 36864
  Target non-zero weights: 18432
  Final non-zero weights: 18431
  Weights pruned: 18433

Layer 2.pwconv2:
  Initial non-zero weights: 36864
  Target non-zero weights: 18432
  Final non-zero weights: 18431
  Weights pruned: 18433

Processing block 1

Layer 0.pwconv1:
  Initial non-zero weights: 147456
  Target non-zero weights: 73728
  Final non-zero weights: 73727
  Weights pruned: 73729

Layer 0.pwconv2:
  Initial non-zero weights: 147456
  Target non-zero weights: 73728
  Final non-zero weights: 73727
  Weights pruned: 73729

Layer 1.pwconv1:
  Initial non-zero weights: 147456
  Target non-zero weights: 73728
  Final non-zero weights: 73727
  Weights pruned: 73729

Layer 1.pwconv2:
  Initial non-zero weights: 147456
  Target non-zero weights: 73728
  Final non-zero weights: 73727
  Weights pruned: 73729

Layer 2.pwconv1:
  Initial non-zero weights: 147456
  Target non-zero weights: 73728
  Final non-zero weights: 73727
  Weights pruned: 73729

Layer 2.pwconv2:
  Initial non-zero weights: 147456
  Target non-zero weights: 73728
  Final non-zero weights: 73727
  Weights pruned: 73729

Processing block 2

Layer 0.pwconv1:
  Initial non-zero weights: 589824
  Target non-zero weights: 294912
  Final non-zero weights: 294911
  Weights pruned: 294913

Layer 0.pwconv2:
  Initial non-zero weights: 589824
  Target non-zero weights: 294912
  Final non-zero weights: 294910
  Weights pruned: 294914

Layer 1.pwconv1:
  Initial non-zero weights: 589824
  Target non-zero weights: 294912
  Final non-zero weights: 294911
  Weights pruned: 294913

Layer 1.pwconv2:
  Initial non-zero weights: 589824
  Target non-zero weights: 294912
  Final non-zero weights: 294911
  Weights pruned: 294913

Layer 2.pwconv1:
  Initial non-zero weights: 589824
  Target non-zero weights: 294912
  Final non-zero weights: 294911
  Weights pruned: 294913

Layer 2.pwconv2:
  Initial non-zero weights: 589824
  Target non-zero weights: 294912
  Final non-zero weights: 294911
  Weights pruned: 294913

Layer 3.pwconv1:
  Initial non-zero weights: 589824
  Target non-zero weights: 294912
  Final non-zero weights: 294911
  Weights pruned: 294913

Layer 3.pwconv2:
  Initial non-zero weights: 589824
  Target non-zero weights: 294912
  Final non-zero weights: 294911
  Weights pruned: 294913

Layer 4.pwconv1:
  Initial non-zero weights: 589824
  Target non-zero weights: 294912
  Final non-zero weights: 294911
  Weights pruned: 294913

Layer 4.pwconv2:
  Initial non-zero weights: 589824
  Target non-zero weights: 294912
  Final non-zero weights: 294911
  Weights pruned: 294913

Layer 5.pwconv1:
  Initial non-zero weights: 589824
  Target non-zero weights: 294912
  Final non-zero weights: 294911
  Weights pruned: 294913

Layer 5.pwconv2:
  Initial non-zero weights: 589824
  Target non-zero weights: 294912
  Final non-zero weights: 294911
  Weights pruned: 294913

Layer 6.pwconv1:
  Initial non-zero weights: 589823
  Target non-zero weights: 294911
  Final non-zero weights: 294910
  Weights pruned: 294913

Layer 6.pwconv2:
  Initial non-zero weights: 589824
  Target non-zero weights: 294912
  Final non-zero weights: 294911
  Weights pruned: 294913

Layer 7.pwconv1:
  Initial non-zero weights: 589823
  Target non-zero weights: 294911
  Final non-zero weights: 294910
  Weights pruned: 294913

Layer 7.pwconv2:
  Initial non-zero weights: 589824
  Target non-zero weights: 294912
  Final non-zero weights: 294911
  Weights pruned: 294913

Layer 8.pwconv1:
  Initial non-zero weights: 589824
  Target non-zero weights: 294912
  Final non-zero weights: 294911
  Weights pruned: 294913

Layer 8.pwconv2:
  Initial non-zero weights: 589824
  Target non-zero weights: 294912
  Final non-zero weights: 294911
  Weights pruned: 294913

Processing block 3

Layer 0.pwconv1:
  Initial non-zero weights: 2359296
  Target non-zero weights: 1179648
  Final non-zero weights: 1179647
  Weights pruned: 1179649

Layer 0.pwconv2:
  Initial non-zero weights: 2359296
  Target non-zero weights: 1179648
  Final non-zero weights: 1179647
  Weights pruned: 1179649

Layer 1.pwconv1:
  Initial non-zero weights: 2359296
  Target non-zero weights: 1179648
  Final non-zero weights: 1179647
  Weights pruned: 1179649

Layer 1.pwconv2:
  Initial non-zero weights: 2359296
  Target non-zero weights: 1179648
  Final non-zero weights: 1179647
  Weights pruned: 1179649

Layer 2.pwconv1:
  Initial non-zero weights: 2359295
  Target non-zero weights: 1179647
  Final non-zero weights: 1179646
  Weights pruned: 1179649

Layer 2.pwconv2:
  Initial non-zero weights: 2359296
  Target non-zero weights: 1179648
  Final non-zero weights: 1179647
  Weights pruned: 1179649

Pruning Summary:
Initial non-zero weights: 1877472
Final non-zero weights: 1877472
Weights pruned: 0
New sparsity: 0.0000
Error: Pruning failed to reduce weights!
