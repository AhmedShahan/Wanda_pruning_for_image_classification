Set warmup steps = 8750
Set warmup steps = 0
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.dwconv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.dwconv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.dwconv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.dwconv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.dwconv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.dwconv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.dwconv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.dwconv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.dwconv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.dwconv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.dwconv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.dwconv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.dwconv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.dwconv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.dwconv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.dwconv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.dwconv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.dwconv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.dwconv.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.dwconv.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.dwconv.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.dwconv.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.dwconv.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.dwconv.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.dwconv.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.dwconv.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.dwconv.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.dwconv.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.dwconv.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.dwconv.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.dwconv.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.dwconv.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.dwconv.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.dwconv.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.dwconv.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.dwconv.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
/home/shahanahmed/.local/lib/python3.10/site-packages/timm/utils/cuda.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
**************Prune Round 1**********************

Current sparsity level: 0.00176105406222953
Actual sparsity after pruning: 0.00176105406222953
/home/shahanahmed/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch: [0]  [  0/176]  eta: 0:43:25  lr: 0.000000  min_lr: 0.000000  loss: 7.0175 (7.0175)  class_acc: 0.0000 (0.0000)  weight_decay: 0.0500 (0.0500)  time: 14.8056  data: 2.3001  max mem: 1586
Epoch: [0]  [ 10/176]  eta: 0:08:10  lr: 0.000005  min_lr: 0.000005  loss: 7.0436 (7.0378)  class_acc: 0.0000 (0.0000)  weight_decay: 0.0499 (0.0499)  time: 2.9574  data: 0.2140  max mem: 1586
Epoch: [0]  [ 20/176]  eta: 0:06:23  lr: 0.000009  min_lr: 0.000009  loss: 7.0319 (7.0306)  class_acc: 0.0000 (0.0004)  weight_decay: 0.0495 (0.0495)  time: 1.8431  data: 0.0060  max mem: 1586
Epoch: [0]  [ 30/176]  eta: 0:05:35  lr: 0.000014  min_lr: 0.000014  loss: 7.0068 (7.0160)  class_acc: 0.0000 (0.0006)  weight_decay: 0.0482 (0.0488)  time: 1.9360  data: 0.0045  max mem: 1586
Epoch: [0]  [ 40/176]  eta: 0:05:07  lr: 0.000018  min_lr: 0.000018  loss: 6.9354 (6.9859)  class_acc: 0.0000 (0.0011)  weight_decay: 0.0462 (0.0479)  time: 2.0520  data: 0.0032  max mem: 1586
Epoch: [0]  [ 50/176]  eta: 0:04:46  lr: 0.000023  min_lr: 0.000023  loss: 6.8453 (6.9498)  class_acc: 0.0039 (0.0026)  weight_decay: 0.0435 (0.0467)  time: 2.2270  data: 0.0050  max mem: 1586
Epoch: [0]  [ 60/176]  eta: 0:04:24  lr: 0.000027  min_lr: 0.000027  loss: 6.7426 (6.9067)  class_acc: 0.0078 (0.0040)  weight_decay: 0.0402 (0.0454)  time: 2.3260  data: 0.0050  max mem: 1586
Epoch: [0]  [ 70/176]  eta: 0:04:02  lr: 0.000032  min_lr: 0.000032  loss: 6.6012 (6.8554)  class_acc: 0.0156 (0.0063)  weight_decay: 0.0364 (0.0439)  time: 2.3435  data: 0.0037  max mem: 1586
Epoch: [0]  [ 80/176]  eta: 0:03:40  lr: 0.000037  min_lr: 0.000037  loss: 6.4171 (6.7897)  class_acc: 0.0195 (0.0082)  weight_decay: 0.0323 (0.0422)  time: 2.3444  data: 0.0037  max mem: 1586
Epoch: [0]  [ 90/176]  eta: 0:03:18  lr: 0.000041  min_lr: 0.000041  loss: 6.2142 (6.7129)  class_acc: 0.0195 (0.0092)  weight_decay: 0.0279 (0.0404)  time: 2.3456  data: 0.0043  max mem: 1586
Epoch: [0]  [100/176]  eta: 0:02:55  lr: 0.000046  min_lr: 0.000046  loss: 5.9414 (6.6233)  class_acc: 0.0195 (0.0106)  weight_decay: 0.0234 (0.0385)  time: 2.3459  data: 0.0044  max mem: 1586
Epoch: [0]  [110/176]  eta: 0:02:32  lr: 0.000050  min_lr: 0.000050  loss: 5.6350 (6.5229)  class_acc: 0.0234 (0.0121)  weight_decay: 0.0190 (0.0366)  time: 2.3454  data: 0.0037  max mem: 1586
Epoch: [0]  [120/176]  eta: 0:02:09  lr: 0.000055  min_lr: 0.000055  loss: 5.3354 (6.4127)  class_acc: 0.0312 (0.0137)  weight_decay: 0.0148 (0.0347)  time: 2.3463  data: 0.0040  max mem: 1586
Epoch: [0]  [130/176]  eta: 0:01:46  lr: 0.000059  min_lr: 0.000059  loss: 5.0028 (6.2955)  class_acc: 0.0352 (0.0155)  weight_decay: 0.0109 (0.0327)  time: 2.3467  data: 0.0043  max mem: 1586
Epoch: [0]  [140/176]  eta: 0:01:23  lr: 0.000064  min_lr: 0.000064  loss: 4.7544 (6.1801)  class_acc: 0.0312 (0.0164)  weight_decay: 0.0074 (0.0308)  time: 2.3479  data: 0.0041  max mem: 1586
Epoch: [0]  [150/176]  eta: 0:01:00  lr: 0.000069  min_lr: 0.000069  loss: 4.6269 (6.0764)  class_acc: 0.0234 (0.0170)  weight_decay: 0.0045 (0.0290)  time: 2.3899  data: 0.0051  max mem: 1586
Epoch: [0]  [160/176]  eta: 0:00:37  lr: 0.000073  min_lr: 0.000073  loss: 4.6024 (5.9846)  class_acc: 0.0234 (0.0173)  weight_decay: 0.0023 (0.0273)  time: 2.4353  data: 0.0057  max mem: 1586
Epoch: [0]  [170/176]  eta: 0:00:14  lr: 0.000078  min_lr: 0.000078  loss: 4.5848 (5.9021)  class_acc: 0.0273 (0.0179)  weight_decay: 0.0008 (0.0257)  time: 2.4925  data: 0.0040  max mem: 1586
Epoch: [0]  [175/176]  eta: 0:00:02  lr: 0.000080  min_lr: 0.000080  loss: 4.5792 (5.8718)  class_acc: 0.0273 (0.0182)  weight_decay: 0.0004 (0.0251)  time: 2.3544  data: 0.0036  max mem: 1586
Epoch: [0] Total time: 0:06:50 (2.3334 s / it)
Averaged stats: lr: 0.000080  min_lr: 0.000080  loss: 4.5792 (5.8718)  class_acc: 0.0273 (0.0182)  weight_decay: 0.0004 (0.0251)
/home/shahanahmed/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Test:  [  0/176]  eta: 0:15:24  loss: 4.5185 (4.5185)  acc1: 2.3438 (2.3438)  acc5: 14.4531 (14.4531)  time: 5.2534  data: 4.2811  max mem: 1586
Test:  [ 10/176]  eta: 0:02:30  loss: 4.5236 (4.5287)  acc1: 3.9062 (3.3026)  acc5: 13.2812 (13.1037)  time: 0.9069  data: 0.4201  max mem: 1586
Test:  [ 20/176]  eta: 0:01:52  loss: 4.5416 (4.5427)  acc1: 3.5156 (3.3296)  acc5: 12.1094 (12.8906)  time: 0.4969  data: 0.0317  max mem: 1586
Test:  [ 30/176]  eta: 0:01:40  loss: 4.5528 (4.5446)  acc1: 3.5156 (3.5282)  acc5: 12.1094 (12.9284)  time: 0.5657  data: 0.0158  max mem: 1586
Test:  [ 40/176]  eta: 0:01:30  loss: 4.5420 (4.5449)  acc1: 3.9062 (3.5347)  acc5: 12.5000 (12.7382)  time: 0.6100  data: 0.0023  max mem: 1586
Test:  [ 50/176]  eta: 0:01:22  loss: 4.5376 (4.5440)  acc1: 3.5156 (3.5692)  acc5: 12.8906 (12.8140)  time: 0.6113  data: 0.0030  max mem: 1586
Test:  [ 60/176]  eta: 0:01:15  loss: 4.5550 (4.5457)  acc1: 3.1250 (3.4964)  acc5: 12.5000 (12.7754)  time: 0.6125  data: 0.0040  max mem: 1586
Test:  [ 70/176]  eta: 0:01:05  loss: 4.5552 (4.5464)  acc1: 2.7344 (3.4386)  acc5: 12.1094 (12.7476)  time: 0.5111  data: 0.0052  max mem: 1586
Test:  [ 80/176]  eta: 0:00:58  loss: 4.5450 (4.5458)  acc1: 3.1250 (3.4433)  acc5: 12.8906 (12.8135)  time: 0.4986  data: 0.0063  max mem: 1586
Test:  [ 90/176]  eta: 0:00:52  loss: 4.5416 (4.5460)  acc1: 3.5156 (3.4298)  acc5: 12.8906 (12.7533)  time: 0.5996  data: 0.0051  max mem: 1586
Test:  [100/176]  eta: 0:00:45  loss: 4.5526 (4.5470)  acc1: 3.5156 (3.3764)  acc5: 12.8906 (12.7127)  time: 0.5563  data: 0.0036  max mem: 1586
Test:  [110/176]  eta: 0:00:39  loss: 4.5549 (4.5472)  acc1: 3.1250 (3.3608)  acc5: 12.1094 (12.5774)  time: 0.5134  data: 0.0041  max mem: 1586
Test:  [120/176]  eta: 0:00:32  loss: 4.5524 (4.5474)  acc1: 3.1250 (3.3542)  acc5: 12.5000 (12.5484)  time: 0.4698  data: 0.0046  max mem: 1586
Test:  [130/176]  eta: 0:00:26  loss: 4.5433 (4.5475)  acc1: 3.5156 (3.3814)  acc5: 12.8906 (12.5567)  time: 0.5135  data: 0.0043  max mem: 1586
Test:  [140/176]  eta: 0:00:20  loss: 4.5544 (4.5475)  acc1: 3.1250 (3.3605)  acc5: 12.5000 (12.5803)  time: 0.5732  data: 0.0035  max mem: 1586
Test:  [150/176]  eta: 0:00:14  loss: 4.5484 (4.5479)  acc1: 2.7344 (3.3475)  acc5: 11.7188 (12.5052)  time: 0.5022  data: 0.0037  max mem: 1586
Test:  [160/176]  eta: 0:00:09  loss: 4.5624 (4.5491)  acc1: 2.7344 (3.3094)  acc5: 10.9375 (12.4078)  time: 0.5406  data: 0.0037  max mem: 1586
Test:  [170/176]  eta: 0:00:03  loss: 4.5616 (4.5493)  acc1: 2.7344 (3.3032)  acc5: 12.1094 (12.4063)  time: 0.5097  data: 0.0029  max mem: 1586
Test:  [175/176]  eta: 0:00:00  loss: 4.5573 (4.5492)  acc1: 3.1250 (3.3111)  acc5: 12.1094 (12.4200)  time: 0.5624  data: 0.0027  max mem: 1586
Test: Total time: 0:01:40 (0.5728 s / it)
* Acc@1 3.311 Acc@5 12.420 loss 4.549
Test:  [ 0/59]  eta: 0:01:53  loss: 3.8832 (3.8832)  acc1: 21.8750 (21.8750)  acc5: 50.3906 (50.3906)  time: 1.9172  data: 1.7131  max mem: 1586
Test:  [10/59]  eta: 0:00:33  loss: 4.5472 (4.4850)  acc1: 0.3906 (7.2443)  acc5: 7.0312 (16.8679)  time: 0.6781  data: 0.1573  max mem: 1586
Test:  [20/59]  eta: 0:00:22  loss: 4.5115 (4.4955)  acc1: 2.3438 (8.0729)  acc5: 12.8906 (19.3824)  time: 0.5100  data: 0.0016  max mem: 1586
Test:  [30/59]  eta: 0:00:16  loss: 4.5251 (4.4953)  acc1: 1.5625 (7.3589)  acc5: 11.3281 (17.2127)  time: 0.5090  data: 0.0022  max mem: 1586
Test:  [40/59]  eta: 0:00:10  loss: 4.4456 (4.4536)  acc1: 1.5625 (7.3933)  acc5: 14.4531 (19.5408)  time: 0.5297  data: 0.0030  max mem: 1586
Test:  [50/59]  eta: 0:00:04  loss: 4.3895 (4.4374)  acc1: 2.3438 (7.1232)  acc5: 26.5625 (20.7721)  time: 0.4819  data: 0.0034  max mem: 1586
Test:  [58/59]  eta: 0:00:00  loss: 4.3999 (4.4428)  acc1: 1.1719 (6.3415)  acc5: 20.3125 (19.9064)  time: 0.5201  data: 0.0035  max mem: 1586
Test: Total time: 0:00:32 (0.5538 s / it)
* Acc@1 6.341 Acc@5 19.906 loss 4.443
**************Prune Round 2**********************

Current sparsity level: 1.0493499486937832e-07
Actual sparsity after pruning: 1.0493499486937832e-07
Epoch: [0]  [  0/176]  eta: 0:09:26  lr: 0.000000  min_lr: 0.000000  loss: 4.5527 (4.5527)  class_acc: 0.0273 (0.0273)  weight_decay: 0.0500 (0.0500)  time: 3.2211  data: 1.1393  max mem: 1586
Epoch: [0]  [ 10/176]  eta: 0:06:41  lr: 0.000005  min_lr: 0.000005  loss: 4.5527 (4.5543)  class_acc: 0.0312 (0.0320)  weight_decay: 0.0499 (0.0499)  time: 2.4179  data: 0.1060  max mem: 1586
Epoch: [0]  [ 20/176]  eta: 0:06:11  lr: 0.000009  min_lr: 0.000009  loss: 4.5486 (4.5509)  class_acc: 0.0352 (0.0346)  weight_decay: 0.0495 (0.0495)  time: 2.3382  data: 0.0022  max mem: 1586
Epoch: [0]  [ 30/176]  eta: 0:05:50  lr: 0.000014  min_lr: 0.000014  loss: 4.5514 (4.5522)  class_acc: 0.0352 (0.0344)  weight_decay: 0.0482 (0.0488)  time: 2.3864  data: 0.0035  max mem: 1586
Epoch: [0]  [ 40/176]  eta: 0:05:29  lr: 0.000018  min_lr: 0.000018  loss: 4.5517 (4.5515)  class_acc: 0.0352 (0.0346)  weight_decay: 0.0462 (0.0479)  time: 2.4622  data: 0.0061  max mem: 1586
Epoch: [0]  [ 50/176]  eta: 0:05:11  lr: 0.000023  min_lr: 0.000023  loss: 4.5311 (4.5474)  class_acc: 0.0352 (0.0363)  weight_decay: 0.0435 (0.0467)  time: 2.5866  data: 0.0071  max mem: 1586
Epoch: [0]  [ 60/176]  eta: 0:04:50  lr: 0.000027  min_lr: 0.000027  loss: 4.5256 (4.5446)  class_acc: 0.0391 (0.0369)  weight_decay: 0.0402 (0.0454)  time: 2.6695  data: 0.0051  max mem: 1586
Epoch: [0]  [ 70/176]  eta: 0:04:26  lr: 0.000032  min_lr: 0.000032  loss: 4.5241 (4.5414)  class_acc: 0.0391 (0.0372)  weight_decay: 0.0364 (0.0439)  time: 2.6279  data: 0.0031  max mem: 1586
Epoch: [0]  [ 80/176]  eta: 0:04:02  lr: 0.000037  min_lr: 0.000037  loss: 4.5181 (4.5386)  class_acc: 0.0391 (0.0375)  weight_decay: 0.0323 (0.0422)  time: 2.5981  data: 0.0033  max mem: 1586
Epoch: [0]  [ 90/176]  eta: 0:03:38  lr: 0.000041  min_lr: 0.000041  loss: 4.5231 (4.5381)  class_acc: 0.0352 (0.0377)  weight_decay: 0.0279 (0.0404)  time: 2.6458  data: 0.0028  max mem: 1586
Epoch: [0]  [100/176]  eta: 0:03:13  lr: 0.000046  min_lr: 0.000046  loss: 4.5198 (4.5353)  class_acc: 0.0352 (0.0376)  weight_decay: 0.0234 (0.0385)  time: 2.6249  data: 0.0023  max mem: 1586
Epoch: [0]  [110/176]  eta: 0:02:47  lr: 0.000050  min_lr: 0.000050  loss: 4.5064 (4.5322)  class_acc: 0.0352 (0.0378)  weight_decay: 0.0190 (0.0366)  time: 2.5432  data: 0.0032  max mem: 1586
Epoch: [0]  [120/176]  eta: 0:02:23  lr: 0.000055  min_lr: 0.000055  loss: 4.4989 (4.5300)  class_acc: 0.0352 (0.0380)  weight_decay: 0.0148 (0.0347)  time: 2.6175  data: 0.0043  max mem: 1586
Epoch: [0]  [130/176]  eta: 0:01:58  lr: 0.000059  min_lr: 0.000059  loss: 4.4898 (4.5267)  class_acc: 0.0391 (0.0384)  weight_decay: 0.0109 (0.0327)  time: 2.6920  data: 0.0041  max mem: 1586
Epoch: [0]  [140/176]  eta: 0:01:32  lr: 0.000064  min_lr: 0.000064  loss: 4.4761 (4.5238)  class_acc: 0.0391 (0.0385)  weight_decay: 0.0074 (0.0308)  time: 2.6366  data: 0.0043  max mem: 1586
Epoch: [0]  [150/176]  eta: 0:01:06  lr: 0.000069  min_lr: 0.000069  loss: 4.4815 (4.5202)  class_acc: 0.0352 (0.0382)  weight_decay: 0.0045 (0.0290)  time: 2.6357  data: 0.0040  max mem: 1586
Epoch: [0]  [160/176]  eta: 0:00:41  lr: 0.000073  min_lr: 0.000073  loss: 4.4815 (4.5183)  class_acc: 0.0312 (0.0380)  weight_decay: 0.0023 (0.0273)  time: 2.6444  data: 0.0036  max mem: 1586
Epoch: [0]  [170/176]  eta: 0:00:15  lr: 0.000078  min_lr: 0.000078  loss: 4.4638 (4.5141)  class_acc: 0.0391 (0.0381)  weight_decay: 0.0008 (0.0257)  time: 2.6442  data: 0.0037  max mem: 1586
Epoch: [0]  [175/176]  eta: 0:00:02  lr: 0.000080  min_lr: 0.000080  loss: 4.4515 (4.5126)  class_acc: 0.0430 (0.0385)  weight_decay: 0.0004 (0.0251)  time: 2.4919  data: 0.0039  max mem: 1586
Epoch: [0] Total time: 0:07:32 (2.5707 s / it)
Averaged stats: lr: 0.000080  min_lr: 0.000080  loss: 4.4515 (4.5126)  class_acc: 0.0430 (0.0385)  weight_decay: 0.0004 (0.0251)
Test:  [  0/176]  eta: 0:08:48  loss: 4.4885 (4.4885)  acc1: 5.4688 (5.4688)  acc5: 16.4062 (16.4062)  time: 3.0034  data: 2.7911  max mem: 1586
Test:  [ 10/176]  eta: 0:02:04  loss: 4.4579 (4.4484)  acc1: 5.4688 (5.1136)  acc5: 15.2344 (16.1222)  time: 0.7509  data: 0.2821  max mem: 1586
Test:  [ 20/176]  eta: 0:01:46  loss: 4.4435 (4.4442)  acc1: 4.6875 (4.8735)  acc5: 15.2344 (16.1458)  time: 0.5679  data: 0.0166  max mem: 1586
Test:  [ 30/176]  eta: 0:01:36  loss: 4.4399 (4.4432)  acc1: 4.6875 (4.7631)  acc5: 16.0156 (16.1542)  time: 0.6106  data: 0.0026  max mem: 1586
Test:  [ 40/176]  eta: 0:01:28  loss: 4.4382 (4.4419)  acc1: 4.2969 (4.7447)  acc5: 16.0156 (16.2633)  time: 0.6108  data: 0.0032  max mem: 1586
Test:  [ 50/176]  eta: 0:01:20  loss: 4.4383 (4.4426)  acc1: 4.6875 (4.7871)  acc5: 16.4062 (16.1994)  time: 0.6110  data: 0.0036  max mem: 1586
Test:  [ 60/176]  eta: 0:01:13  loss: 4.4480 (4.4434)  acc1: 4.6875 (4.7195)  acc5: 16.0156 (16.2846)  time: 0.6112  data: 0.0037  max mem: 1586
Test:  [ 70/176]  eta: 0:01:07  loss: 4.4545 (4.4455)  acc1: 4.2969 (4.6325)  acc5: 16.0156 (16.1587)  time: 0.6194  data: 0.0061  max mem: 1586
Test:  [ 80/176]  eta: 0:01:00  loss: 4.4545 (4.4456)  acc1: 4.2969 (4.6200)  acc5: 15.2344 (16.1458)  time: 0.6153  data: 0.0112  max mem: 1586
Test:  [ 90/176]  eta: 0:00:53  loss: 4.4622 (4.4482)  acc1: 4.2969 (4.5716)  acc5: 14.8438 (16.0242)  time: 0.5602  data: 0.0198  max mem: 1586
Test:  [100/176]  eta: 0:00:46  loss: 4.4616 (4.4491)  acc1: 4.2969 (4.5289)  acc5: 14.8438 (15.9924)  time: 0.5640  data: 0.0146  max mem: 1586
Test:  [110/176]  eta: 0:00:40  loss: 4.4398 (4.4477)  acc1: 4.2969 (4.5925)  acc5: 15.6250 (15.9628)  time: 0.6107  data: 0.0034  max mem: 1586
Test:  [120/176]  eta: 0:00:34  loss: 4.4409 (4.4492)  acc1: 4.6875 (4.5745)  acc5: 15.6250 (15.9704)  time: 0.6105  data: 0.0032  max mem: 1586
Test:  [130/176]  eta: 0:00:28  loss: 4.4694 (4.4516)  acc1: 4.2969 (4.6040)  acc5: 14.4531 (15.8844)  time: 0.6086  data: 0.0023  max mem: 1586
Test:  [140/176]  eta: 0:00:22  loss: 4.4479 (4.4504)  acc1: 4.6875 (4.5961)  acc5: 16.4062 (15.9547)  time: 0.6089  data: 0.0026  max mem: 1586
Test:  [150/176]  eta: 0:00:15  loss: 4.4395 (4.4502)  acc1: 4.2969 (4.5944)  acc5: 16.7969 (15.9846)  time: 0.5546  data: 0.0040  max mem: 1586
Test:  [160/176]  eta: 0:00:09  loss: 4.4429 (4.4489)  acc1: 4.2969 (4.5880)  acc5: 16.7969 (16.0787)  time: 0.5269  data: 0.0058  max mem: 1586
Test:  [170/176]  eta: 0:00:03  loss: 4.4456 (4.4491)  acc1: 4.2969 (4.5482)  acc5: 16.0156 (16.0613)  time: 0.5818  data: 0.0049  max mem: 1586
Test:  [175/176]  eta: 0:00:00  loss: 4.4449 (4.4495)  acc1: 3.9062 (4.5289)  acc5: 15.6250 (16.0222)  time: 0.5656  data: 0.0029  max mem: 1586
Test: Total time: 0:01:45 (0.6012 s / it)
* Acc@1 4.529 Acc@5 16.022 loss 4.449
Test:  [ 0/59]  eta: 0:02:05  loss: 3.7853 (3.7853)  acc1: 7.4219 (7.4219)  acc5: 46.0938 (46.0938)  time: 2.1253  data: 1.9224  max mem: 1586
Test:  [10/59]  eta: 0:00:34  loss: 4.2754 (4.2350)  acc1: 3.5156 (3.3736)  acc5: 15.6250 (20.3835)  time: 0.6992  data: 0.1799  max mem: 1586
Test:  [20/59]  eta: 0:00:23  loss: 4.2616 (4.2553)  acc1: 3.5156 (6.2500)  acc5: 17.9688 (22.0796)  time: 0.5314  data: 0.0050  max mem: 1586
Test:  [30/59]  eta: 0:00:16  loss: 4.2081 (4.2313)  acc1: 3.5156 (6.6028)  acc5: 22.2656 (22.1018)  time: 0.5043  data: 0.0033  max mem: 1586
Test:  [40/59]  eta: 0:00:11  loss: 4.1838 (4.2095)  acc1: 3.5156 (7.0122)  acc5: 25.7812 (24.3521)  time: 0.5557  data: 0.0024  max mem: 1586
Test:  [50/59]  eta: 0:00:04  loss: 4.0954 (4.1934)  acc1: 5.4688 (7.9657)  acc5: 30.0781 (25.6664)  time: 0.4928  data: 0.0040  max mem: 1586
Test:  [58/59]  eta: 0:00:00  loss: 4.1179 (4.1900)  acc1: 4.2969 (7.8249)  acc5: 23.8281 (25.4728)  time: 0.4769  data: 0.0042  max mem: 1586
Test: Total time: 0:00:32 (0.5478 s / it)
* Acc@1 7.825 Acc@5 25.473 loss 4.190
**************Prune Round 3**********************

Current sparsity level: 1.0493499486937832e-07
Actual sparsity after pruning: 1.0493499486937832e-07
Epoch: [0]  [  0/176]  eta: 0:15:08  lr: 0.000000  min_lr: 0.000000  loss: 4.4089 (4.4089)  class_acc: 0.0547 (0.0547)  weight_decay: 0.0500 (0.0500)  time: 5.1632  data: 3.1896  max mem: 1586
Epoch: [0]  [ 10/176]  eta: 0:07:18  lr: 0.000005  min_lr: 0.000005  loss: 4.4538 (4.4504)  class_acc: 0.0469 (0.0465)  weight_decay: 0.0499 (0.0499)  time: 2.6424  data: 0.2916  max mem: 1586
Epoch: [0]  [ 20/176]  eta: 0:06:29  lr: 0.000009  min_lr: 0.000009  loss: 4.4543 (4.4507)  class_acc: 0.0430 (0.0458)  weight_decay: 0.0495 (0.0495)  time: 2.3648  data: 0.0017  max mem: 1586
Epoch: [0]  [ 30/176]  eta: 0:06:07  lr: 0.000014  min_lr: 0.000014  loss: 4.4481 (4.4475)  class_acc: 0.0391 (0.0437)  weight_decay: 0.0482 (0.0488)  time: 2.4524  data: 0.0023  max mem: 1586
Epoch: [0]  [ 40/176]  eta: 0:05:45  lr: 0.000018  min_lr: 0.000018  loss: 4.4288 (4.4395)  class_acc: 0.0430 (0.0452)  weight_decay: 0.0462 (0.0479)  time: 2.5891  data: 0.0026  max mem: 1586
Epoch: [0]  [ 50/176]  eta: 0:05:24  lr: 0.000023  min_lr: 0.000023  loss: 4.4190 (4.4387)  class_acc: 0.0508 (0.0467)  weight_decay: 0.0435 (0.0467)  time: 2.6586  data: 0.0021  max mem: 1586
